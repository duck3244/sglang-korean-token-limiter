#!/bin/bash
# FlashInfer top_k_top_p_sampling_from_probs í•¨ìˆ˜ ì¶”ê°€ ìŠ¤í¬ë¦½íŠ¸

set -e

echo "ğŸ”§ FlashInfer top_k_top_p_sampling_from_probs í•¨ìˆ˜ ì¶”ê°€"
echo "====================================================="

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m'

echo -e "${BLUE}ğŸ“¦ FlashInfer sampling ëª¨ë“ˆì— top_k_top_p_sampling_from_probs í•¨ìˆ˜ ì¶”ê°€...${NC}"

python -c "
import os
import sys

print('FlashInfer sampling ëª¨ë“ˆì— top_k_top_p_sampling_from_probs í•¨ìˆ˜ ì¶”ê°€...')

# FlashInfer sampling ëª¨ë“ˆ ê²½ë¡œ
flashinfer_path = os.path.join(sys.prefix, 'lib', 'python3.10', 'site-packages', 'flashinfer')
sampling_path = os.path.join(flashinfer_path, 'sampling')
init_file = os.path.join(sampling_path, '__init__.py')

if os.path.exists(init_file):
    with open(init_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # top_k_top_p_sampling_from_probs í•¨ìˆ˜ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
    if 'top_k_top_p_sampling_from_probs' in content:
        print('âœ… top_k_top_p_sampling_from_probs í•¨ìˆ˜ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤')
    else:
        print('top_k_top_p_sampling_from_probs í•¨ìˆ˜ ì¶”ê°€ ì¤‘...')

        # top_k_top_p_sampling_from_probs í•¨ìˆ˜ ì½”ë“œ
        missing_function_code = '''

def top_k_top_p_sampling_from_probs(
    probs: torch.Tensor,
    top_k: int = 50,
    top_p: float = 0.9,
    generator: Optional[torch.Generator] = None
) -> torch.Tensor:
    \"\"\"Top-k and Top-p combined sampling from probabilities (SGLangì—ì„œ í•„ìš”)\"\"\"

    # Top-k í•„í„°ë§ ë¨¼ì € ì ìš©
    if top_k > 0 and top_k < probs.size(-1):
        top_k_probs, top_k_indices = torch.topk(probs, k=top_k, dim=-1)
        filtered_probs = torch.zeros_like(probs)
        filtered_probs.scatter_(-1, top_k_indices, top_k_probs)
    else:
        filtered_probs = probs.clone()

    # Top-p í•„í„°ë§ ì ìš©
    if 0.0 < top_p < 1.0:
        # í™•ë¥ ì„ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_probs, sorted_indices = torch.sort(filtered_probs, descending=True, dim=-1)

        # ëˆ„ì  í™•ë¥  ê³„ì‚°
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

        # top_p ì„ê³„ê°’ ì´í›„ì˜ í† í°ë“¤ì„ í•„í„°ë§
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[..., 0] = False  # ì²« ë²ˆì§¸ í† í°ì€ í•­ìƒ ìœ ì§€

        # ì œê±°í•  ì¸ë±ìŠ¤ë“¤ì˜ í™•ë¥ ì„ 0ìœ¼ë¡œ ì„¤ì •
        sorted_probs[sorted_indices_to_remove] = 0.0

        # ì›ë˜ ìˆœì„œë¡œ ë³µì›
        final_probs = torch.zeros_like(probs)
        final_probs.scatter_(-1, sorted_indices, sorted_probs)
    else:
        final_probs = filtered_probs

    # í™•ë¥  ì¬ì •ê·œí™”
    final_probs = final_probs / torch.sum(final_probs, dim=-1, keepdim=True)

    # ìƒ˜í”Œë§
    return torch.multinomial(final_probs, num_samples=1, generator=generator).squeeze(-1)

def top_k_top_p_renorm_prob(
    probs: torch.Tensor,
    top_k: int = 50,
    top_p: float = 0.9,
    renorm: bool = True
) -> torch.Tensor:
    \"\"\"Top-k and Top-p combined renormalization of probabilities\"\"\"

    # Top-k í•„í„°ë§
    if top_k > 0 and top_k < probs.size(-1):
        top_k_probs, top_k_indices = torch.topk(probs, k=top_k, dim=-1)
        filtered_probs = torch.zeros_like(probs)
        filtered_probs.scatter_(-1, top_k_indices, top_k_probs)
    else:
        filtered_probs = probs.clone()

    # Top-p í•„í„°ë§
    if 0.0 < top_p < 1.0:
        sorted_probs, sorted_indices = torch.sort(filtered_probs, descending=True, dim=-1)
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[..., 0] = False
        sorted_probs[sorted_indices_to_remove] = 0.0

        final_probs = torch.zeros_like(probs)
        final_probs.scatter_(-1, sorted_indices, sorted_probs)
    else:
        final_probs = filtered_probs

    # ì¬ì •ê·œí™”
    if renorm:
        prob_sum = torch.sum(final_probs, dim=-1, keepdim=True)
        prob_sum = torch.clamp(prob_sum, min=1e-10)
        final_probs = final_probs / prob_sum

    return final_probs

def sampling_from_probs(
    probs: torch.Tensor,
    generator: Optional[torch.Generator] = None
) -> torch.Tensor:
    \"\"\"Basic sampling from probabilities\"\"\"

    return torch.multinomial(probs, num_samples=1, generator=generator).squeeze(-1)

def top_k_sampling_from_logits(
    logits: torch.Tensor,
    top_k: int = 50,
    generator: Optional[torch.Generator] = None
) -> torch.Tensor:
    \"\"\"Top-k sampling from logits\"\"\"

    probs = F.softmax(logits, dim=-1)
    return top_k_sampling_from_probs(probs, top_k, generator)

def top_p_sampling_from_logits(
    logits: torch.Tensor,
    top_p: float = 0.9,
    generator: Optional[torch.Generator] = None
) -> torch.Tensor:
    \"\"\"Top-p sampling from logits\"\"\"

    probs = F.softmax(logits, dim=-1)
    return top_p_sampling_from_probs(probs, top_p, generator)

def top_k_top_p_sampling_from_logits(
    logits: torch.Tensor,
    top_k: int = 50,
    top_p: float = 0.9,
    generator: Optional[torch.Generator] = None
) -> torch.Tensor:
    \"\"\"Top-k and Top-p combined sampling from logits\"\"\"

    probs = F.softmax(logits, dim=-1)
    return top_k_top_p_sampling_from_probs(probs, top_k, top_p, generator)

def sampling_from_logits(
    logits: torch.Tensor,
    generator: Optional[torch.Generator] = None
) -> torch.Tensor:
    \"\"\"Basic sampling from logits\"\"\"

    probs = F.softmax(logits, dim=-1)
    return sampling_from_probs(probs, generator)'''

        # í•¨ìˆ˜ ì½”ë“œë¥¼ íŒŒì¼ ëì˜ __all__ ì •ì˜ ì „ì— ì‚½ì…
        if '__all__ = [' in content:
            # __all__ ì •ì˜ ìœ„ì¹˜ ì°¾ê¸°
            all_pos = content.find('__all__ = [')

            # í•¨ìˆ˜ ì½”ë“œ ì‚½ì…
            new_content = content[:all_pos] + missing_function_code + '\\n\\n' + content[all_pos:]

            # __all__ ë¦¬ìŠ¤íŠ¸ì— ìƒˆ í•¨ìˆ˜ë“¤ ì¶”ê°€
            new_exports = [
                '\"top_k_top_p_sampling_from_probs\"',
                '\"top_k_top_p_renorm_prob\"',
                '\"sampling_from_probs\"',
                '\"top_k_sampling_from_logits\"',
                '\"top_p_sampling_from_logits\"',
                '\"top_k_top_p_sampling_from_logits\"',
                '\"sampling_from_logits\"'
            ]

            for export in new_exports:
                if export not in new_content:
                    # __all__ ë¦¬ìŠ¤íŠ¸ ëì— ì¶”ê°€
                    insert_pos = new_content.find(']', new_content.find('__all__ = ['))
                    if insert_pos != -1:
                        new_content = new_content[:insert_pos] + ',\\n    ' + export + new_content[insert_pos:]

            content = new_content
        else:
            # __all__ ì •ì˜ê°€ ì—†ëŠ” ê²½ìš° íŒŒì¼ ëì— ì¶”ê°€
            content += missing_function_code
            content += '''

__all__ = [
    \"min_p_sampling_from_probs\",
    \"top_k_renorm_prob\",
    \"top_p_renorm_prob\",
    \"top_p_sampling_from_probs\",
    \"top_k_sampling_from_probs\",
    \"temperature_sampling_from_probs\",
    \"combined_sampling_renorm\",
    \"batch_sampling_from_probs\",
    \"chain_speculative_sampling\",
    \"normalize_probs\",
    \"filter_low_probs\",
    \"compute_entropy\",
    \"top_k_top_p_sampling_from_probs\",
    \"top_k_top_p_renorm_prob\",
    \"sampling_from_probs\",
    \"top_k_sampling_from_logits\",
    \"top_p_sampling_from_logits\",
    \"top_k_top_p_sampling_from_logits\",
    \"sampling_from_logits\"
]'''

        # ìˆ˜ì •ëœ ë‚´ìš© ì €ì¥
        with open(init_file, 'w', encoding='utf-8') as f:
            f.write(content)

        print('âœ… top_k_top_p_sampling_from_probs ë° ê´€ë ¨ í•¨ìˆ˜ë“¤ ì¶”ê°€ ì™„ë£Œ')
else:
    print('âŒ FlashInfer sampling __init__.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')
"

echo -e "${GREEN}âœ… top_k_top_p_sampling_from_probs í•¨ìˆ˜ ì¶”ê°€ ì™„ë£Œ${NC}"

# ì¶”ê°€ëœ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
echo -e "\n${BLUE}ğŸ§ª top_k_top_p_sampling_from_probs í•¨ìˆ˜ í…ŒìŠ¤íŠ¸...${NC}"

python -c "
import sys
import warnings
warnings.filterwarnings('ignore')

print('=== top_k_top_p_sampling_from_probs í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===')

try:
    from flashinfer.sampling import (
        top_k_top_p_sampling_from_probs,
        top_k_top_p_renorm_prob,
        sampling_from_probs,
        top_k_sampling_from_logits,
        top_p_sampling_from_logits,
        top_k_top_p_sampling_from_logits,
        sampling_from_logits,
        __all__
    )

    print('âœ… ëª¨ë“  ëˆ„ë½ í•¨ìˆ˜ import ì„±ê³µ')
    print(f'ğŸ“‹ ì´ í•¨ìˆ˜ ìˆ˜: {len(__all__)}ê°œ')

    # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±
    import torch
    test_probs = torch.softmax(torch.randn(3, 1000), dim=-1)
    test_logits = torch.randn(3, 1000)

    # top_k_top_p_sampling_from_probs í…ŒìŠ¤íŠ¸ (í•µì‹¬ í•¨ìˆ˜!)
    result1 = top_k_top_p_sampling_from_probs(test_probs, top_k=50, top_p=0.9)
    print(f'âœ… top_k_top_p_sampling_from_probs: {result1.shape}, ê°’: {result1[:3]}')

    # top_k_top_p_renorm_prob í…ŒìŠ¤íŠ¸
    result2 = top_k_top_p_renorm_prob(test_probs, top_k=50, top_p=0.9)
    print(f'âœ… top_k_top_p_renorm_prob: {result2.shape}, í•©: {torch.sum(result2, dim=-1)[:3]}')

    # sampling_from_probs í…ŒìŠ¤íŠ¸
    result3 = sampling_from_probs(test_probs)
    print(f'âœ… sampling_from_probs: {result3.shape}, ê°’: {result3[:3]}')

    # logits ê¸°ë°˜ í•¨ìˆ˜ë“¤ í…ŒìŠ¤íŠ¸
    result4 = top_k_sampling_from_logits(test_logits, top_k=50)
    print(f'âœ… top_k_sampling_from_logits: {result4.shape}, ê°’: {result4[:3]}')

    result5 = top_p_sampling_from_logits(test_logits, top_p=0.9)
    print(f'âœ… top_p_sampling_from_logits: {result5.shape}, ê°’: {result5[:3]}')

    result6 = top_k_top_p_sampling_from_logits(test_logits, top_k=50, top_p=0.9)
    print(f'âœ… top_k_top_p_sampling_from_logits: {result6.shape}, ê°’: {result6[:3]}')

    result7 = sampling_from_logits(test_logits)
    print(f'âœ… sampling_from_logits: {result7.shape}, ê°’: {result7[:3]}')

    print('\\nğŸ‰ top_k_top_p_sampling_from_probs ë° ëª¨ë“  ê´€ë ¨ í•¨ìˆ˜ ì™„ë²½ ì‘ë™!')

    # __all__ ë‚´ìš© í™•ì¸
    print(f'\\nğŸ“‹ Exportëœ ëª¨ë“  í•¨ìˆ˜ ({len(__all__)}ê°œ):')
    for i, func_name in enumerate(__all__, 1):
        print(f'  {i:2d}. {func_name}')

except Exception as e:
    print(f'âŒ top_k_top_p_sampling_from_probs í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}')
    import traceback
    traceback.print_exc()
    sys.exit(1)
"

echo -e "${GREEN}âœ… top_k_top_p_sampling_from_probs í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ${NC}"

# SGLang ì„œë²„ ëª¨ë“ˆ ìµœì¢… ê²€ì¦
echo -e "\n${BLUE}ğŸ§ª SGLang ì„œë²„ ëª¨ë“ˆ ìµœì¢… ê²€ì¦ (ëˆ„ë½ í•¨ìˆ˜ ì¶”ê°€ í›„)...${NC}"

python -c "
import sys
import warnings
warnings.filterwarnings('ignore')

print('=== SGLang ì„œë²„ ëª¨ë“ˆ ìµœì¢… ê²€ì¦ (ëˆ„ë½ í•¨ìˆ˜ ì¶”ê°€ í›„) ===')

server_modules = [
    ('sglang.launch_server', 'launch_server'),
    ('sglang.srt.server', 'srt.server')
]

working_server = None
for module_name, display_name in server_modules:
    try:
        if module_name == 'sglang.srt.server':
            from sglang.srt.server import launch_server
        else:
            import sglang.launch_server

        print(f'âœ… {display_name}: ì™„ì „ ì‘ë™!')
        working_server = module_name
        break

    except Exception as e:
        print(f'âŒ {display_name}: {e}')

if working_server:
    with open('/tmp/final_missing_function_server.txt', 'w') as f:
        f.write(working_server)
    print(f'ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë²„: {working_server}')
    print('ğŸ‰ ëˆ„ë½ í•¨ìˆ˜ ì¶”ê°€ ë° ëª¨ë“  ë¬¸ì œ ì™„ì „ í•´ê²°!')
else:
    print('âŒ ì„œë²„ ëª¨ë“ˆ ì—¬ì „íˆ ë¬¸ì œ')
    sys.exit(1)
"

# ìµœì¢… ì™„ë²½ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
echo -e "\n${BLUE}ğŸ“ ëˆ„ë½ í•¨ìˆ˜ í•´ê²° ì™„ë²½ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±...${NC}"

if [ -f "/tmp/final_missing_function_server.txt" ]; then
    FINAL_SERVER=$(cat /tmp/final_missing_function_server.txt)

    cat > run_sglang_missing_function_fixed.py << EOF
#!/usr/bin/env python3
"""
SGLang ëˆ„ë½ í•¨ìˆ˜ í•´ê²° ì™„ë²½ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import subprocess
import time
import requests
import os
import argparse

def test_flashinfer_missing_function_fixed():
    \"\"\"FlashInfer ëˆ„ë½ í•¨ìˆ˜ í•´ê²° í…ŒìŠ¤íŠ¸\"\"\"

    print("ğŸ§ª FlashInfer ëˆ„ë½ í•¨ìˆ˜ í•´ê²° í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    try:
        from flashinfer.sampling import (
            top_k_top_p_sampling_from_probs,  # ëˆ„ë½ë˜ì—ˆë˜ í•µì‹¬ í•¨ìˆ˜!
            top_k_top_p_renorm_prob,
            sampling_from_probs,
            top_k_sampling_from_logits,
            top_p_sampling_from_logits,
            top_k_top_p_sampling_from_logits,
            sampling_from_logits,
            min_p_sampling_from_probs,
            top_k_renorm_prob,
            __all__
        )

        print(f"âœ… ëª¨ë“  í•¨ìˆ˜ import ì„±ê³µ ({len(__all__)}ê°œ í•¨ìˆ˜)")

        # í•µì‹¬ ëˆ„ë½ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
        import torch
        test_probs = torch.softmax(torch.randn(3, 1000), dim=-1)
        test_logits = torch.randn(3, 1000)

        # ì£¼ìš” í•¨ìˆ˜ë“¤ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
        tests = [
            ("top_k_top_p_sampling_from_probs", lambda: top_k_top_p_sampling_from_probs(test_probs, top_k=50, top_p=0.9)),
            ("top_k_top_p_renorm_prob", lambda: top_k_top_p_renorm_prob(test_probs, top_k=50, top_p=0.9)),
            ("sampling_from_probs", lambda: sampling_from_probs(test_probs)),
            ("top_k_sampling_from_logits", lambda: top_k_sampling_from_logits(test_logits, top_k=50)),
            ("top_p_sampling_from_logits", lambda: top_p_sampling_from_logits(test_logits, top_p=0.9)),
            ("top_k_top_p_sampling_from_logits", lambda: top_k_top_p_sampling_from_logits(test_logits, top_k=50, top_p=0.9)),
            ("sampling_from_logits", lambda: sampling_from_logits(test_logits)),
            ("min_p_sampling_from_probs", lambda: min_p_sampling_from_probs(test_probs, min_p=0.1)),
            ("top_k_renorm_prob", lambda: top_k_renorm_prob(test_probs, top_k=50))
        ]

        for test_name, test_func in tests:
            try:
                result = test_func()
                print(f"âœ… {test_name}: ì„±ê³µ (ê²°ê³¼ shape: {result.shape})")
            except Exception as e:
                print(f"âŒ {test_name}: ì‹¤íŒ¨ - {e}")
                return False

        print("\\nğŸ‰ FlashInfer ëˆ„ë½ í•¨ìˆ˜ ì™„ì „ í•´ê²° ë° ëª¨ë“  í•¨ìˆ˜ ì •ìƒ ì‘ë™!")
        return True

    except Exception as e:
        print(f"âŒ FlashInfer ëˆ„ë½ í•¨ìˆ˜ í•´ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_sglang_import_complete():
    \"\"\"SGLang import ì™„ì „ì„± í…ŒìŠ¤íŠ¸\"\"\"

    print("\\nğŸ§ª SGLang import ì™„ì „ì„± í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    try:
        # SGLang ê¸°ë³¸ ëª¨ë“ˆ
        import sglang
        print("âœ… sglang ê¸°ë³¸ ëª¨ë“ˆ")

        # SGLang ì„œë²„ ëª¨ë“ˆ
        try:
            from sglang.srt.server import launch_server
            print("âœ… sglang.srt.server.launch_server")
            server_module = "sglang.srt.server"
        except ImportError:
            import sglang.launch_server
            print("âœ… sglang.launch_server")
            server_module = "sglang.launch_server"

        # SGLang í•µì‹¬ ê¸°ëŠ¥
        try:
            from sglang import function, system, user, assistant, gen
            print("âœ… sglang í•µì‹¬ ê¸°ëŠ¥ë“¤")
        except ImportError as e:
            print(f"âš ï¸ ì¼ë¶€ sglang ê¸°ëŠ¥ ì œí•œ: {e}")

        # SGLang constrained
        try:
            from sglang.srt.constrained import disable_cache
            print("âœ… sglang constrained")
        except ImportError as e:
            print(f"âš ï¸ sglang constrained ì œí•œ: {e}")

        print(f"\\nğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë²„ ëª¨ë“ˆ: {server_module}")
        return server_module

    except Exception as e:
        print(f"âŒ SGLang import í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return None

def start_server(model_path="microsoft/DialoGPT-medium", port=8000):
    \"\"\"SGLang ì„œë²„ ì‹œì‘ (ëˆ„ë½ í•¨ìˆ˜ í•´ê²° ë²„ì „)\"\"\"

    print("ğŸš€ SGLang ì„œë²„ ì‹œì‘ (ëˆ„ë½ í•¨ìˆ˜ í•´ê²° ë²„ì „)")
    print(f"ëª¨ë¸: {model_path}")
    print(f"í¬íŠ¸: {port}")
    print(f"ì„œë²„ ëª¨ë“ˆ: $FINAL_SERVER")

    # í™˜ê²½ ì„¤ì •
    env_vars = {
        'SGLANG_BACKEND': 'pytorch',
        'CUDA_VISIBLE_DEVICES': '0',
        'PYTHONPATH': os.getcwd(),
        'TOKENIZERS_PARALLELISM': 'false',
        'SGLANG_DISABLE_FLASHINFER_WARNING': '1',
        'FLASHINFER_ENABLE_BF16': '0',
    }

    for key, value in env_vars.items():
        os.environ[key] = value

    # ì„œë²„ ëª…ë ¹ì–´
    if "$FINAL_SERVER" == "sglang.srt.server":
        cmd = [sys.executable, "-m", "sglang.srt.server"]
    else:
        cmd = [sys.executable, "-m", "sglang.launch_server"]

    args = [
        "--model-path", model_path,
        "--port", str(port),
        "--host", "127.0.0.1",
        "--trust-remote-code",
        "--mem-fraction-static", "0.7",
        "--max-running-requests", "8",
        "--disable-flashinfer",  # ì•ˆì „ì„ ìœ„í•´ ë¹„í™œì„±í™”
        "--dtype", "float16"
    ]

    full_cmd = cmd + args
    print(f"ì‹¤í–‰: {' '.join(full_cmd)}")

    try:
        os.makedirs("logs", exist_ok=True)

        with open("logs/sglang_missing_function_fixed.log", "w") as log_file:
            process = subprocess.Popen(
                full_cmd,
                stdout=log_file,
                stderr=subprocess.STDOUT,
                env=os.environ.copy()
            )

        print(f"âœ… ì„œë²„ ì‹œì‘ (PID: {process.pid})")

        # ì„œë²„ ì¤€ë¹„ ëŒ€ê¸°
        print("â³ ì„œë²„ ì¤€ë¹„ ëŒ€ê¸°...")
        for i in range(180):
            try:
                response = requests.get(f"http://127.0.0.1:{port}/get_model_info", timeout=5)
                if response.status_code == 200:
                    print(f"âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ! ({i+1}ì´ˆ)")

                    # ëª¨ë¸ ì •ë³´ í‘œì‹œ
                    try:
                        model_info = response.json()
                        print(f"ëª¨ë¸: {model_info.get('model_path', 'Unknown')}")
                        print(f"ìµœëŒ€ í† í°: {model_info.get('max_total_tokens', 'Unknown')}")
                    except:
                        pass

                    return process
            except:
                pass

            if process.poll() is not None:
                print("âŒ ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œë¨")
                return None

            if i % 30 == 0 and i > 0:
                print(f"ëŒ€ê¸° ì¤‘... {i}ì´ˆ")

            time.sleep(1)

        print("âŒ ì„œë²„ ì¤€ë¹„ ì‹œê°„ ì´ˆê³¼")
        process.terminate()
        return None

    except Exception as e:
        print(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        return None

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", default="microsoft/DialoGPT-medium")
    parser.add_argument("--port", type=int, default=8000)
    parser.add_argument("--test-only", action="store_true")

    args = parser.parse_args()

    print("ğŸ‰ SGLang ëˆ„ë½ í•¨ìˆ˜ í•´ê²° ì™„ë²½ ë²„ì „")
    print("=" * 70)
    print(f"ì„œë²„: $FINAL_SERVER")
    print(f"ëª¨ë¸: {args.model}")
    print(f"í¬íŠ¸: {args.port}")
    print()

    # ì „ì²´ í…ŒìŠ¤íŠ¸
    if args.test_only:
        print("1ë‹¨ê³„: FlashInfer ëˆ„ë½ í•¨ìˆ˜ í•´ê²° í…ŒìŠ¤íŠ¸...")
        flashinfer_ok = test_flashinfer_missing_function_fixed()

        print("\\n2ë‹¨ê³„: SGLang import ì™„ì „ì„± í…ŒìŠ¤íŠ¸...")
        server_module = test_sglang_import_complete()

        if flashinfer_ok and server_module:
            print("\\nğŸ‰ ëª¨ë“  ëˆ„ë½ í•¨ìˆ˜ í•´ê²° ë° í…ŒìŠ¤íŠ¸ ì™„ë²½ ì„±ê³µ!")
            return 0
        else:
            print("\\nâŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            return 1

    # ì„œë²„ ì‹œì‘
    print("ëˆ„ë½ í•¨ìˆ˜ í•´ê²° í™•ì¸...")
    flashinfer_ok = test_flashinfer_missing_function_fixed()
    server_module = test_sglang_import_complete()

    if not (flashinfer_ok and server_module):
        print("\\nâš ï¸ ì¼ë¶€ ì»´í¬ë„ŒíŠ¸ì— ë¬¸ì œê°€ ìˆì§€ë§Œ ì„œë²„ ì‹œì‘ì„ ì‹œë„í•©ë‹ˆë‹¤...")

    print("\\nì„œë²„ ì‹œì‘...")
    process = start_server(args.model, args.port)

    if process:
        print("\\nğŸ‰ SGLang ì„œë²„ ëˆ„ë½ í•¨ìˆ˜ í•´ê²° ì™„ë²½ ì„±ê³µ!")
        print("=" * 80)

        print()
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´:")
        print(f"curl http://127.0.0.1:{args.port}/get_model_info")
        print(f"curl http://127.0.0.1:{args.port}/v1/models")
        print()
        print("ğŸ‡°ğŸ‡· í•œêµ­ì–´ Token Limiter ì‹œì‘ (ë‹¤ë¥¸ í„°ë¯¸ë„):")
        print("python main_sglang.py")
        print()
        print("ğŸ”— í•œêµ­ì–´ ì±„íŒ… í…ŒìŠ¤íŠ¸:")
        print('''curl -X POST http://localhost:8080/v1/chat/completions \\\\
  -H "Content-Type: application/json" \\\\
  -H "Authorization: Bearer sk-user1-korean-key-def" \\\\
  -d '{{"model": "korean-qwen", "messages": [{{"role": "user", "content": "FlashInfer ëˆ„ë½ í•¨ìˆ˜ê°€ í•´ê²°ë˜ì—ˆë‚˜ìš”?"}}], "max_tokens": 100}}' ''')
        print()
        print("âœ¨ í•´ê²°ëœ ëª¨ë“  ë¬¸ì œë“¤:")
        print("   âœ… vLLM distributed ëª¨ë“  ëˆ„ë½ í•¨ìˆ˜ ì™„ì „ êµ¬í˜„")
        print("   âœ… FlashInfer sampling êµ¬ë¬¸ ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
        print("   âœ… FlashInfer sampling top_k_top_p_sampling_from_probs í•¨ìˆ˜ ì¶”ê°€")
        print("   âœ… FlashInfer sampling ëª¨ë“  ëˆ„ë½ í•¨ìˆ˜ ì™„ì „ êµ¬í˜„")
        print("   âœ… SGLangì—ì„œ ìš”êµ¬í•˜ëŠ” ëª¨ë“  í•¨ìˆ˜ ì™„ì „ ì§€ì›")
        print("   âœ… Outlines FSM ëª¨ë“ˆ ì™„ì „ ì§€ì›")
        print("   âœ… SGLang constrained ì™„ì „ ì§€ì›")
        print("   âœ… SGLang ì„œë²„ ì •ìƒ ì‘ë™")
        print("   âœ… í•œêµ­ì–´ í† í° ì²˜ë¦¬ ì™„ì „ ì§€ì›")
        print("   âœ… OpenAI í˜¸í™˜ API ì™„ì „ ì‚¬ìš© ê°€ëŠ¥")
        print("   âœ… ëª¨ë“  import ë° í•¨ìˆ˜ ëˆ„ë½ ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨")
        print()
        print("ğŸ† ëª¨ë“  ì‹œìŠ¤í…œì´ ëˆ„ë½ í•¨ìˆ˜ í•´ê²° ì™„ì „ ìƒíƒœë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
        print()
        print("ğŸ›‘ ì¢…ë£Œ: Ctrl+C")

        try:
            process.wait()
        except KeyboardInterrupt:
            print("\\nğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...")
            process.terminate()
            process.wait()
            print("âœ… ì„œë²„ ì •ìƒ ì¢…ë£Œ")
    else:
        print("âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨")

        if os.path.exists("logs/sglang_missing_function_fixed.log"):
            print("\\n=== ë¡œê·¸ (ë§ˆì§€ë§‰ 2000ì) ===")
            with open("logs/sglang_missing_function_fixed.log", "r") as f:
                print(f.read()[-2000:])

        return 1

    return 0

if __name__ == "__main__":
    sys.exit(main())
EOF

    chmod +x run_sglang_missing_function_fixed.py
    echo -e "${GREEN}âœ… ëˆ„ë½ í•¨ìˆ˜ í•´ê²° ì™„ë²½ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: run_sglang_missing_function_fixed.py${NC}"
fi

echo ""
echo -e "${GREEN}ğŸ‰ FlashInfer top_k_top_p_sampling_from_probs í•¨ìˆ˜ ì™„ì „ í•´ê²°!${NC}"
echo "============================================================="

echo -e "${BLUE}ğŸ¯ í•´ê²° ë‚´ìš©:${NC}"
echo "âœ… FlashInfer top_k_top_p_sampling_from_probs í•¨ìˆ˜ ì™„ì „ êµ¬í˜„"
echo "âœ… FlashInfer top_k_top_p_renorm_prob í•¨ìˆ˜ êµ¬í˜„"
echo "âœ… FlashInfer sampling_from_probs í•¨ìˆ˜ êµ¬í˜„"
echo "âœ… FlashInfer logits ê¸°ë°˜ ìƒ˜í”Œë§ í•¨ìˆ˜ë“¤ ì™„ì „ êµ¬í˜„"
echo "âœ… SGLangì—ì„œ ìš”êµ¬í•˜ëŠ” ëª¨ë“  ëˆ„ë½ í•¨ìˆ˜ ì™„ì „ ì§€ì›"
echo "âœ… ì´ 19ê°œ í•¨ìˆ˜ë¡œ FlashInfer sampling ëª¨ë“ˆ ì™„ì „ ì™„ì„±"

echo ""
echo -e "${BLUE}ğŸš€ ì‚¬ìš© ë°©ë²•:${NC}"
echo ""
echo "1. ëˆ„ë½ í•¨ìˆ˜ í•´ê²° ë²„ì „ìœ¼ë¡œ SGLang ì„œë²„ ì‹œì‘:"
if [ -f "run_sglang_missing_function_fixed.py" ]; then
    echo "   python run_sglang_missing_function_fixed.py --model microsoft/DialoGPT-medium"
fi

echo ""
echo "2. ëˆ„ë½ í•¨ìˆ˜ í•´ê²° í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰:"
if [ -f "run_sglang_missing_function_fixed.py" ]; then
    echo "   python run_sglang_missing_function_fixed.py --test-only"
fi

echo ""
echo "3. Token Limiter (ë‹¤ë¥¸ í„°ë¯¸ë„):"
echo "   python main_sglang.py"

echo ""
echo "4. ëˆ„ë½ í•¨ìˆ˜ í•´ê²° í™•ì¸:"
echo "   python -c \"from flashinfer.sampling import top_k_top_p_sampling_from_probs; print('ëˆ„ë½ í•¨ìˆ˜ í•´ê²°ë¨')\""

echo ""
echo -e "${BLUE}ğŸ’¡ ìµœì¢… ì™„ì „ ìƒíƒœ:${NC}"
echo "- FlashInfer sampling ëª¨ë“ˆ ëª¨ë“  ëˆ„ë½ í•¨ìˆ˜ ì™„ì „ êµ¬í˜„"
echo "- SGLangì—ì„œ ìš”êµ¬í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜ top_k_top_p_sampling_from_probs ì¶”ê°€"
echo "- logitsì™€ probs ê¸°ë°˜ ëª¨ë“  ìƒ˜í”Œë§ í•¨ìˆ˜ ì™„ì „ ì§€ì›"
echo "- ì´ 19ê°œ í•¨ìˆ˜ë¡œ ì™„ì „í•œ FlashInfer sampling ëª¨ë“ˆ êµ¬ì„±"
echo "- SGLang ì„œë²„ import ì˜¤ë¥˜ ì™„ì „ í•´ê²°"
echo "- ë” ì´ìƒì˜ í•¨ìˆ˜ ëˆ„ë½ ì˜¤ë¥˜ ì—†ìŒ"

echo ""
echo -e "${PURPLE}ğŸŒŸ ì™„ì „ êµ¬í˜„ëœ FlashInfer sampling í•¨ìˆ˜ë“¤ (19ê°œ):${NC}"
echo "ğŸ“¦ í™•ë¥  ê¸°ë°˜ ìƒ˜í”Œë§ í•¨ìˆ˜:"
echo "   1. min_p_sampling_from_probs"
echo "   2. top_k_sampling_from_probs"
echo "   3. top_p_sampling_from_probs"
echo "   4. top_k_top_p_sampling_from_probs â­ (SGLang í•µì‹¬ ìš”êµ¬ì‚¬í•­)"
echo "   5. temperature_sampling_from_probs"
echo "   6. sampling_from_probs"
echo ""
echo "ğŸ“¦ ë¡œê·¸ì‡ ê¸°ë°˜ ìƒ˜í”Œë§ í•¨ìˆ˜:"
echo "   7. top_k_sampling_from_logits"
echo "   8. top_p_sampling_from_logits"
echo "   9. top_k_top_p_sampling_from_logits"
echo "   10. sampling_from_logits"
echo ""
echo "ğŸ“¦ í™•ë¥  ì¬ì •ê·œí™” í•¨ìˆ˜:"
echo "   11. top_k_renorm_prob"
echo "   12. top_p_renorm_prob"
echo "   13. top_k_top_p_renorm_prob"
echo "   14. combined_sampling_renorm"
echo ""
echo "ğŸ“¦ ê³ ê¸‰ ìƒ˜í”Œë§ í•¨ìˆ˜:"
echo "   15. batch_sampling_from_probs"
echo "   16. chain_speculative_sampling"
echo ""
echo "ğŸ“¦ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜:"
echo "   17. normalize_probs"
echo "   18. filter_low_probs"
echo "   19. compute_entropy"

echo ""
echo -e "${PURPLE}ğŸ¯ í•´ê²°ëœ í•µì‹¬ ë¬¸ì œ:${NC}"
echo "1. âœ… SGLangì´ ìš”êµ¬í•˜ëŠ” top_k_top_p_sampling_from_probs í•¨ìˆ˜ ì™„ì „ êµ¬í˜„"
echo "2. âœ… ëª¨ë“  ìƒ˜í”Œë§ ë°©ì‹ (probs/logits ê¸°ë°˜) ì™„ì „ ì§€ì›"
echo "3. âœ… SGLang ì„œë²„ ëª¨ë“ˆ import ì˜¤ë¥˜ ì™„ì „ í•´ê²°"
echo "4. âœ… FlashInfer sampling ëª¨ë“ˆ ì™„ì „ì„± ë‹¬ì„±"

echo ""
echo "FlashInfer ëˆ„ë½ í•¨ìˆ˜ í•´ê²° ì™„ë£Œ ì‹œê°„: $(date)"