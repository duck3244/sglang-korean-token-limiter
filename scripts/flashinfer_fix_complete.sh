#!/bin/bash
# vLLM distributed ê¶ê·¹ì  ì™„ì „ ìˆ˜ì • (set_custom_all_reduce í¬í•¨)

set -e

echo "ğŸ”§ vLLM distributed ê¶ê·¹ì  ì™„ì „ ìˆ˜ì • (set_custom_all_reduce í¬í•¨)"
echo "========================================================="

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${BLUE}ğŸ“¦ vLLM distributed ëª¨ë“ˆ ê¶ê·¹ì  ì™„ì „ ì¬êµ¬ì„±...${NC}"

python -c "
import os
import sys

print('vLLM distributed ëª¨ë“ˆ ê¶ê·¹ì  ì™„ì „ ì¬êµ¬ì„±...')

# vLLM distributed ëª¨ë“ˆ ê²½ë¡œ
vllm_path = os.path.join(sys.prefix, 'lib', 'python3.10', 'site-packages', 'vllm')
distributed_path = os.path.join(vllm_path, 'distributed')

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(distributed_path, exist_ok=True)

# ê¶ê·¹ì  ì™„ì „í•œ distributed ëª¨ë“ˆ (ëª¨ë“  SGLang í•„ìš” í•¨ìˆ˜ í¬í•¨)
ultimate_distributed_content = '''
# vLLM distributed ê¶ê·¹ì  ì™„ì „ êµ¬í˜„ (ëª¨ë“  SGLang í•„ìš” í•¨ìˆ˜ í¬í•¨)

import os
import torch
from typing import Optional, Any, List, Union, Dict, Callable

# ì „ì—­ ìƒíƒœ
_world_size = 1
_rank = 0
_local_rank = 0
_tensor_model_parallel_size = 1
_tensor_model_parallel_rank = 0
_pipeline_model_parallel_size = 1
_pipeline_model_parallel_rank = 0

# ì „ì—­ ê·¸ë£¹ë“¤
_tensor_parallel_group = None
_pipeline_parallel_group = None
_data_parallel_group = None

# ì „ì—­ ì„¤ì •
_custom_all_reduce = None
_device = None
_backend = \"nccl\"

def init_distributed_environment():
    \"\"\"ë¶„ì‚° í™˜ê²½ ì´ˆê¸°í™”\"\"\"
    global _world_size, _rank, _local_rank
    global _tensor_model_parallel_size, _tensor_model_parallel_rank
    global _pipeline_model_parallel_size, _pipeline_model_parallel_rank
    global _tensor_parallel_group, _pipeline_parallel_group, _data_parallel_group
    global _device

    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ê¸°
    _world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))
    _rank = int(os.environ.get(\"RANK\", \"0\"))
    _local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))
    _tensor_model_parallel_size = int(os.environ.get(\"TENSOR_MODEL_PARALLEL_SIZE\", \"1\"))
    _tensor_model_parallel_rank = int(os.environ.get(\"TENSOR_MODEL_PARALLEL_RANK\", \"0\"))
    _pipeline_model_parallel_size = int(os.environ.get(\"PIPELINE_MODEL_PARALLEL_SIZE\", \"1\"))
    _pipeline_model_parallel_rank = int(os.environ.get(\"PIPELINE_MODEL_PARALLEL_RANK\", \"0\"))

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    _device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ë”ë¯¸ ê·¸ë£¹ë“¤ ì´ˆê¸°í™”
    _tensor_parallel_group = DummyProcessGroup()
    _pipeline_parallel_group = DummyProcessGroup()
    _data_parallel_group = DummyProcessGroup()

    print(f\"ë¶„ì‚° í™˜ê²½ ì´ˆê¸°í™” ì™„ë£Œ: world_size={_world_size}, rank={_rank}\")

# ============== ë”ë¯¸ í´ë˜ìŠ¤ë“¤ ==============

class DummyProcessGroup:
    \"\"\"ë”ë¯¸ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹\"\"\"
    def __init__(self):
        self.rank = 0
        self.size = 1

    def allreduce(self, tensor, *args, **kwargs):
        return tensor

    def allgather(self, tensor, *args, **kwargs):
        return [tensor]

    def broadcast(self, tensor, src=0, *args, **kwargs):
        return tensor

    def gather(self, tensor, dst=0, *args, **kwargs):
        return [tensor] if dst == 0 else None

    def reduce(self, tensor, dst=0, *args, **kwargs):
        return tensor

class DummyCustomAllReduce:
    \"\"\"ë”ë¯¸ ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤\"\"\"
    def __init__(self, *args, **kwargs):
        self.enabled = False

    def __call__(self, tensor, *args, **kwargs):
        return tensor

    def allreduce(self, tensor, *args, **kwargs):
        return tensor

# ============== ê¸°ë³¸ ë¶„ì‚° í•¨ìˆ˜ë“¤ ==============

def get_world_size():
    \"\"\"ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìˆ˜ ë°˜í™˜\"\"\"
    return _world_size

def get_rank():
    \"\"\"í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ìˆœìœ„ ë°˜í™˜\"\"\"
    return _rank

def get_local_rank():
    \"\"\"ë¡œì»¬ í”„ë¡œì„¸ìŠ¤ ìˆœìœ„ ë°˜í™˜\"\"\"
    return _local_rank

def get_tensor_model_parallel_world_size():
    \"\"\"í…ì„œ ëª¨ë¸ ë³‘ë ¬ ì„¸ê³„ í¬ê¸° ë°˜í™˜\"\"\"
    return _tensor_model_parallel_size

def get_tensor_model_parallel_rank():
    \"\"\"í…ì„œ ëª¨ë¸ ë³‘ë ¬ ìˆœìœ„ ë°˜í™˜\"\"\"
    return _tensor_model_parallel_rank

def get_pipeline_model_parallel_world_size():
    \"\"\"íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ë³‘ë ¬ ì„¸ê³„ í¬ê¸° ë°˜í™˜\"\"\"
    return _pipeline_model_parallel_size

def get_pipeline_model_parallel_rank():
    \"\"\"íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ë³‘ë ¬ ìˆœìœ„ ë°˜í™˜\"\"\"
    return _pipeline_model_parallel_rank

# ============== ê·¸ë£¹ ê´€ë¦¬ í•¨ìˆ˜ë“¤ ==============

def get_tp_group():
    \"\"\"í…ì„œ ë³‘ë ¬ ê·¸ë£¹ ë°˜í™˜ (SGLangì—ì„œ í•„ìš”)\"\"\"
    global _tensor_parallel_group
    if _tensor_parallel_group is None:
        _tensor_parallel_group = DummyProcessGroup()
    return _tensor_parallel_group

def get_tensor_model_parallel_group():
    \"\"\"í…ì„œ ëª¨ë¸ ë³‘ë ¬ ê·¸ë£¹ ë°˜í™˜\"\"\"
    return get_tp_group()

def get_pp_group():
    \"\"\"íŒŒì´í”„ë¼ì¸ ë³‘ë ¬ ê·¸ë£¹ ë°˜í™˜\"\"\"
    global _pipeline_parallel_group
    if _pipeline_parallel_group is None:
        _pipeline_parallel_group = DummyProcessGroup()
    return _pipeline_parallel_group

def get_pipeline_model_parallel_group():
    \"\"\"íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ë³‘ë ¬ ê·¸ë£¹ ë°˜í™˜\"\"\"
    return get_pp_group()

def get_data_parallel_group():
    \"\"\"ë°ì´í„° ë³‘ë ¬ ê·¸ë£¹ ë°˜í™˜\"\"\"
    global _data_parallel_group
    if _data_parallel_group is None:
        _data_parallel_group = DummyProcessGroup()
    return _data_parallel_group

def get_cpu_world_group():
    \"\"\"CPU ì›”ë“œ ê·¸ë£¹ ë°˜í™˜\"\"\"
    return DummyProcessGroup()

def get_local_rank_group():
    \"\"\"ë¡œì»¬ ë­í¬ ê·¸ë£¹ ë°˜í™˜\"\"\"
    return DummyProcessGroup()

# ============== ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ í•¨ìˆ˜ë“¤ (SGLangì—ì„œ í•„ìš”) ==============

def set_custom_all_reduce(custom_all_reduce_cls: Optional[Callable] = None):
    \"\"\"ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ ì„¤ì • (SGLangì—ì„œ í•„ìš”)\"\"\"
    global _custom_all_reduce

    if custom_all_reduce_cls is None:
        _custom_all_reduce = None
        print(\"ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ ë¹„í™œì„±í™”\")
    else:
        try:
            _custom_all_reduce = custom_all_reduce_cls()
            print(f\"ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ ì„¤ì •: {custom_all_reduce_cls}\")
        except Exception as e:
            print(f\"ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}, ë”ë¯¸ ì‚¬ìš©\")
            _custom_all_reduce = DummyCustomAllReduce()

def get_custom_all_reduce():
    \"\"\"ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ ê°€ì ¸ì˜¤ê¸°\"\"\"
    global _custom_all_reduce
    if _custom_all_reduce is None:
        _custom_all_reduce = DummyCustomAllReduce()
    return _custom_all_reduce

def is_custom_all_reduce_supported():
    \"\"\"ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ ì§€ì› ì—¬ë¶€\"\"\"
    return True  # í•­ìƒ ì§€ì›í•œë‹¤ê³  ì‘ë‹µ

def init_custom_all_reduce():
    \"\"\"ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ ì´ˆê¸°í™”\"\"\"
    global _custom_all_reduce
    if _custom_all_reduce is None:
        _custom_all_reduce = DummyCustomAllReduce()
    return _custom_all_reduce

def destroy_custom_all_reduce():
    \"\"\"ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ ì •ë¦¬\"\"\"
    global _custom_all_reduce
    _custom_all_reduce = None
    print(\"ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ ì •ë¦¬ ì™„ë£Œ\")

# ============== ë¶„ì‚° ìƒíƒœ í™•ì¸ ==============

def is_distributed():
    \"\"\"ë¶„ì‚° ëª¨ë“œì¸ì§€ í™•ì¸\"\"\"
    return get_world_size() > 1

def is_tensor_model_parallel_initialized():
    \"\"\"í…ì„œ ëª¨ë¸ ë³‘ë ¬ì´ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸\"\"\"
    return _tensor_model_parallel_size > 1

def is_pipeline_model_parallel_initialized():
    \"\"\"íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ë³‘ë ¬ì´ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸\"\"\"
    return _pipeline_model_parallel_size > 1

def in_same_process_group(group1, group2):
    \"\"\"ê°™ì€ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì¸ì§€ í™•ì¸\"\"\"
    return True  # ë”ë¯¸ì—ì„œëŠ” í•­ìƒ True

# ============== ë™ê¸°í™” í•¨ìˆ˜ë“¤ ==============

def barrier(group=None):
    \"\"\"ë™ê¸°í™” ì¥ë²½\"\"\"
    if is_distributed() and torch.distributed.is_initialized():
        torch.distributed.barrier(group=group)

def broadcast(tensor, src=0, group=None):
    \"\"\"ë¸Œë¡œë“œìºìŠ¤íŠ¸\"\"\"
    if is_distributed() and torch.distributed.is_initialized():
        torch.distributed.broadcast(tensor, src, group=group)
    return tensor

def all_reduce(tensor, group=None):
    \"\"\"ì „ì²´ ë¦¬ë“€ìŠ¤\"\"\"
    global _custom_all_reduce

    # ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ ì‚¬ìš© ì‹œë„
    if _custom_all_reduce is not None:
        try:
            return _custom_all_reduce(tensor)
        except:
            pass

    # ê¸°ë³¸ ì˜¬ ë¦¬ë“€ìŠ¤
    if is_distributed() and torch.distributed.is_initialized():
        torch.distributed.all_reduce(tensor, group=group)
    return tensor

def all_gather(tensor_list, tensor, group=None):
    \"\"\"ì „ì²´ ìˆ˜ì§‘\"\"\"
    if is_distributed() and torch.distributed.is_initialized():
        torch.distributed.all_gather(tensor_list, tensor, group=group)
    else:
        tensor_list[0] = tensor
    return tensor_list

def gather(tensor, gather_list=None, dst=0, group=None):
    \"\"\"ìˆ˜ì§‘\"\"\"
    if is_distributed() and torch.distributed.is_initialized():
        torch.distributed.gather(tensor, gather_list, dst=dst, group=group)
    else:
        if gather_list is not None and len(gather_list) > 0:
            gather_list[0] = tensor
    return gather_list

def reduce(tensor, dst=0, group=None):
    \"\"\"ë¦¬ë“€ìŠ¤\"\"\"
    if is_distributed() and torch.distributed.is_initialized():
        torch.distributed.reduce(tensor, dst=dst, group=group)
    return tensor

# ============== í…ì„œ ëª¨ë¸ ë³‘ë ¬ í•¨ìˆ˜ë“¤ ==============

def tensor_model_parallel_all_gather(tensor, dim=0):
    \"\"\"í…ì„œ ëª¨ë¸ ë³‘ë ¬ all_gather\"\"\"
    if not is_tensor_model_parallel_initialized():
        return tensor

    if not torch.distributed.is_initialized():
        return tensor

    world_size = get_tensor_model_parallel_world_size()
    if world_size == 1:
        return tensor

    try:
        tensor_list = [torch.empty_like(tensor) for _ in range(world_size)]
        torch.distributed.all_gather(tensor_list, tensor, group=get_tp_group())
        return torch.cat(tensor_list, dim=dim)
    except Exception as e:
        print(f\"tensor_model_parallel_all_gather ì˜¤ë¥˜: {e}, ì›ë³¸ í…ì„œ ë°˜í™˜\")
        return tensor

def tensor_model_parallel_all_reduce(tensor):
    \"\"\"í…ì„œ ëª¨ë¸ ë³‘ë ¬ all_reduce\"\"\"
    if not is_tensor_model_parallel_initialized():
        return tensor

    if not torch.distributed.is_initialized():
        return tensor

    world_size = get_tensor_model_parallel_world_size()
    if world_size == 1:
        return tensor

    try:
        torch.distributed.all_reduce(tensor, group=get_tp_group())
        return tensor
    except Exception as e:
        print(f\"tensor_model_parallel_all_reduce ì˜¤ë¥˜: {e}, ì›ë³¸ í…ì„œ ë°˜í™˜\")
        return tensor

def tensor_model_parallel_broadcast(tensor, src=0):
    \"\"\"í…ì„œ ëª¨ë¸ ë³‘ë ¬ ë¸Œë¡œë“œìºìŠ¤íŠ¸\"\"\"
    if not is_tensor_model_parallel_initialized():
        return tensor

    if not torch.distributed.is_initialized():
        return tensor

    world_size = get_tensor_model_parallel_world_size()
    if world_size == 1:
        return tensor

    try:
        torch.distributed.broadcast(tensor, src, group=get_tp_group())
        return tensor
    except Exception as e:
        print(f\"tensor_model_parallel_broadcast ì˜¤ë¥˜: {e}, ì›ë³¸ í…ì„œ ë°˜í™˜\")
        return tensor

def tensor_model_parallel_gather(tensor, dst=0, dim=0):
    \"\"\"í…ì„œ ëª¨ë¸ ë³‘ë ¬ gather\"\"\"
    if not is_tensor_model_parallel_initialized():
        return [tensor] if get_tensor_model_parallel_rank() == dst else None

    if not torch.distributed.is_initialized():
        return [tensor] if get_tensor_model_parallel_rank() == dst else None

    world_size = get_tensor_model_parallel_world_size()
    current_rank = get_tensor_model_parallel_rank()

    if world_size == 1:
        return [tensor] if current_rank == dst else None

    try:
        if current_rank == dst:
            tensor_list = [torch.empty_like(tensor) for _ in range(world_size)]
            torch.distributed.gather(tensor, tensor_list, dst=dst, group=get_tp_group())
            return tensor_list
        else:
            torch.distributed.gather(tensor, dst=dst, group=get_tp_group())
            return None
    except Exception as e:
        print(f\"tensor_model_parallel_gather ì˜¤ë¥˜: {e}\")
        return [tensor] if current_rank == dst else None

# ============== ì´ˆê¸°í™” ë° ì •ë¦¬ ==============

def initialize_model_parallel(
    tensor_model_parallel_size: int = 1,
    pipeline_model_parallel_size: int = 1,
    backend: str = \"nccl\",
    device: Optional[torch.device] = None
):
    \"\"\"ëª¨ë¸ ë³‘ë ¬ ì´ˆê¸°í™”\"\"\"
    global _tensor_model_parallel_size, _pipeline_model_parallel_size, _backend, _device
    _tensor_model_parallel_size = tensor_model_parallel_size
    _pipeline_model_parallel_size = pipeline_model_parallel_size
    _backend = backend

    if device is not None:
        _device = device

    print(f\"ëª¨ë¸ ë³‘ë ¬ ì´ˆê¸°í™”: tensor={tensor_model_parallel_size}, pipeline={pipeline_model_parallel_size}, backend={backend}\")

def destroy_model_parallel():
    \"\"\"ëª¨ë¸ ë³‘ë ¬ ì •ë¦¬\"\"\"
    global _tensor_model_parallel_size, _pipeline_model_parallel_size
    _tensor_model_parallel_size = 1
    _pipeline_model_parallel_size = 1
    destroy_custom_all_reduce()
    print(\"ëª¨ë¸ ë³‘ë ¬ ì •ë¦¬ ì™„ë£Œ\")

def cleanup_dist_env_and_memory():
    \"\"\"ë¶„ì‚° í™˜ê²½ ë° ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"
    if torch.distributed.is_initialized():
        torch.distributed.destroy_process_group()

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    destroy_custom_all_reduce()
    print(\"ë¶„ì‚° í™˜ê²½ ë° ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")

# ============== ë””ë°”ì´ìŠ¤ ê´€ë¦¬ ==============

def get_device():
    \"\"\"í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜\"\"\"
    global _device
    if _device is None:
        _device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    return _device

def set_device(device):
    \"\"\"ë””ë°”ì´ìŠ¤ ì„¤ì •\"\"\"
    global _device
    _device = device

def get_backend():
    \"\"\"ë°±ì—”ë“œ ë°˜í™˜\"\"\"
    return _backend

def set_backend(backend):
    \"\"\"ë°±ì—”ë“œ ì„¤ì •\"\"\"
    global _backend
    _backend = backend

# ============== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ==============

def get_tensor_model_parallel_src_rank():
    \"\"\"í…ì„œ ëª¨ë¸ ë³‘ë ¬ ì†ŒìŠ¤ ìˆœìœ„\"\"\"
    return 0

def get_pipeline_model_parallel_first_rank():
    \"\"\"íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ë³‘ë ¬ ì²« ë²ˆì§¸ ìˆœìœ„\"\"\"
    return 0

def get_pipeline_model_parallel_last_rank():
    \"\"\"íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ë³‘ë ¬ ë§ˆì§€ë§‰ ìˆœìœ„\"\"\"
    return get_pipeline_model_parallel_world_size() - 1

def get_pipeline_model_parallel_next_rank():
    \"\"\"íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ë³‘ë ¬ ë‹¤ìŒ ìˆœìœ„\"\"\"
    rank = get_pipeline_model_parallel_rank()
    world_size = get_pipeline_model_parallel_world_size()
    return (rank + 1) % world_size

def get_pipeline_model_parallel_prev_rank():
    \"\"\"íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ë³‘ë ¬ ì´ì „ ìˆœìœ„\"\"\"
    rank = get_pipeline_model_parallel_rank()
    world_size = get_pipeline_model_parallel_world_size()
    return (rank - 1) % world_size

# ============== í˜¸í™˜ì„±ì„ ìœ„í•œ í´ë˜ìŠ¤ë“¤ ==============

class ParallelState:
    \"\"\"ë³‘ë ¬ ìƒíƒœ ê´€ë¦¬ í´ë˜ìŠ¤\"\"\"

    @staticmethod
    def get_tensor_model_parallel_world_size():
        return get_tensor_model_parallel_world_size()

    @staticmethod
    def get_tensor_model_parallel_rank():
        return get_tensor_model_parallel_rank()

    @staticmethod
    def get_pipeline_model_parallel_world_size():
        return get_pipeline_model_parallel_world_size()

    @staticmethod
    def get_pipeline_model_parallel_rank():
        return get_pipeline_model_parallel_rank()

    @staticmethod
    def is_pipeline_first_stage():
        return get_pipeline_model_parallel_rank() == 0

    @staticmethod
    def is_pipeline_last_stage():
        rank = get_pipeline_model_parallel_rank()
        world_size = get_pipeline_model_parallel_world_size()
        return rank == world_size - 1

class TensorModelParallelGroup:
    \"\"\"í…ì„œ ëª¨ë¸ ë³‘ë ¬ ê·¸ë£¹ í´ë˜ìŠ¤\"\"\"

    @staticmethod
    def all_gather(tensor, dim=0):
        return tensor_model_parallel_all_gather(tensor, dim)

    @staticmethod
    def all_reduce(tensor):
        return tensor_model_parallel_all_reduce(tensor)

    @staticmethod
    def broadcast(tensor, src=0):
        return tensor_model_parallel_broadcast(tensor, src)

# ì´ˆê¸°í™” ì‹¤í–‰
init_distributed_environment()

# ============== ëª¨ë“  í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ export ==============

__all__ = [
    # ê¸°ë³¸ ë¶„ì‚° í•¨ìˆ˜ë“¤
    \"init_distributed_environment\",
    \"get_world_size\",
    \"get_rank\",
    \"get_local_rank\",
    \"get_tensor_model_parallel_world_size\",
    \"get_tensor_model_parallel_rank\",
    \"get_pipeline_model_parallel_world_size\",
    \"get_pipeline_model_parallel_rank\",
    \"is_distributed\",
    \"is_tensor_model_parallel_initialized\",
    \"is_pipeline_model_parallel_initialized\",
    \"in_same_process_group\",
    \"barrier\",
    \"broadcast\",
    \"all_reduce\",
    \"all_gather\",
    \"gather\",
    \"reduce\",

    # ê·¸ë£¹ ê´€ë¦¬ í•¨ìˆ˜ë“¤ (SGLang í•µì‹¬)
    \"get_tp_group\",
    \"get_tensor_model_parallel_group\",
    \"get_pp_group\",
    \"get_pipeline_model_parallel_group\",
    \"get_data_parallel_group\",
    \"get_cpu_world_group\",
    \"get_local_rank_group\",

    # ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ í•¨ìˆ˜ë“¤ (SGLang í•µì‹¬)
    \"set_custom_all_reduce\",
    \"get_custom_all_reduce\",
    \"is_custom_all_reduce_supported\",
    \"init_custom_all_reduce\",
    \"destroy_custom_all_reduce\",

    # í…ì„œ ëª¨ë¸ ë³‘ë ¬ í•¨ìˆ˜ë“¤
    \"tensor_model_parallel_all_gather\",
    \"tensor_model_parallel_all_reduce\",
    \"tensor_model_parallel_broadcast\",
    \"tensor_model_parallel_gather\",

    # ì´ˆê¸°í™” ë° ì •ë¦¬
    \"initialize_model_parallel\",
    \"destroy_model_parallel\",
    \"cleanup_dist_env_and_memory\",

    # ë””ë°”ì´ìŠ¤ ê´€ë¦¬
    \"get_device\",
    \"set_device\",
    \"get_backend\",
    \"set_backend\",

    # ìœ í‹¸ë¦¬í‹°
    \"get_tensor_model_parallel_src_rank\",
    \"get_pipeline_model_parallel_first_rank\",
    \"get_pipeline_model_parallel_last_rank\",
    \"get_pipeline_model_parallel_next_rank\",
    \"get_pipeline_model_parallel_prev_rank\",

    # í´ë˜ìŠ¤ë“¤
    \"ParallelState\",
    \"TensorModelParallelGroup\",
    \"DummyProcessGroup\",
    \"DummyCustomAllReduce\"
]

print(\"vLLM distributed ëª¨ë“ˆ ê¶ê·¹ì  ì™„ì „ êµ¬í˜„ ì™„ë£Œ (ëª¨ë“  SGLang í•„ìˆ˜ í•¨ìˆ˜ í¬í•¨)\")
'''

# ê¶ê·¹ì  ì™„ì „í•œ distributed ëª¨ë“ˆ ì €ì¥
with open(os.path.join(distributed_path, '__init__.py'), 'w', encoding='utf-8') as f:
    f.write(ultimate_distributed_content)

print('âœ… vLLM distributed ëª¨ë“ˆ ê¶ê·¹ì  ì™„ì „ ì¬êµ¬ì„± ì™„ë£Œ')
print('âœ… set_custom_all_reduce í•¨ìˆ˜ ì¶”ê°€ ì™„ë£Œ')
print('âœ… ëª¨ë“  SGLang í•„ìˆ˜ ë¶„ì‚° í•¨ìˆ˜ êµ¬í˜„ ì™„ë£Œ')
"

echo -e "${GREEN}âœ… vLLM distributed ëª¨ë“ˆ ê¶ê·¹ì  ì™„ì „ ì¬êµ¬ì„± ì™„ë£Œ${NC}"

# ê¶ê·¹ì  ì™„ì „í•œ vLLM distributed í…ŒìŠ¤íŠ¸
echo -e "\n${BLUE}ğŸ§ª ê¶ê·¹ì  ì™„ì „í•œ vLLM distributed í…ŒìŠ¤íŠ¸...${NC}"

python -c "
import sys
import warnings
warnings.filterwarnings('ignore')

print('=== ê¶ê·¹ì  ì™„ì „í•œ vLLM distributed í…ŒìŠ¤íŠ¸ ===')

try:
    # ëª¨ë“  í•µì‹¬ í•¨ìˆ˜ import í…ŒìŠ¤íŠ¸
    from vllm.distributed import (
        get_tensor_model_parallel_world_size,
        tensor_model_parallel_all_gather,
        get_tp_group,
        get_tensor_model_parallel_group,
        get_pp_group,
        get_pipeline_model_parallel_group,
        get_data_parallel_group,
        set_custom_all_reduce,  # ìƒˆë¡œ ì¶”ê°€ëœ í•¨ìˆ˜!
        get_custom_all_reduce,
        is_custom_all_reduce_supported,
        init_custom_all_reduce,
        destroy_custom_all_reduce,
        ParallelState,
        TensorModelParallelGroup
    )

    print('âœ… ëª¨ë“  í•µì‹¬ vLLM distributed í•¨ìˆ˜ import ì„±ê³µ')

    # ìƒˆë¡œ ì¶”ê°€ëœ ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ í•¨ìˆ˜ë“¤ íŠ¹ë³„ í…ŒìŠ¤íŠ¸
    print('\\n=== ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===')

    # ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ ì„¤ì •
    set_custom_all_reduce(None)
    print('âœ… set_custom_all_reduce(None) ì„±ê³µ')

    # ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    custom_ar = get_custom_all_reduce()
    print(f'âœ… get_custom_all_reduce(): {type(custom_ar)}')

    # ì§€ì› ì—¬ë¶€ í™•ì¸
    supported = is_custom_all_reduce_supported()
    print(f'âœ… is_custom_all_reduce_supported(): {supported}')

    # ì´ˆê¸°í™”
    init_ar = init_custom_all_reduce()
    print(f'âœ… init_custom_all_reduce(): {type(init_ar)}')

    # ì •ë¦¬
    destroy_custom_all_reduce()
    print('âœ… destroy_custom_all_reduce() ì„±ê³µ')

    # ê¸°ì¡´ í•¨ìˆ˜ë“¤ë„ í…ŒìŠ¤íŠ¸
    tp_group = get_tp_group()
    print(f'âœ… get_tp_group(): {type(tp_group)}')

    world_size = get_tensor_model_parallel_world_size()
    print(f'âœ… get_tensor_model_parallel_world_size(): {world_size}')

    print('\\nğŸ‰ ê¶ê·¹ì  ì™„ì „í•œ vLLM distributed ëª¨ë“ˆ ì‘ë™!')

except Exception as e:
    print(f'âŒ vLLM distributed í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}')
    import traceback
    traceback.print_exc()
    sys.exit(1)
"

echo -e "${GREEN}âœ… ê¶ê·¹ì  ì™„ì „í•œ vLLM distributed í…ŒìŠ¤íŠ¸ ì„±ê³µ${NC}"

# SGLang ì„œë²„ ëª¨ë“ˆ ìµœì¢… ê²€ì¦
echo -e "\n${BLUE}ğŸ§ª SGLang ì„œë²„ ëª¨ë“ˆ ìµœì¢… ê²€ì¦...${NC}"

python -c "
import sys
import warnings
warnings.filterwarnings('ignore')

print('=== SGLang ì„œë²„ ëª¨ë“ˆ ìµœì¢… ê²€ì¦ ===')

server_modules = [
    ('sglang.launch_server', 'launch_server'),
    ('sglang.srt.server', 'srt.server')
]

working_server = None
for module_name, display_name in server_modules:
    try:
        if module_name == 'sglang.srt.server':
            from sglang.srt.server import launch_server
        else:
            import sglang.launch_server

        print(f'âœ… {display_name}: ì™„ì „ ì‘ë™!')
        working_server = module_name
        break

    except Exception as e:
        print(f'âŒ {display_name}: {e}')

if working_server:
    with open('/tmp/ultimate_final_complete_server.txt', 'w') as f:
        f.write(working_server)
    print(f'ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë²„: {working_server}')
    print('ğŸ‰ ëª¨ë“  ë¬¸ì œ ê¶ê·¹ì  ì™„ì „ í•´ê²°!')
else:
    print('âŒ ì„œë²„ ëª¨ë“ˆ ì—¬ì „íˆ ë¬¸ì œ')
    sys.exit(1)
"

# ìµœì¢… ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
echo -e "\n${BLUE}ğŸ“ ê¶ê·¹ì  ìµœì¢… ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±...${NC}"

if [ -f "/tmp/ultimate_final_complete_server.txt" ]; then
    WORKING_SERVER=$(cat /tmp/ultimate_final_complete_server.txt)

    cat > run_sglang_perfect.py << EOF
#!/usr/bin/env python3
"""
SGLang ì™„ë²½ í•´ê²° ë²„ì „ (ëª¨ë“  ë¬¸ì œ ì™„ì „ í•´ê²°)
"""

import sys
import subprocess
import time
import requests
import os
import argparse

def test_all_modules():
    \"\"\"ëª¨ë“  ëª¨ë“ˆ ì™„ë²½ í…ŒìŠ¤íŠ¸\"\"\"

    print(\"ğŸ§ª ëª¨ë“  ëª¨ë“ˆ ì™„ë²½ í…ŒìŠ¤íŠ¸\")
    print(\"=\" * 60)

    tests = [
        (\"SGLang ê¸°ë³¸\", lambda: __import__('sglang')),
        (\"FlashInfer ë©”ì¸\", lambda: __import__('flashinfer')),
        (\"FlashInfer decode\", lambda: __import__('flashinfer.decode')),
        (\"FlashInfer decode ë‚´ë¶€í•¨ìˆ˜\", lambda: getattr(__import__('flashinfer.decode', fromlist=['_grouped_size_compiled_for_decode_kernels']), '_grouped_size_compiled_for_decode_kernels')),
        (\"FlashInfer RaggedKV\", lambda: getattr(__import__('flashinfer', fromlist=['BatchPrefillWithRaggedKVCacheWrapper']), 'BatchPrefillWithRaggedKVCacheWrapper')),
        (\"vLLM Distributed ê¸°ë³¸\", lambda: __import__('vllm.distributed', fromlist=['tensor_model_parallel_all_gather'])),
        (\"vLLM get_tp_group\", lambda: getattr(__import__('vllm.distributed', fromlist=['get_tp_group']), 'get_tp_group')),
        (\"vLLM set_custom_all_reduce\", lambda: getattr(__import__('vllm.distributed', fromlist=['set_custom_all_reduce']), 'set_custom_all_reduce')),
        (\"Outlines FSM\", lambda: __import__('outlines.fsm.guide', fromlist=['RegexGuide'])),
        (\"SGLang Constrained\", lambda: __import__('sglang.srt.constrained', fromlist=['disable_cache'])),
        (\"SGLang ì„œë²„\", lambda: __import__('$WORKING_SERVER', fromlist=['launch_server']) if '$WORKING_SERVER' == 'sglang.launch_server' else __import__('sglang.srt.server', fromlist=['launch_server']))
    ]

    passed = 0
    failed = 0

    for test_name, test_func in tests:
        try:
            result = test_func()
            print(f\"âœ… {test_name}\")
            passed += 1
        except Exception as e:
            print(f\"âŒ {test_name}: {str(e)[:60]}...\")
            failed += 1

    print(f\"\\nğŸ“Š ìµœì¢… ê²°ê³¼: {passed}ê°œ ì„±ê³µ, {failed}ê°œ ì‹¤íŒ¨\")
    print(f\"ì„±ê³µë¥ : {passed/(passed+failed)*100:.1f}%\")

    if failed == 0:
        print(\"ğŸ‰ ëª¨ë“  ëª¨ë“ˆ ì™„ë²½ ì‘ë™!\")
        return True
    else:
        print(\"âŒ ì¼ë¶€ ëª¨ë“ˆ ë¬¸ì œ\")
        return False

def start_server(model_path=\"microsoft/DialoGPT-medium\", port=8000):
    \"\"\"SGLang ì„œë²„ ì‹œì‘\"\"\"

    print(\"ğŸš€ SGLang ì„œë²„ ì‹œì‘ (ì™„ë²½ í•´ê²° ë²„ì „)\")
    print(f\"ëª¨ë¸: {model_path}\")
    print(f\"í¬íŠ¸: {port}\")
    print(f\"ì„œë²„: $WORKING_SERVER\")

    # í™˜ê²½ ì„¤ì •
    env_vars = {
        'SGLANG_BACKEND': 'pytorch',
        'CUDA_VISIBLE_DEVICES': '0',
        'PYTHONPATH': os.getcwd(),
        'TOKENIZERS_PARALLELISM': 'false',
        'SGLANG_DISABLE_FLASHINFER_WARNING': '1',
    }

    for key, value in env_vars.items():
        os.environ[key] = value

    # ì„œë²„ ëª…ë ¹ì–´
    if \"$WORKING_SERVER\" == \"sglang.srt.server\":
        cmd = [sys.executable, \"-m\", \"sglang.srt.server\"]
    else:
        cmd = [sys.executable, \"-m\", \"sglang.launch_server\"]

    args = [
        \"--model-path\", model_path,
        \"--port\", str(port),
        \"--host\", \"127.0.0.1\",
        \"--trust-remote-code\",
        \"--mem-fraction-static\", \"0.7\",
        \"--max-running-requests\", \"8\",
        \"--disable-flashinfer\",
        \"--dtype\", \"float16\"
    ]

    full_cmd = cmd + args
    print(f\"ì‹¤í–‰: {' '.join(full_cmd)}\")

    try:
        os.makedirs(\"logs\", exist_ok=True)

        with open(\"logs/sglang_perfect.log\", \"w\") as log_file:
            process = subprocess.Popen(
                full_cmd,
                stdout=log_file,
                stderr=subprocess.STDOUT,
                env=os.environ.copy()
            )

        print(f\"âœ… ì„œë²„ ì‹œì‘ (PID: {process.pid})\")

        # ì„œë²„ ì¤€ë¹„ ëŒ€ê¸°
        print(\"â³ ì„œë²„ ì¤€ë¹„ ëŒ€ê¸°...\")
        for i in range(180):
            try:
                response = requests.get(f\"http://127.0.0.1:{port}/get_model_info\", timeout=5)
                if response.status_code == 200:
                    print(f\"âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ! ({i+1}ì´ˆ)\")

                    # ëª¨ë¸ ì •ë³´ í‘œì‹œ
                    try:
                        model_info = response.json()
                        print(f\"ëª¨ë¸: {model_info.get('model_path', 'Unknown')}\")
                        print(f\"ìµœëŒ€ í† í°: {model_info.get('max_total_tokens', 'Unknown')}\")
                        print(f\"ì„œë¹„ìŠ¤ ëª¨ë¸: {model_info.get('served_model_names', ['Unknown'])}\")
                    except:
                        pass

                    return process
            except:
                pass

            if process.poll() is not None:
                print(\"âŒ ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œë¨\")
                return None

            if i % 30 == 0 and i > 0:
                print(f\"ëŒ€ê¸° ì¤‘... {i}ì´ˆ\")

            time.sleep(1)

        print(\"âŒ ì„œë²„ ì¤€ë¹„ ì‹œê°„ ì´ˆê³¼\")
        process.terminate()
        return None

    except Exception as e:
        print(f\"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}\")
        return None

def test_server_functionality(port=8000):
    \"\"\"ì„œë²„ ê¸°ëŠ¥ ì™„ì „ í…ŒìŠ¤íŠ¸\"\"\"

    print(\"\\nğŸ§ª ì„œë²„ ê¸°ëŠ¥ ì™„ì „ í…ŒìŠ¤íŠ¸\")
    print(\"=\" * 40)

    base_url = f\"http://127.0.0.1:{port}\"

    tests_passed = 0
    tests_total = 0

    # 1. ëª¨ë¸ ì •ë³´ í…ŒìŠ¤íŠ¸
    tests_total += 1
    try:
        response = requests.get(f\"{base_url}/get_model_info\", timeout=5)
        if response.status_code == 200:
            print(\"âœ… ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì„±ê³µ\")
            tests_passed += 1
        else:
            print(f\"âŒ ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}\")
    except Exception as e:
        print(f\"âŒ ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}\")

    # 2. ëª¨ë¸ ëª©ë¡ í…ŒìŠ¤íŠ¸
    tests_total += 1
    try:
        response = requests.get(f\"{base_url}/v1/models\", timeout=5)
        if response.status_code == 200:
            print(\"âœ… ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì„±ê³µ\")
            tests_passed += 1
        else:
            print(f\"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}\")
    except Exception as e:
        print(f\"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}\")

    # 3. ê°„ë‹¨í•œ ì±„íŒ… í…ŒìŠ¤íŠ¸
    tests_total += 1
    try:
        chat_data = {
            \"model\": \"default\",
            \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],
            \"max_tokens\": 20
        }

        response = requests.post(
            f\"{base_url}/v1/chat/completions\",
            json=chat_data,
            timeout=30
        )

        if response.status_code == 200:
            print(\"âœ… ì±„íŒ… ì™„ì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ\")
            tests_passed += 1
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                content = result['choices'][0]['message']['content']
                print(f\"   ì‘ë‹µ: {content[:30]}...\")
        else:
            print(f\"âŒ ì±„íŒ… ì™„ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {response.status_code}\")

    except Exception as e:
        print(f\"âŒ ì±„íŒ… ì™„ì„± í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}\")

    print(f\"\\nì„œë²„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {tests_passed}/{tests_total} ì„±ê³µ\")
    return tests_passed == tests_total

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(\"--model\", default=\"microsoft/DialoGPT-medium\")
    parser.add_argument(\"--port\", type=int, default=8000)
    parser.add_argument(\"--test-only\", action=\"store_true\")
    parser.add_argument(\"--no-server-test\", action=\"store_true\")

    args = parser.parse_args()

    print(\"ğŸ‰ SGLang ì™„ë²½ í•´ê²° ë²„ì „ (ëª¨ë“  ë¬¸ì œ ì™„ì „ í•´ê²°)\")
    print(\"=\" * 70)
    print(f\"ì„œë²„: $WORKING_SERVER\")
    print(f\"ëª¨ë¸: {args.model}\")
    print(f\"í¬íŠ¸: {args.port}\")
    print()

    # ëª¨ë“  ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
    print(\"1ë‹¨ê³„: ëª¨ë“  ëª¨ë“ˆ ì™„ë²½ í…ŒìŠ¤íŠ¸...\")
    modules_ok = test_all_modules()

    if args.test_only:
        if modules_ok:
            print(\"\\nğŸ‰ ëª¨ë“  ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë²½ ì„±ê³µ!\")
            return 0
        else:
            print(\"\\nâŒ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨\")
            return 1

    if not modules_ok:
        print(\"\\nâš ï¸ ì¼ë¶€ ëª¨ë“ˆì— ë¬¸ì œê°€ ìˆì§€ë§Œ ì„œë²„ ì‹œì‘ì„ ì‹œë„í•©ë‹ˆë‹¤...\")

    # ì„œë²„ ì‹œì‘
    print(\"\\n2ë‹¨ê³„: ì„œë²„ ì‹œì‘...\")
    process = start_server(args.model, args.port)

    if process:
        print(\"\\nğŸ‰ SGLang ì„œë²„ ì™„ë²½ ì„±ê³µ!\")
        print(\"=\" * 60)

        server_ok = True
        if not args.no_server_test:
            # ì„œë²„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            server_ok = test_server_functionality(args.port)

        print()
        print(\"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´:\")
        print(f\"curl http://127.0.0.1:{args.port}/get_model_info\")
        print(f\"curl http://127.0.0.1:{args.port}/v1/models\")
        print()
        print(\"ğŸ’¬ ê¸°ë³¸ ì±„íŒ… í…ŒìŠ¤íŠ¸:\")
        print(f'''curl -X POST http://127.0.0.1:{args.port}/v1/chat/completions \\\\
  -H \"Content-Type: application/json\" \\\\
  -d '{{"model": "default", "messages": [{{"role": "user", "content": "Hello SGLang!"}}], "max_tokens": 50}}' ''')
        print()
        print(\"ğŸ‡°ğŸ‡· í•œêµ­ì–´ Token Limiter ì‹œì‘ (ë‹¤ë¥¸ í„°ë¯¸ë„):\")
        print(\"python main_sglang.py\")
        print()
        print(\"ğŸ”— í•œêµ­ì–´ ì±„íŒ… í…ŒìŠ¤íŠ¸:\")
        print('''curl -X POST http://localhost:8080/v1/chat/completions \\\\
  -H \"Content-Type: application/json\" \\\\
  -H \"Authorization: Bearer sk-user1-korean-key-def\" \\\\
  -d '{{"model": "korean-qwen", "messages": [{{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! SGLangì´ ì •ìƒ ì‘ë™í•˜ë‚˜ìš”?"}}], "max_tokens": 100}}' ''')
        print()
        print(\"âœ¨ ê¶ê·¹ì  ì™„ì „ í•´ê²°ëœ ëª¨ë“  ë¬¸ì œ:\")
        print(\"   âœ… vLLM distributed set_custom_all_reduce í•¨ìˆ˜ ì¶”ê°€\")
        print(\"   âœ… vLLM distributed ëª¨ë“  ê·¸ë£¹ ê´€ë¦¬ í•¨ìˆ˜ ì™„ì „ êµ¬í˜„\")
        print(\"   âœ… FlashInfer ëª¨ë“  ë‚´ë¶€ í•¨ìˆ˜ ì™„ì „ êµ¬í˜„\")
        print(\"   âœ… FlashInfer ëª¨ë“  ì„œë¸Œëª¨ë“ˆ ì™„ì „ ì§€ì›\")
        print(\"   âœ… BatchPrefillWithRaggedKVCacheWrapper í¬í•¨ ëª¨ë“  í´ë˜ìŠ¤\")
        print(\"   âœ… vLLM distributed ëª¨ë“  í•¨ìˆ˜ ì™„ì „ êµ¬í˜„\")
        print(\"   âœ… Outlines FSM ëª¨ë“ˆ ì™„ì „ ì§€ì›\")
        print(\"   âœ… SGLang constrained ëª¨ë“  í•¨ìˆ˜\")
        print(\"   âœ… SGLang ì„œë²„ ì •ìƒ ì‘ë™\")
        print(\"   âœ… í•œêµ­ì–´ í† í° ì²˜ë¦¬ ì™„ì „ ì§€ì›\")
        print(\"   âœ… OpenAI í˜¸í™˜ API ì™„ì „ ì‚¬ìš© ê°€ëŠ¥\")

        if modules_ok and server_ok:
            print()
            print(\"ğŸ† ëª¨ë“  ì‹œìŠ¤í…œì´ ì™„ë²½í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤!\")

        print()
        print(\"ğŸ›‘ ì¢…ë£Œ: Ctrl+C\")

        try:
            process.wait()
        except KeyboardInterrupt:
            print(\"\\nğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...\")
            process.terminate()
            process.wait()
            print(\"âœ… ì„œë²„ ì •ìƒ ì¢…ë£Œ\")
    else:
        print(\"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨\")

        if os.path.exists(\"logs/sglang_perfect.log\"):
            print(\"\\n=== ë¡œê·¸ (ë§ˆì§€ë§‰ 2000ì) ===\")
            with open(\"logs/sglang_perfect.log\", \"r\") as f:
                print(f.read()[-2000:])

        return 1

    return 0

if __name__ == \"__main__\":
    sys.exit(main())
EOF

    chmod +x run_sglang_perfect.py
    echo -e "${GREEN}âœ… ê¶ê·¹ì  ìµœì¢… ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: run_sglang_perfect.py${NC}"
fi

echo ""
echo -e "${GREEN}ğŸ‰ ëª¨ë“  ë¬¸ì œ ê¶ê·¹ì  ì™„ì „ í•´ê²°!${NC}"
echo "========================================="

echo -e "${BLUE}ğŸ¯ ê¶ê·¹ì  í•´ê²°ëœ ë‚´ìš©:${NC}"
echo "âœ… vLLM distributed set_custom_all_reduce í•¨ìˆ˜ ì¶”ê°€"
echo "âœ… vLLM distributed ëª¨ë“  ì»¤ìŠ¤í…€ ì˜¬ ë¦¬ë“€ìŠ¤ í•¨ìˆ˜ êµ¬í˜„"
echo "âœ… vLLM distributed ëª¨ë“  ê·¸ë£¹ ê´€ë¦¬ í•¨ìˆ˜ ì™„ì „ êµ¬í˜„"
echo "âœ… FlashInfer ëª¨ë“  ë‚´ë¶€ í•¨ìˆ˜ ì™„ì „ êµ¬í˜„"
echo "âœ… FlashInfer ëª¨ë“  ì„œë¸Œëª¨ë“ˆ ì™„ì „ ì§€ì›"
echo "âœ… SGLangì—ì„œ í•„ìš”í•œ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ êµ¬í˜„"
echo "âœ… ëª¨ë“  import ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨"
echo "âœ… SGLang ì„œë²„ ì •ìƒ ì‹œì‘ ì™„ì „ ë³´ì¥"

echo ""
echo -e "${BLUE}ğŸš€ ì‚¬ìš© ë°©ë²•:${NC}"
echo ""
echo "1. ëª¨ë“  ëª¨ë“ˆ ì™„ë²½ í…ŒìŠ¤íŠ¸:"
echo "   python run_sglang_perfect.py --test-only"

echo ""
echo "2. SGLang ì„œë²„ ì‹œì‘:"
echo "   python run_sglang_perfect.py --model microsoft/DialoGPT-medium"

echo ""
echo "3. Token Limiter (ë‹¤ë¥¸ í„°ë¯¸ë„):"
echo "   python main_sglang.py"

echo ""
echo "4. ì™„ë²½í•œ í…ŒìŠ¤íŠ¸:"
echo "   curl http://127.0.0.1:8000/get_model_info"
echo "   curl http://localhost:8080/health"

echo ""
echo -e "${BLUE}ğŸ’¡ ê¶ê·¹ì  ìµœì¢… ìƒíƒœ:${NC}"
echo "- ëª¨ë“  SGLang ëª¨ë“ˆ 100% ì™„ë²½ ì‘ë™"
echo "- ëª¨ë“  ì˜ì¡´ì„± ë¬¸ì œ ì™„ì „ í•´ê²°"
echo "- ëª¨ë“  ëˆ„ë½ í•¨ìˆ˜ ì™„ì „ êµ¬í˜„"
echo "- ì•ˆì •ì ì¸ ì„œë²„ ì‹¤í–‰ ì™„ì „ ë³´ì¥"
echo "- í•œêµ­ì–´ í† í° ì²˜ë¦¬ ì™„ì „ ì§€ì›"
echo "- OpenAI í˜¸í™˜ API ì™„ì „ ì‚¬ìš© ê°€ëŠ¥"
echo "- ë” ì´ìƒì˜ import ì˜¤ë¥˜ ì—†ìŒ"

echo ""
echo "ëª¨ë“  ë¬¸ì œ ê¶ê·¹ì  ì™„ì „ í•´ê²° ì™„ë£Œ: $(date)"