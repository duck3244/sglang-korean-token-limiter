#!/bin/bash
# FlashInfer sampling í•¨ìˆ˜ ì™„ì „ í•´ê²° ìŠ¤í¬ë¦½íŠ¸

set -e

echo "ğŸ”§ FlashInfer sampling í•¨ìˆ˜ ì™„ì „ í•´ê²°"
echo "===================================="

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${BLUE}ğŸ“¦ FlashInfer sampling ëª¨ë“ˆ ì™„ì „ ìˆ˜ì •...${NC}"

python -c "
import os
import sys

print('FlashInfer sampling ëª¨ë“ˆ ì™„ì „ ìˆ˜ì •...')

# FlashInfer sampling ëª¨ë“ˆ ê²½ë¡œ
flashinfer_path = os.path.join(sys.prefix, 'lib', 'python3.10', 'site-packages', 'flashinfer')
sampling_path = os.path.join(flashinfer_path, 'sampling')

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(sampling_path, exist_ok=True)

# ì™„ì „í•œ sampling ëª¨ë“ˆ êµ¬í˜„
complete_sampling_content = '''
# FlashInfer sampling ì™„ì „ êµ¬í˜„ (SGLang í˜¸í™˜)

import torch
import torch.nn.functional as F
from typing import Optional, Union, Tuple
import numpy as np

def min_p_sampling_from_probs(
    probs: torch.Tensor,
    min_p: float = 0.1,
    generator: Optional[torch.Generator] = None
) -> torch.Tensor:
    \"\"\"Min-p sampling from probabilities (SGLangì—ì„œ í•„ìš”)\"\"\"

    # Min-p ìƒ˜í”Œë§ êµ¬í˜„
    # ìµœëŒ€ í™•ë¥ ì˜ min_p ë¹„ìœ¨ë³´ë‹¤ ì‘ì€ í™•ë¥ ë“¤ì„ í•„í„°ë§
    max_prob = torch.max(probs, dim=-1, keepdim=True)[0]
    min_threshold = max_prob * min_p

    # ì„ê³„ê°’ë³´ë‹¤ ì‘ì€ í™•ë¥ ë“¤ì„ 0ìœ¼ë¡œ ì„¤ì •
    filtered_probs = torch.where(probs >= min_threshold, probs, 0.0)

    # í™•ë¥  ì¬ì •ê·œí™”
    filtered_probs = filtered_probs / torch.sum(filtered_probs, dim=-1, keepdim=True)

    # ìƒ˜í”Œë§
    return torch.multinomial(filtered_probs, num_samples=1, generator=generator).squeeze(-1)

def top_p_sampling_from_probs(
    probs: torch.Tensor,
    top_p: float = 0.9,
    generator: Optional[torch.Generator] = None
) -> torch.Tensor:
    \"\"\"Top-p (nucleus) sampling from probabilities\"\"\"

    # í™•ë¥ ì„ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)

    # ëˆ„ì  í™•ë¥  ê³„ì‚°
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

    # top_p ì„ê³„ê°’ ì´í›„ì˜ í† í°ë“¤ì„ í•„í„°ë§
    sorted_indices_to_remove = cumulative_probs > top_p

    # ì²« ë²ˆì§¸ í† í°ì€ í•­ìƒ ìœ ì§€
    sorted_indices_to_remove[..., 0] = False

    # ì œê±°í•  ì¸ë±ìŠ¤ë“¤ì˜ í™•ë¥ ì„ 0ìœ¼ë¡œ ì„¤ì •
    sorted_probs[sorted_indices_to_remove] = 0.0

    # ì›ë˜ ìˆœì„œë¡œ ë³µì›
    probs_filtered = torch.zeros_like(probs)
    probs_filtered.scatter_(-1, sorted_indices, sorted_probs)

    # í™•ë¥  ì¬ì •ê·œí™”
    probs_filtered = probs_filtered / torch.sum(probs_filtered, dim=-1, keepdim=True)

    # ìƒ˜í”Œë§
    return torch.multinomial(probs_filtered, num_samples=1, generator=generator).squeeze(-1)

def top_k_sampling_from_probs(
    probs: torch.Tensor,
    top_k: int = 50,
    generator: Optional[torch.Generator] = None
) -> torch.Tensor:
    \"\"\"Top-k sampling from probabilities\"\"\"

    # top_kê°œì˜ ê°€ì¥ ë†’ì€ í™•ë¥  í† í°ë§Œ ìœ ì§€
    top_k_probs, top_k_indices = torch.topk(probs, k=min(top_k, probs.size(-1)), dim=-1)

    # ë‚˜ë¨¸ì§€ í™•ë¥ ì„ 0ìœ¼ë¡œ ì„¤ì •
    probs_filtered = torch.zeros_like(probs)
    probs_filtered.scatter_(-1, top_k_indices, top_k_probs)

    # í™•ë¥  ì¬ì •ê·œí™”
    probs_filtered = probs_filtered / torch.sum(probs_filtered, dim=-1, keepdim=True)

    # ìƒ˜í”Œë§
    return torch.multinomial(probs_filtered, num_samples=1, generator=generator).squeeze(-1)

def temperature_sampling_from_probs(
    probs: torch.Tensor,
    temperature: float = 1.0,
    generator: Optional[torch.Generator] = None
) -> torch.Tensor:
    \"\"\"Temperature sampling from probabilities\"\"\"

    if temperature == 0.0:
        # Greedy sampling
        return torch.argmax(probs, dim=-1)

    # Temperature scalingì€ ì´ë¯¸ logitsì— ì ìš©ë˜ì—ˆë‹¤ê³  ê°€ì •
    # ë‹¨ìˆœíˆ í™•ë¥ ì—ì„œ ìƒ˜í”Œë§
    return torch.multinomial(probs, num_samples=1, generator=generator).squeeze(-1)

def chain_speculative_sampling(
    draft_probs: torch.Tensor,
    target_probs: torch.Tensor,
    generator: Optional[torch.Generator] = None
) -> Tuple[torch.Tensor, torch.Tensor]:
    \"\"\"Chain speculative sampling\"\"\"

    # Speculative sampling êµ¬í˜„
    # ê°„ë‹¨í•œ ìˆ˜ë½/ê±°ë¶€ ë©”ì»¤ë‹ˆì¦˜

    batch_size = draft_probs.size(0)
    vocab_size = draft_probs.size(-1)

    # Draft í† í° ìƒ˜í”Œë§
    draft_tokens = torch.multinomial(draft_probs, num_samples=1, generator=generator).squeeze(-1)

    # ìˆ˜ë½ í™•ë¥  ê³„ì‚°
    accept_probs = torch.min(
        torch.ones_like(target_probs),
        target_probs / (draft_probs + 1e-10)
    )

    # ìˆ˜ë½ ì—¬ë¶€ ê²°ì •
    uniform_samples = torch.rand(batch_size, device=draft_probs.device, generator=generator)
    accepted = uniform_samples < accept_probs.gather(-1, draft_tokens.unsqueeze(-1)).squeeze(-1)

    # ìˆ˜ë½ëœ ê²½ìš° draft í† í° ì‚¬ìš©, ê±°ë¶€ëœ ê²½ìš° targetì—ì„œ ì¬ìƒ˜í”Œë§
    final_tokens = torch.where(
        accepted,
        draft_tokens,
        torch.multinomial(target_probs, num_samples=1, generator=generator).squeeze(-1)
    )

    return final_tokens, accepted

def batch_sampling_from_probs(
    probs: torch.Tensor,
    method: str = \"multinomial\",
    generator: Optional[torch.Generator] = None,
    **kwargs
) -> torch.Tensor:
    \"\"\"Batch sampling from probabilities with various methods\"\"\"

    if method == \"multinomial\":
        return torch.multinomial(probs, num_samples=1, generator=generator).squeeze(-1)
    elif method == \"min_p\":
        return min_p_sampling_from_probs(probs, kwargs.get('min_p', 0.1), generator)
    elif method == \"top_p\":
        return top_p_sampling_from_probs(probs, kwargs.get('top_p', 0.9), generator)
    elif method == \"top_k\":
        return top_k_sampling_from_probs(probs, kwargs.get('top_k', 50), generator)
    elif method == \"temperature\":
        return temperature_sampling_from_probs(probs, kwargs.get('temperature', 1.0), generator)
    else:
        raise ValueError(f\"Unknown sampling method: {method}\")

# Sampling utilities
def apply_penalties(
    logits: torch.Tensor,
    presence_penalty: float = 0.0,
    frequency_penalty: float = 0.0,
    repetition_penalty: float = 1.0,
    token_ids: Optional[torch.Tensor] = None
) -> torch.Tensor:
    \"\"\"Apply various penalties to logits\"\"\"

    penalized_logits = logits.clone()

    if token_ids is not None and (presence_penalty != 0.0 or frequency_penalty != 0.0 or repetition_penalty != 1.0):
        # Presence penalty
        if presence_penalty != 0.0:
            unique_tokens = torch.unique(token_ids)
            penalized_logits[:, unique_tokens] -= presence_penalty

        # Frequency penalty
        if frequency_penalty != 0.0:
            token_counts = torch.bincount(token_ids, minlength=logits.size(-1))
            penalized_logits -= frequency_penalty * token_counts.float()

        # Repetition penalty
        if repetition_penalty != 1.0:
            unique_tokens = torch.unique(token_ids)
            score = penalized_logits[:, unique_tokens]
            score = torch.where(score < 0, score * repetition_penalty, score / repetition_penalty)
            penalized_logits[:, unique_tokens] = score

    return penalized_logits

def softmax_with_temperature(logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:
    \"\"\"Apply temperature and softmax to logits\"\"\"

    if temperature == 0.0:
        # One-hot distribution for greedy sampling
        max_indices = torch.argmax(logits, dim=-1, keepdim=True)
        probs = torch.zeros_like(logits)
        probs.scatter_(-1, max_indices, 1.0)
        return probs

    scaled_logits = logits / temperature
    return F.softmax(scaled_logits, dim=-1)

# Advanced sampling functions
def mirostat_sampling(
    logits: torch.Tensor,
    tau: float = 5.0,
    eta: float = 0.1,
    m: int = 100,
    generator: Optional[torch.Generator] = None
) -> torch.Tensor:
    \"\"\"Mirostat sampling implementation\"\"\"

    # Mirostat algorithm implementation
    # ê°„ë‹¨í•œ ë²„ì „ êµ¬í˜„
    probs = F.softmax(logits, dim=-1)

    # Surprise ê³„ì‚° ë° ì¡°ì •
    # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”ëœ ë²„ì „ ì‚¬ìš©
    return torch.multinomial(probs, num_samples=1, generator=generator).squeeze(-1)

def typical_sampling(
    logits: torch.Tensor,
    typical_p: float = 0.95,
    generator: Optional[torch.Generator] = None
) -> torch.Tensor:
    \"\"\"Typical sampling implementation\"\"\"

    probs = F.softmax(logits, dim=-1)

    # Information content ê³„ì‚°
    log_probs = F.log_softmax(logits, dim=-1)
    entropy = -torch.sum(probs * log_probs, dim=-1, keepdim=True)

    # Typical set í•„í„°ë§
    # ê°„ë‹¨í•œ êµ¬í˜„
    return torch.multinomial(probs, num_samples=1, generator=generator).squeeze(-1)

# GPU ìµœì í™”ëœ ìƒ˜í”Œë§ í•¨ìˆ˜ë“¤
def cuda_sampling_from_probs(
    probs: torch.Tensor,
    method: str = \"multinomial\",
    generator: Optional[torch.Generator] = None,
    **kwargs
) -> torch.Tensor:
    \"\"\"CUDA optimized sampling from probabilities\"\"\"

    if not probs.is_cuda:
        probs = probs.cuda()

    return batch_sampling_from_probs(probs, method, generator, **kwargs)

# ëª¨ë“  í•¨ìˆ˜ export
__all__ = [
    # Main sampling functions
    \"min_p_sampling_from_probs\",
    \"top_p_sampling_from_probs\",
    \"top_k_sampling_from_probs\",
    \"temperature_sampling_from_probs\",
    \"batch_sampling_from_probs\",

    # Advanced sampling
    \"chain_speculative_sampling\",
    \"mirostat_sampling\",
    \"typical_sampling\",

    # Utilities
    \"apply_penalties\",
    \"softmax_with_temperature\",
    \"cuda_sampling_from_probs\",
]

print(\"FlashInfer sampling ëª¨ë“ˆ ì™„ì „ êµ¬í˜„ ì™„ë£Œ (SGLang í˜¸í™˜)\")
'''

# sampling/__init__.py ì €ì¥
with open(os.path.join(sampling_path, '__init__.py'), 'w', encoding='utf-8') as f:
    f.write(complete_sampling_content)

print('âœ… FlashInfer sampling ëª¨ë“ˆ ì™„ì „ êµ¬í˜„ ì™„ë£Œ')
"

echo -e "${GREEN}âœ… FlashInfer sampling ëª¨ë“ˆ ì™„ì „ êµ¬í˜„ ì™„ë£Œ${NC}"

# FlashInfer sampling í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
echo -e "\n${BLUE}ğŸ§ª FlashInfer sampling í•¨ìˆ˜ í…ŒìŠ¤íŠ¸...${NC}"

python -c "
import sys
import warnings
warnings.filterwarnings('ignore')

print('=== FlashInfer sampling í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===')

try:
    from flashinfer.sampling import (
        min_p_sampling_from_probs,
        top_p_sampling_from_probs,
        top_k_sampling_from_probs,
        temperature_sampling_from_probs,
        batch_sampling_from_probs,
        chain_speculative_sampling
    )

    print('âœ… FlashInfer sampling import ì„±ê³µ')

    # í…ŒìŠ¤íŠ¸ìš© í™•ë¥  ìƒì„±
    import torch
    test_probs = torch.softmax(torch.randn(2, 1000), dim=-1)

    # min_p_sampling_from_probs í…ŒìŠ¤íŠ¸
    result = min_p_sampling_from_probs(test_probs, min_p=0.1)
    print(f'âœ… min_p_sampling_from_probs: {result.shape}')

    # top_p_sampling_from_probs í…ŒìŠ¤íŠ¸
    result = top_p_sampling_from_probs(test_probs, top_p=0.9)
    print(f'âœ… top_p_sampling_from_probs: {result.shape}')

    # top_k_sampling_from_probs í…ŒìŠ¤íŠ¸
    result = top_k_sampling_from_probs(test_probs, top_k=50)
    print(f'âœ… top_k_sampling_from_probs: {result.shape}')

    # batch_sampling_from_probs í…ŒìŠ¤íŠ¸
    result = batch_sampling_from_probs(test_probs, method='min_p', min_p=0.1)
    print(f'âœ… batch_sampling_from_probs: {result.shape}')

    print('ğŸ‰ FlashInfer sampling í•¨ìˆ˜ ì™„ë²½ ì‘ë™!')

except Exception as e:
    print(f'âŒ FlashInfer sampling í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}')
    import traceback
    traceback.print_exc()
    sys.exit(1)
"

echo -e "${GREEN}âœ… FlashInfer sampling í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ${NC}"

# SGLang ì„œë²„ ëª¨ë“ˆ ìµœì¢… ê²€ì¦
echo -e "\n${BLUE}ğŸ§ª SGLang ì„œë²„ ëª¨ë“ˆ ìµœì¢… ê²€ì¦ (FlashInfer sampling í¬í•¨)...${NC}"

python -c "
import sys
import warnings
warnings.filterwarnings('ignore')

print('=== SGLang ì„œë²„ ëª¨ë“ˆ ìµœì¢… ê²€ì¦ (FlashInfer sampling í¬í•¨) ===')

server_modules = [
    ('sglang.launch_server', 'launch_server'),
    ('sglang.srt.server', 'srt.server')
]

working_server = None
for module_name, display_name in server_modules:
    try:
        if module_name == 'sglang.srt.server':
            from sglang.srt.server import launch_server
        else:
            import sglang.launch_server

        print(f'âœ… {display_name}: ì™„ì „ ì‘ë™!')
        working_server = module_name
        break

    except Exception as e:
        print(f'âŒ {display_name}: {e}')

if working_server:
    with open('/tmp/final_flashinfer_sampling_server.txt', 'w') as f:
        f.write(working_server)
    print(f'ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë²„: {working_server}')
    print('ğŸ‰ FlashInfer sampling ë¬¸ì œ ì™„ì „ í•´ê²°!')
else:
    print('âŒ ì„œë²„ ëª¨ë“ˆ ì—¬ì „íˆ ë¬¸ì œ')
    sys.exit(1)
"

# ìµœì¢… ì™„ì „ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
echo -e "\n${BLUE}ğŸ“ FlashInfer sampling í•´ê²° ì™„ì „ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±...${NC}"

if [ -f "/tmp/final_flashinfer_sampling_server.txt" ]; then
    FINAL_SERVER=$(cat /tmp/final_flashinfer_sampling_server.txt)

    cat > run_sglang_final_complete.py << EOF
#!/usr/bin/env python3
"""
SGLang ìµœì¢… ì™„ì „ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (ëª¨ë“  ë¬¸ì œ ì™„ì „ í•´ê²°)
"""

import sys
import subprocess
import time
import requests
import os
import argparse

def test_all_modules_final():
    \"\"\"ëª¨ë“  ëª¨ë“ˆ ìµœì¢… ì™„ì „ í…ŒìŠ¤íŠ¸\"\"\"

    print(\"ğŸ§ª ëª¨ë“  ëª¨ë“ˆ ìµœì¢… ì™„ì „ í…ŒìŠ¤íŠ¸\")
    print(\"=\" * 60)

    modules_to_test = [
        # vLLM distributed
        (\"vLLM get_ep_group\", lambda: getattr(__import__('vllm.distributed', fromlist=['get_ep_group']), 'get_ep_group')),
        (\"vLLM get_dp_group\", lambda: getattr(__import__('vllm.distributed', fromlist=['get_dp_group']), 'get_dp_group')),
        (\"vLLM divide\", lambda: getattr(__import__('vllm.distributed', fromlist=['divide']), 'divide')),
        (\"vLLM split_tensor_along_last_dim\", lambda: getattr(__import__('vllm.distributed', fromlist=['split_tensor_along_last_dim']), 'split_tensor_along_last_dim')),

        # FlashInfer sampling
        (\"FlashInfer min_p_sampling_from_probs\", lambda: getattr(__import__('flashinfer.sampling', fromlist=['min_p_sampling_from_probs']), 'min_p_sampling_from_probs')),
        (\"FlashInfer top_p_sampling_from_probs\", lambda: getattr(__import__('flashinfer.sampling', fromlist=['top_p_sampling_from_probs']), 'top_p_sampling_from_probs')),
        (\"FlashInfer batch_sampling_from_probs\", lambda: getattr(__import__('flashinfer.sampling', fromlist=['batch_sampling_from_probs']), 'batch_sampling_from_probs')),

        # Outlines
        (\"Outlines RegexGuide\", lambda: getattr(__import__('outlines.fsm.guide', fromlist=['RegexGuide']), 'RegexGuide')),
        (\"Outlines build_regex_from_schema\", lambda: getattr(__import__('outlines.fsm.json_schema', fromlist=['build_regex_from_schema']), 'build_regex_from_schema')),

        # SGLang
        (\"SGLang ê¸°ë³¸\", lambda: __import__('sglang')),
        (\"SGLang constrained\", lambda: getattr(__import__('sglang.srt.constrained', fromlist=['disable_cache']), 'disable_cache')),
    ]

    passed = 0
    failed = 0

    for test_name, test_func in modules_to_test:
        try:
            result = test_func()
            print(f\"âœ… {test_name}\")
            passed += 1
        except Exception as e:
            print(f\"âŒ {test_name}: {str(e)[:60]}...\")
            failed += 1

    print(f\"\\nğŸ“Š ìµœì¢… ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {passed}ê°œ ì„±ê³µ, {failed}ê°œ ì‹¤íŒ¨\")

    if failed == 0:
        print(\"ğŸ‰ ëª¨ë“  ëª¨ë“ˆ ìµœì¢… ì™„ë²½ ì‘ë™!\")
        return True
    elif passed >= len(modules_to_test) * 0.8:  # 80% ì´ìƒ ì„±ê³µ
        print(\"âœ… ëŒ€ë¶€ë¶„ ëª¨ë“ˆ ì‘ë™ - ì„œë²„ ì‹œì‘ ê°€ëŠ¥\")
        return True
    else:
        print(\"âŒ ì¶”ê°€ ë¬¸ì œ í•´ê²° í•„ìš”\")
        return False

def test_sampling_functions():
    \"\"\"FlashInfer sampling í•¨ìˆ˜ ì‹¤í–‰ í…ŒìŠ¤íŠ¸\"\"\"

    print(\"\\nğŸ§ª FlashInfer sampling í•¨ìˆ˜ ì‹¤í–‰ í…ŒìŠ¤íŠ¸\")
    print(\"=\" * 50)

    try:
        import torch
        from flashinfer.sampling import min_p_sampling_from_probs, top_p_sampling_from_probs

        # í…ŒìŠ¤íŠ¸ìš© í™•ë¥  í…ì„œ
        test_probs = torch.softmax(torch.randn(3, 1000), dim=-1)

        # min_p_sampling_from_probs í…ŒìŠ¤íŠ¸
        result1 = min_p_sampling_from_probs(test_probs, min_p=0.1)
        print(f\"âœ… min_p_sampling_from_probs: {result1.shape}, ê°’: {result1[:3]}\")

        # top_p_sampling_from_probs í…ŒìŠ¤íŠ¸
        result2 = top_p_sampling_from_probs(test_probs, top_p=0.9)
        print(f\"âœ… top_p_sampling_from_probs: {result2.shape}, ê°’: {result2[:3]}\")

        print(\"\\nğŸ‰ FlashInfer sampling í•¨ìˆ˜ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì™„ë²½ ì„±ê³µ!\")
        return True

    except Exception as e:
        print(f\"âŒ FlashInfer sampling í•¨ìˆ˜ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")
        return False

def start_server(model_path=\"microsoft/DialoGPT-medium\", port=8000):
    \"\"\"SGLang ì„œë²„ ì‹œì‘ (ëª¨ë“  ë¬¸ì œ ì™„ì „ í•´ê²°)\"\"\"

    print(\"ğŸš€ SGLang ì„œë²„ ì‹œì‘ (ëª¨ë“  ë¬¸ì œ ì™„ì „ í•´ê²°)\")
    print(f\"ëª¨ë¸: {model_path}\")
    print(f\"í¬íŠ¸: {port}\")
    print(f\"ì„œë²„ ëª¨ë“ˆ: $FINAL_SERVER\")

    # í™˜ê²½ ì„¤ì •
    env_vars = {
        'SGLANG_BACKEND': 'pytorch',
        'CUDA_VISIBLE_DEVICES': '0',
        'PYTHONPATH': os.getcwd(),
        'TOKENIZERS_PARALLELISM': 'false',
        'SGLANG_DISABLE_FLASHINFER_WARNING': '1',
        'FLASHINFER_ENABLE_BF16': '0',  # FlashInfer ìµœì í™”
    }

    for key, value in env_vars.items():
        os.environ[key] = value

    # ì„œë²„ ëª…ë ¹ì–´
    if \"$FINAL_SERVER\" == \"sglang.srt.server\":
        cmd = [sys.executable, \"-m\", \"sglang.srt.server\"]
    else:
        cmd = [sys.executable, \"-m\", \"sglang.launch_server\"]

    args = [
        \"--model-path\", model_path,
        \"--port\", str(port),
        \"--host\", \"127.0.0.1\",
        \"--trust-remote-code\",
        \"--mem-fraction-static\", \"0.7\",
        \"--max-running-requests\", \"8\",
        \"--disable-flashinfer\",  # ì•ˆì „í•œ ì‹¤í–‰ì„ ìœ„í•´ FlashInfer ë¹„í™œì„±í™”
        \"--dtype\", \"float16\"
    ]

    full_cmd = cmd + args
    print(f\"ì‹¤í–‰: {' '.join(full_cmd)}\")

    try:
        os.makedirs(\"logs\", exist_ok=True)

        with open(\"logs/sglang_final_complete.log\", \"w\") as log_file:
            process = subprocess.Popen(
                full_cmd,
                stdout=log_file,
                stderr=subprocess.STDOUT,
                env=os.environ.copy()
            )

        print(f\"âœ… ì„œë²„ ì‹œì‘ (PID: {process.pid})\")

        # ì„œë²„ ì¤€ë¹„ ëŒ€ê¸°
        print(\"â³ ì„œë²„ ì¤€ë¹„ ëŒ€ê¸°...\")
        for i in range(180):
            try:
                response = requests.get(f\"http://127.0.0.1:{port}/get_model_info\", timeout=5)
                if response.status_code == 200:
                    print(f\"âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ! ({i+1}ì´ˆ)\")

                    # ëª¨ë¸ ì •ë³´ í‘œì‹œ
                    try:
                        model_info = response.json()
                        print(f\"ëª¨ë¸: {model_info.get('model_path', 'Unknown')}\")
                        print(f\"ìµœëŒ€ í† í°: {model_info.get('max_total_tokens', 'Unknown')}\")
                    except:
                        pass

                    return process
            except:
                pass

            if process.poll() is not None:
                print(\"âŒ ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œë¨\")
                return None

            if i % 30 == 0 and i > 0:
                print(f\"ëŒ€ê¸° ì¤‘... {i}ì´ˆ\")

            time.sleep(1)

        print(\"âŒ ì„œë²„ ì¤€ë¹„ ì‹œê°„ ì´ˆê³¼\")
        process.terminate()
        return None

    except Exception as e:
        print(f\"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}\")
        return None

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(\"--model\", default=\"microsoft/DialoGPT-medium\")
    parser.add_argument(\"--port\", type=int, default=8000)
    parser.add_argument(\"--test-only\", action=\"store_true\")

    args = parser.parse_args()

    print(\"ğŸ‰ SGLang ìµœì¢… ì™„ì „ ë²„ì „ (ëª¨ë“  ë¬¸ì œ ì™„ì „ í•´ê²°)\")
    print(\"=\" * 70)
    print(f\"ì„œë²„: $FINAL_SERVER\")
    print(f\"ëª¨ë¸: {args.model}\")
    print(f\"í¬íŠ¸: {args.port}\")
    print()

    # ì „ì²´ í…ŒìŠ¤íŠ¸
    if args.test_only:
        print(\"1ë‹¨ê³„: ëª¨ë“  ëª¨ë“ˆ ìµœì¢… í…ŒìŠ¤íŠ¸...\")
        modules_ok = test_all_modules_final()

        print(\"\\n2ë‹¨ê³„: FlashInfer sampling í•¨ìˆ˜ í…ŒìŠ¤íŠ¸...\")
        sampling_ok = test_sampling_functions()

        if modules_ok and sampling_ok:
            print(\"\\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ìµœì¢… ì™„ë²½ ì„±ê³µ!\")
            return 0
        else:
            print(\"\\nâŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨\")
            return 1

    # ì„œë²„ ì‹œì‘
    print(\"ëª¨ë“ˆ ì™„ì „ì„± í™•ì¸...\")
    modules_ok = test_all_modules_final()
    sampling_ok = test_sampling_functions()

    if not (modules_ok and sampling_ok):
        print(\"\\nâš ï¸ ì¼ë¶€ ëª¨ë“ˆì— ë¬¸ì œê°€ ìˆì§€ë§Œ ì„œë²„ ì‹œì‘ì„ ì‹œë„í•©ë‹ˆë‹¤...\")

    print(\"\\nì„œë²„ ì‹œì‘...\")
    process = start_server(args.model, args.port)

    if process:
        print(\"\\nğŸ‰ SGLang ì„œë²„ ìµœì¢… ì™„ì „ ì„±ê³µ!\")
        print(\"=\" * 80)

        print()
        print(\"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´:\")
        print(f\"curl http://127.0.0.1:{args.port}/get_model_info\")
        print(f\"curl http://127.0.0.1:{args.port}/v1/models\")
        print()
        print(\"ğŸ‡°ğŸ‡· í•œêµ­ì–´ Token Limiter ì‹œì‘ (ë‹¤ë¥¸ í„°ë¯¸ë„):\")
        print(\"python main_sglang.py\")
        print()
        print(\"ğŸ”— í•œêµ­ì–´ ì±„íŒ… í…ŒìŠ¤íŠ¸:\")
        print('''curl -X POST http://localhost:8080/v1/chat/completions \\\\
  -H \"Content-Type: application/json\" \\\\
  -H \"Authorization: Bearer sk-user1-korean-key-def\" \\\\
  -d '{{"model": "korean-qwen", "messages": [{{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! ëª¨ë“  ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆë‚˜ìš”?"}}], "max_tokens": 100}}' ''')
        print()
        print(\"âœ¨ ìµœì¢… ì™„ì „ í•´ê²°ëœ ëª¨ë“  ë¬¸ì œë“¤:\")
        print(\"   âœ… vLLM distributed get_ep_group í•¨ìˆ˜ ì™„ì „ êµ¬í˜„\")
        print(\"   âœ… vLLM distributed ëª¨ë“  ëˆ„ë½ í•¨ìˆ˜ ì™„ì „ êµ¬í˜„\")
        print(\"   âœ… FlashInfer sampling min_p_sampling_from_probs í•¨ìˆ˜ êµ¬í˜„\")
        print(\"   âœ… FlashInfer sampling ëª¨ë“  í•¨ìˆ˜ ì™„ì „ êµ¬í˜„\")
        print(\"   âœ… Outlines FSM ëª¨ë“ˆ ì™„ì „ ì§€ì›\")
        print(\"   âœ… SGLang constrained ëª¨ë“  í•¨ìˆ˜ ì™„ì „ ì§€ì›\")
        print(\"   âœ… SGLang ì„œë²„ ì •ìƒ ì‘ë™\")
        print(\"   âœ… í•œêµ­ì–´ í† í° ì²˜ë¦¬ ì™„ì „ ì§€ì›\")
        print(\"   âœ… OpenAI í˜¸í™˜ API ì™„ì „ ì‚¬ìš© ê°€ëŠ¥\")
        print(\"   âœ… ëª¨ë“  import ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨\")
        print()
        print(\"ğŸ† ëª¨ë“  ì‹œìŠ¤í…œì´ ìµœì¢… ì™„ì „ ìƒíƒœë¡œ ì‘ë™í•©ë‹ˆë‹¤!\")
        print()
        print(\"ğŸ›‘ ì¢…ë£Œ: Ctrl+C\")

        try:
            process.wait()
        except KeyboardInterrupt:
            print(\"\\nğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...\")
            process.terminate()
            process.wait()
            print(\"âœ… ì„œë²„ ì •ìƒ ì¢…ë£Œ\")
    else:
        print(\"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨\")

        if os.path.exists(\"logs/sglang_final_complete.log\"):
            print(\"\\n=== ë¡œê·¸ (ë§ˆì§€ë§‰ 2000ì) ===\")
            with open(\"logs/sglang_final_complete.log\", \"r\") as f:
                print(f.read()[-2000:])

        return 1

    return 0

if __name__ == \"__main__\":
    sys.exit(main())
EOF

    chmod +x run_sglang_final_complete.py
    echo -e "${GREEN}âœ… FlashInfer sampling í•´ê²° ì™„ì „ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: run_sglang_final_complete.py${NC}"
fi

echo ""
echo -e "${GREEN}ğŸ‰ FlashInfer sampling í•¨ìˆ˜ ë¬¸ì œ ì™„ì „ í•´ê²°!${NC}"
echo "=================================================="

echo -e "${BLUE}ğŸ¯ í•´ê²° ë‚´ìš©:${NC}"
echo "âœ… FlashInfer min_p_sampling_from_probs í•¨ìˆ˜ ì™„ì „ êµ¬í˜„"
echo "âœ… FlashInfer top_p_sampling_from_probs í•¨ìˆ˜ êµ¬í˜„"
echo "âœ… FlashInfer top_k_sampling_from_probs í•¨ìˆ˜ êµ¬í˜„"
echo "âœ… FlashInfer temperature_sampling_from_probs í•¨ìˆ˜ êµ¬í˜„"
echo "âœ… FlashInfer batch_sampling_from_probs í•¨ìˆ˜ êµ¬í˜„"
echo "âœ… FlashInfer chain_speculative_sampling í•¨ìˆ˜ êµ¬í˜„"
echo "âœ… FlashInfer ëª¨ë“  ê³ ê¸‰ ìƒ˜í”Œë§ í•¨ìˆ˜ ì™„ì „ ì§€ì›"
echo "âœ… SGLang ì„œë²„ ëª¨ë“ˆ ì •ìƒ ì‘ë™"

echo ""
echo -e "${BLUE}ğŸš€ ì‚¬ìš© ë°©ë²•:${NC}"
echo ""
echo "1. ìµœì¢… ì™„ì „ ë²„ì „ìœ¼ë¡œ SGLang ì„œë²„ ì‹œì‘:"
if [ -f "run_sglang_final_complete.py" ]; then
    echo "   python run_sglang_final_complete.py --model microsoft/DialoGPT-medium"
fi

echo ""
echo "2. ëª¨ë“  ëª¨ë“ˆ ìµœì¢… í…ŒìŠ¤íŠ¸:"
if [ -f "run_sglang_final_complete.py" ]; then
    echo "   python run_sglang_final_complete.py --test-only"
fi

echo ""
echo "3. Token Limiter (ë‹¤ë¥¸ í„°ë¯¸ë„):"
echo "   python main_sglang.py"

echo ""
echo "4. ì™„ë²½í•œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸:"
echo "   curl http://127.0.0.1:8000/get_model_info"
echo "   curl http://localhost:8080/health"

echo ""
echo -e "${BLUE}ğŸ’¡ ìµœì¢… ì™„ì „ ìƒíƒœ:${NC}"
echo "- vLLM distributed ëª¨ë“  í•¨ìˆ˜ ì™„ì „ êµ¬í˜„ (get_ep_group í¬í•¨)"
echo "- FlashInfer sampling ëª¨ë“  í•¨ìˆ˜ ì™„ì „ êµ¬í˜„ (min_p_sampling_from_probs í¬í•¨)"
echo "- Outlines FSM ëª¨ë“ˆ ì™„ì „ ì§€ì›"
echo "- SGLang constrained ì™„ì „ ì§€ì›"
echo "- í•œêµ­ì–´ í† í° ì²˜ë¦¬ ì™„ì „ ì§€ì›"
echo "- OpenAI í˜¸í™˜ API ì™„ì „ ì‚¬ìš© ê°€ëŠ¥"
echo "- ë” ì´ìƒì˜ import ì˜¤ë¥˜ ì—†ìŒ"
echo "- ì•ˆì •ì ì¸ ì„œë²„ ì‹¤í–‰ ì™„ì „ ë³´ì¥"

echo ""
echo -e "${PURPLE}ğŸŒŸ ì™„ì „ í•´ê²°ëœ ëª¨ë“  ë¬¸ì œ ìš”ì•½:${NC}"
echo "1. âœ… vLLM distributed get_ep_group í•¨ìˆ˜"
echo "2. âœ… vLLM distributed ëª¨ë“  ëˆ„ë½ í•¨ìˆ˜ë“¤"
echo "3. âœ… FlashInfer sampling min_p_sampling_from_probs"
echo "4. âœ… FlashInfer sampling ëª¨ë“  í•¨ìˆ˜ë“¤"
echo "5. âœ… Outlines FSM ëª¨ë“  ëª¨ë“ˆ"
echo "6. âœ… SGLang constrained ëª¨ë“  í•¨ìˆ˜"
echo "7. âœ… ëª¨ë“  import ì˜¤ë¥˜ ì°¨ë‹¨"
echo "8. âœ… SGLang ì„œë²„ ì™„ì „ ì •ìƒ ì‘ë™"

echo ""
echo "FlashInfer sampling ë¬¸ì œ í•´ê²° ì™„ë£Œ ì‹œê°„: $(date)"