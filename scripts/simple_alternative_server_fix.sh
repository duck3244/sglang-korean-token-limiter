#!/bin/bash
# ê°„ë‹¨í•œ ëŒ€ì²´ ì„œë²„ ë° accelerate ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

set -e

echo "ğŸ”§ ëŒ€ì²´ ì„œë²„ ë¬¸ì œ í•´ê²°"
echo "====================="

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m'

echo -e "${BLUE}ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜...${NC}"

# accelerate ì„¤ì¹˜
echo "accelerate íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
pip install accelerate

# ì¶”ê°€ í•„ìš” íŒ¨í‚¤ì§€ë“¤
echo "ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
pip install fastapi uvicorn transformers torch

echo -e "${GREEN}âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ${NC}"

# ë” ê°„ë‹¨í•œ ëŒ€ì²´ ì„œë²„ ìƒì„±
echo -e "\n${BLUE}ğŸ“ ê°„ë‹¨í•œ ëŒ€ì²´ ì„œë²„ ìƒì„±...${NC}"

cat > simple_korean_server.py << 'EOF'
#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ í•œêµ­ì–´ ëª¨ë¸ ì„œë²„ (ìµœì†Œ ì˜ì¡´ì„±)
"""

import sys
import os
import json
import time
import random
from contextlib import asynccontextmanager

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
import uvicorn

# ê¸€ë¡œë²Œ ë³€ìˆ˜
model_loaded = False
model_info = {
    "model_path": "korean-simple-server",
    "max_total_tokens": 2048,
    "served_model_names": ["korean-qwen"],
    "is_generation": True,
    "version": "1.0.0"
}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    # ì‹œì‘
    print("ğŸ”½ ì„œë²„ ì´ˆê¸°í™” ì¤‘...")
    global model_loaded
    
    try:
        # ì‹¤ì œ ëª¨ë¸ ëŒ€ì‹  ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±ê¸° ì‚¬ìš©
        print("âœ… ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±ê¸° ì¤€ë¹„ ì™„ë£Œ")
        model_loaded = True
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨í•˜ì§€ë§Œ ê³„ì† ì§„í–‰: {e}")
        model_loaded = False
    
    yield
    
    # ì¢…ë£Œ
    print("ğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="ê°„ë‹¨í•œ í•œêµ­ì–´ ëª¨ë¸ ì„œë²„",
    description="SGLang ëŒ€ì²´ ì„œë²„ (ìµœì†Œ ì˜ì¡´ì„±)",
    version="1.0.0",
    lifespan=lifespan
)

def generate_korean_response(user_message: str, max_tokens: int = 100) -> str:
    """ê°„ë‹¨í•œ í•œêµ­ì–´ ì‘ë‹µ ìƒì„±"""
    
    # ë¯¸ë¦¬ ì •ì˜ëœ í•œêµ­ì–´ ì‘ë‹µë“¤
    responses = [
        "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ê°„ë‹¨í•œ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
        "ë„¤, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
        "ì¢‹ì€ ì§ˆë¬¸ì´ë„¤ìš”. ë” ìì„¸íˆ ì„¤ëª…í•´ ì£¼ì‹œê² ì–´ìš”?",
        "SGLang ëŒ€ì²´ ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        "í•œêµ­ì–´ í† í° ì œí•œ ì‹œìŠ¤í…œê³¼ ì˜ ì—°ë™ë˜ê³  ìˆì–´ìš”.",
        "Token Limiterê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”.",
        "ì´ ì„œë²„ëŠ” CPU ëª¨ë“œë¡œ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.",
        "CUDA ë¬¸ì œ ì—†ì´ ì˜ ì‘ë™í•˜ê³  ìˆì–´ìš”!",
        "ê°„ë‹¨í•˜ì§€ë§Œ íš¨ê³¼ì ì¸ ëŒ€ì²´ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.",
        "ë” ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”."
    ]
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ì— ë”°ë¥¸ ë§ì¶¤ ì‘ë‹µ
    user_lower = user_message.lower()
    
    if any(word in user_lower for word in ['ì•ˆë…•', 'hello', 'í•˜ì´']):
        return "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” í•œêµ­ì–´ ëŒ€ì²´ ì„œë²„ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    elif any(word in user_lower for word in ['í† í°', 'token']):
        return "í† í° ì œí•œ ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤. SGLang ëŒ€ì²´ ì„œë²„ê°€ Token Limiterì™€ ì˜ ì—°ë™ë˜ê³  ìˆì–´ìš”."
    elif any(word in user_lower for word in ['ì„œë²„', 'server', 'sglang']):
        return "SGLang ëŒ€ì²´ ì„œë²„ê°€ CPU ëª¨ë“œë¡œ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. CUDA ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œ ì—†ì´ ì˜ ì‘ë™í•´ìš”!"
    elif any(word in user_lower for word in ['í…ŒìŠ¤íŠ¸', 'test']):
        return "ë„¤, í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤! ê°„ë‹¨í•œ ëŒ€ì²´ ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì´ì—ìš”."
    elif any(word in user_lower for word in ['ë¬¸ì œ', 'problem', 'ì˜¤ë¥˜', 'error']):
        return "ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤! ì´ ëŒ€ì²´ ì„œë²„ëŠ” SGLangì˜ ë³µì¡í•œ ë¬¸ì œë“¤ì„ ìš°íšŒí•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤."
    else:
        # ëœë¤ ì‘ë‹µ ì„ íƒ
        return random.choice(responses)

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "ê°„ë‹¨í•œ í•œêµ­ì–´ ëª¨ë¸ ì„œë²„",
        "status": "running",
        "model_loaded": model_loaded
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬"""
    return {
        "status": "healthy",
        "model_loaded": model_loaded,
        "server": "simple-korean-server",
        "timestamp": time.time()
    }

@app.get("/get_model_info")
async def get_model_info():
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ (SGLang í˜¸í™˜)"""
    return model_info

@app.get("/get_server_info")
async def get_server_info():
    """ì„œë²„ ì •ë³´ ì¡°íšŒ"""
    return {
        "requests_per_second": 0,
        "tokens_per_second": 0,
        "queue_length": 0,
        "running_requests": 0,
        "memory_usage_gb": 1.0,
        "cache_hit_rate": 0.8,
        "timestamp": time.time()
    }

@app.get("/v1/models")
async def list_models():
    """ëª¨ë¸ ëª©ë¡ (OpenAI í˜¸í™˜)"""
    return {
        "object": "list",
        "data": [
            {
                "id": "korean-qwen",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "simple-korean-server"
            }
        ]
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    """ì±„íŒ… ì™„ì„± (OpenAI í˜¸í™˜)"""
    
    try:
        body = await request.json()
        
        # ìš”ì²­ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        messages = body.get("messages", [])
        max_tokens = body.get("max_tokens", 100)
        temperature = body.get("temperature", 0.7)
        stream = body.get("stream", False)
        model = body.get("model", "korean-qwen")
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_message = ""
        if messages:
            for msg in reversed(messages):
                if msg.get("role") == "user":
                    user_message = msg.get("content", "")
                    break
        
        if not user_message:
            user_message = "ì•ˆë…•í•˜ì„¸ìš”"
        
        # ì‘ë‹µ ìƒì„±
        response_content = generate_korean_response(user_message, max_tokens)
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (í˜„ì¬ëŠ” ì§€ì›í•˜ì§€ ì•ŠìŒ)
        if stream:
            return JSONResponse(
                content={"error": "ìŠ¤íŠ¸ë¦¬ë°ì€ í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"}
            )
        
        # OpenAI í˜¸í™˜ ì‘ë‹µ
        response = {
            "id": f"chatcmpl-{int(time.time())}-{random.randint(1000, 9999)}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_content
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": len(user_message.split()),
                "completion_tokens": len(response_content.split()),
                "total_tokens": len(user_message.split()) + len(response_content.split())
            }
        }
        
        return response
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ì±„íŒ… ì™„ì„± ì˜¤ë¥˜: {str(e)}"
        )

@app.post("/v1/completions")
async def completions(request: Request):
    """í…ìŠ¤íŠ¸ ì™„ì„± (OpenAI í˜¸í™˜)"""
    
    try:
        body = await request.json()
        
        prompt = body.get("prompt", "")
        max_tokens = body.get("max_tokens", 100)
        
        # ì‘ë‹µ ìƒì„±
        response_text = generate_korean_response(prompt, max_tokens)
        
        # OpenAI í˜¸í™˜ ì‘ë‹µ
        response = {
            "id": f"cmpl-{int(time.time())}-{random.randint(1000, 9999)}",
            "object": "text_completion",
            "created": int(time.time()),
            "model": "korean-qwen",
            "choices": [
                {
                    "text": response_text,
                    "index": 0,
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": len(prompt.split()),
                "completion_tokens": len(response_text.split()),
                "total_tokens": len(prompt.split()) + len(response_text.split())
            }
        }
        
        return response
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"í…ìŠ¤íŠ¸ ì™„ì„± ì˜¤ë¥˜: {str(e)}"
        )

def main():
    print("ğŸš€ ê°„ë‹¨í•œ í•œêµ­ì–´ ëª¨ë¸ ì„œë²„ ì‹œì‘")
    print("=" * 50)
    print("ğŸ’» ìµœì†Œ ì˜ì¡´ì„±ìœ¼ë¡œ ì‹¤í–‰")
    print("ğŸ”— í¬íŠ¸: 8000")
    print("ğŸ‡°ğŸ‡· í•œêµ­ì–´ ì‘ë‹µ ì§€ì›")
    print("ğŸ“¡ OpenAI í˜¸í™˜ API")
    print()
    
    try:
        uvicorn.run(
            app,
            host="127.0.0.1",
            port=8000,
            log_level="info"
        )
    except KeyboardInterrupt:
        print("\nğŸ›‘ ì„œë²„ ì¢…ë£Œ")

if __name__ == "__main__":
    main()
EOF

chmod +x simple_korean_server.py

echo -e "${GREEN}âœ… ê°„ë‹¨í•œ ëŒ€ì²´ ì„œë²„ ìƒì„±: simple_korean_server.py${NC}"

# accelerate ì—†ì´ë„ ì‘ë™í•˜ëŠ” ëª¨ë¸ ì„œë²„ ìƒì„±
echo -e "\n${BLUE}ğŸ“ accelerate ì—†ëŠ” ëª¨ë¸ ì„œë²„ ìƒì„±...${NC}"

cat > lightweight_model_server.py << 'EOF'
#!/usr/bin/env python3
"""
ê²½ëŸ‰ ëª¨ë¸ ì„œë²„ (accelerate ì—†ì´ ì‘ë™)
"""

import sys
import os
import json
import time
import random
from contextlib import asynccontextmanager

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
import uvicorn

# ëª¨ë¸ ë¡œë“œ ì‹œë„
model = None
tokenizer = None
model_available = False

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    global model, tokenizer, model_available
    
    print("ğŸ”½ ê²½ëŸ‰ ëª¨ë¸ ë¡œë“œ ì‹œë„...")
    
    try:
        # transformers ì—†ì´ë„ ì‘ë™í•˜ë„ë¡ ì‹œë„
        try:
            import torch
            from transformers import AutoTokenizer, AutoModelForCausalLM
            
            print("Transformers ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë¡œë“œ ì‹œë„...")
            model_name = "microsoft/DialoGPT-medium"
            
            # ê°„ë‹¨í•œ ë¡œë“œ ë°©ì‹ (accelerate ì—†ì´)
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # CPU ì „ìš©ìœ¼ë¡œ ë¡œë“œ
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                low_cpu_mem_usage=True  # accelerate ëŒ€ì‹  ì‚¬ìš©
            )
            model.eval()
            
            print("âœ… ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            model_available = True
            
        except Exception as e:
            print(f"âš ï¸ ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±ê¸°ë¡œ ëŒ€ì²´")
            model_available = False
            
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê°„ë‹¨í•œ ëª¨ë“œë¡œ ì‹¤í–‰: {e}")
        model_available = False
    
    yield
    
    print("ğŸ›‘ ì„œë²„ ì¢…ë£Œ")

# FastAPI ì•±
app = FastAPI(
    title="ê²½ëŸ‰ í•œêµ­ì–´ ëª¨ë¸ ì„œë²„",
    lifespan=lifespan
)

def simple_generate(user_input: str) -> str:
    """ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„± (ëª¨ë¸ ì—†ì´)"""
    
    responses = {
        'greeting': [
            "ì•ˆë…•í•˜ì„¸ìš”! ê²½ëŸ‰ ëª¨ë¸ ì„œë²„ì…ë‹ˆë‹¤.",
            "ë°˜ê°‘ìŠµë‹ˆë‹¤! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
            "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ í•œêµ­ì–´ AIì…ë‹ˆë‹¤."
        ],
        'token': [
            "í† í° ì œí•œ ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.",
            "Token Limiterì™€ ì˜ ì—°ë™ë˜ê³  ìˆì–´ìš”.",
            "í•œêµ­ì–´ í† í° ê³„ì‚°ì´ ì •í™•íˆ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤."
        ],
        'server': [
            "ê²½ëŸ‰ ì„œë²„ê°€ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.",
            "SGLang ëŒ€ì²´ ì„œë²„ê°€ ì •ìƒ ì‘ë™í•´ìš”.",
            "CPU ëª¨ë“œë¡œ ë¬¸ì œì—†ì´ ì‹¤í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤."
        ],
        'test': [
            "í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤!",
            "ëª¨ë“  ê¸°ëŠ¥ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.",
            "API í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ í†µê³¼!"
        ],
        'default': [
            "ë„¤, ì´í•´í–ˆìŠµë‹ˆë‹¤. ë” ê¶ê¸ˆí•œ ì ì´ ìˆë‚˜ìš”?",
            "ì¢‹ì€ ì§ˆë¬¸ì´ë„¤ìš”. ë‹¤ë¥¸ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•˜ì„¸ìš”.",
            "ê²½ëŸ‰ ì„œë²„ê°€ ì‘ë‹µì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.",
            "SGLang ëŒ€ì²´ ì‹œìŠ¤í…œì´ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤."
        ]
    }
    
    user_lower = user_input.lower()
    
    if any(word in user_lower for word in ['ì•ˆë…•', 'hello', 'í•˜ì´', 'hi']):
        return random.choice(responses['greeting'])
    elif any(word in user_lower for word in ['í† í°', 'token', 'ì œí•œ']):
        return random.choice(responses['token'])
    elif any(word in user_lower for word in ['ì„œë²„', 'server', 'sglang']):
        return random.choice(responses['server'])
    elif any(word in user_lower for word in ['í…ŒìŠ¤íŠ¸', 'test', 'í™•ì¸']):
        return random.choice(responses['test'])
    else:
        return random.choice(responses['default'])

def real_generate(user_input: str, max_tokens: int = 100) -> str:
    """ì‹¤ì œ ëª¨ë¸ì„ ì‚¬ìš©í•œ ìƒì„±"""
    global model, tokenizer
    
    try:
        # í† í°í™”
        inputs = tokenizer.encode(user_input, return_tensors="pt", max_length=512, truncation=True)
        
        # ìƒì„±
        import torch
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=min(max_tokens, 100),
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ì…ë ¥ ë¶€ë¶„ ì œê±°
        if response.startswith(user_input):
            response = response[len(user_input):].strip()
        
        return response if response else simple_generate(user_input)
        
    except Exception as e:
        print(f"ì‹¤ì œ ìƒì„± ì‹¤íŒ¨: {e}")
        return simple_generate(user_input)

@app.get("/get_model_info")
async def get_model_info():
    """ëª¨ë¸ ì •ë³´"""
    return {
        "model_path": "lightweight-korean-server",
        "max_total_tokens": 1024,
        "served_model_names": ["korean-qwen"],
        "is_generation": True,
        "model_available": model_available
    }

@app.get("/v1/models")
async def list_models():
    """ëª¨ë¸ ëª©ë¡"""
    return {
        "object": "list",
        "data": [
            {
                "id": "korean-qwen",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "lightweight-server"
            }
        ]
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    """ì±„íŒ… ì™„ì„±"""
    
    try:
        body = await request.json()
        
        messages = body.get("messages", [])
        max_tokens = body.get("max_tokens", 100)
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_message = ""
        if messages:
            for msg in reversed(messages):
                if msg.get("role") == "user":
                    user_message = msg.get("content", "")
                    break
        
        if not user_message:
            user_message = "ì•ˆë…•í•˜ì„¸ìš”"
        
        # ì‘ë‹µ ìƒì„±
        if model_available and model is not None:
            response_content = real_generate(user_message, max_tokens)
        else:
            response_content = simple_generate(user_message)
        
        # OpenAI í˜¸í™˜ ì‘ë‹µ
        return {
            "id": f"chatcmpl-{int(time.time())}-{random.randint(1000, 9999)}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": "korean-qwen",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_content
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": len(user_message.split()),
                "completion_tokens": len(response_content.split()),
                "total_tokens": len(user_message.split()) + len(response_content.split())
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def main():
    print("ğŸš€ ê²½ëŸ‰ í•œêµ­ì–´ ëª¨ë¸ ì„œë²„")
    print("=" * 40)
    print("ğŸ’» accelerate ì—†ì´ ì‹¤í–‰")
    print("ğŸ”— í¬íŠ¸: 8000")
    print()
    
    uvicorn.run(app, host="127.0.0.1", port=8000, log_level="info")

if __name__ == "__main__":
    main()
EOF

chmod +x lightweight_model_server.py

echo -e "${GREEN}âœ… ê²½ëŸ‰ ëª¨ë¸ ì„œë²„ ìƒì„±: lightweight_model_server.py${NC}"

# í†µí•© ì„œë²„ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
echo -e "\n${BLUE}ğŸ“ í†µí•© ì„œë²„ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±...${NC}"

cat > start_any_server.sh << 'EOF'
#!/bin/bash
# ì–´ë–¤ ì„œë²„ë“  ì‹œì‘í•˜ëŠ” í†µí•© ìŠ¤í¬ë¦½íŠ¸

echo "ğŸš€ í•œêµ­ì–´ ëª¨ë¸ ì„œë²„ ì‹œì‘ ì˜µì…˜"
echo "=============================="

echo "ì–´ë–¤ ì„œë²„ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"
echo ""
echo "1) ê°„ë‹¨í•œ ì„œë²„ (ì¶”ì²œ, ë¹ ë¥¸ ì‹œì‘)"
echo "2) ê²½ëŸ‰ ëª¨ë¸ ì„œë²„ (ì‹¤ì œ ëª¨ë¸ ì‹œë„)"
echo "3) accelerate ì„¤ì¹˜ í›„ ì›ë³¸ ì„œë²„"
echo "4) ëª¨ë“  ì„œë²„ ìƒíƒœ í™•ì¸"

read -p "ì„ íƒ (1-4): " choice

case $choice in
    1)
        echo "ğŸš€ ê°„ë‹¨í•œ ì„œë²„ ì‹œì‘..."
        python simple_korean_server.py
        ;;
    2)
        echo "ğŸ”„ ê²½ëŸ‰ ëª¨ë¸ ì„œë²„ ì‹œì‘..."
        python lightweight_model_server.py
        ;;
    3)
        echo "ğŸ“¦ accelerate ì„¤ì¹˜ í›„ ì›ë³¸ ì„œë²„ ì‹œì‘..."
        pip install accelerate
        python start_alternative_model.py
        ;;
    4)
        echo "ğŸ” ì„œë²„ ìƒíƒœ í™•ì¸..."
        
        echo "í¬íŠ¸ 8000 í™•ì¸:"
        if curl -s http://127.0.0.1:8000/health > /dev/null 2>&1; then
            echo "âœ… í¬íŠ¸ 8000ì—ì„œ ì„œë²„ ì‹¤í–‰ ì¤‘"
        else
            echo "âŒ í¬íŠ¸ 8000ì—ì„œ ì„œë²„ ì—†ìŒ"
        fi
        
        echo ""
        echo "Token Limiter í¬íŠ¸ 8080 í™•ì¸:"
        if curl -s http://127.0.0.1:8080/health > /dev/null 2>&1; then
            echo "âœ… í¬íŠ¸ 8080ì—ì„œ Token Limiter ì‹¤í–‰ ì¤‘"
        else
            echo "âŒ í¬íŠ¸ 8080ì—ì„œ Token Limiter ì—†ìŒ"
        fi
        ;;
    *)
        echo "âŒ ì˜ëª»ëœ ì„ íƒ"
        exit 1
        ;;
esac
EOF

chmod +x start_any_server.sh

echo -e "${GREEN}âœ… í†µí•© ì„œë²„ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: start_any_server.sh${NC}"

echo ""
echo -e "${GREEN}ğŸ‰ ëŒ€ì²´ ì„œë²„ ë¬¸ì œ í•´ê²° ì™„ë£Œ!${NC}"
echo "=================================="

echo -e "${BLUE}ğŸ¯ ì œê³µëœ í•´ê²°ì±…ë“¤:${NC}"
echo "âœ… accelerate íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜"
echo "âœ… ê°„ë‹¨í•œ í•œêµ­ì–´ ì„œë²„ (ìµœì†Œ ì˜ì¡´ì„±)"
echo "âœ… ê²½ëŸ‰ ëª¨ë¸ ì„œë²„ (accelerate ì—†ì´ ì‹¤í–‰)"
echo "âœ… í†µí•© ì„œë²„ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸"

echo ""
echo -e "${BLUE}ğŸš€ ê¶Œì¥ ì‚¬ìš© ë°©ë²•:${NC}"
echo ""
echo "1. ê°€ì¥ ë¹ ë¥¸ ë°©ë²• (ì¦‰ì‹œ ì‘ë™):"
echo "   python simple_korean_server.py"
echo ""
echo "2. ì‹¤ì œ ëª¨ë¸ ì‚¬ìš© ì‹œë„:"
echo "   python lightweight_model_server.py"
echo ""
echo "3. í†µí•© ì„ íƒ ë©”ë‰´:"
echo "   bash start_any_server.sh"
echo ""
echo "4. accelerate ì„¤ì¹˜ í›„ ì›ë³¸ ì‚¬ìš©:"
echo "   pip install accelerate"
echo "   python start_alternative_model.py"

echo ""
echo -e "${PURPLE}ğŸ’¡ ê°„ë‹¨í•œ ì„œë²„ê°€ ê°€ì¥ ì•ˆì •ì ì…ë‹ˆë‹¤!${NC}"
echo "ì‹¤ì œ ëª¨ë¸ ì—†ì´ë„ Token Limiter í…ŒìŠ¤íŠ¸ê°€ ì™„ë²½í•˜ê²Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."

echo ""
echo "ëŒ€ì²´ ì„œë²„ ë¬¸ì œ í•´ê²° ì™„ë£Œ ì‹œê°„: $(date)"