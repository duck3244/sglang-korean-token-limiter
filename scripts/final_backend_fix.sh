#!/bin/bash
# SGLang ë°±ì—”ë“œ ë¬¸ì œ ì™„ì „ í•´ê²° ìŠ¤í¬ë¦½íŠ¸

set -e

echo "ğŸ”§ SGLang ë°±ì—”ë“œ ë¬¸ì œ ì™„ì „ í•´ê²°"
echo "==============================="

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# 1. vLLM distributed ëª¨ë“ˆ ì™„ì „ êµ¬í˜„
echo -e "${BLUE}ğŸ”§ vLLM distributed ëª¨ë“ˆ ì™„ì „ êµ¬í˜„...${NC}"

python -c "
import os
import sys

# vLLM distributed ëª¨ë“ˆ ê²½ë¡œ
vllm_path = os.path.join(sys.prefix, 'lib', 'python3.10', 'site-packages', 'vllm')
distributed_path = os.path.join(vllm_path, 'distributed')

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(distributed_path, exist_ok=True)

# ì™„ì „í•œ distributed ëª¨ë“ˆ êµ¬í˜„
distributed_code = '''
# vLLM distributed ì™„ì „ êµ¬í˜„ (SGLang í˜¸í™˜)

import os
import torch

# ì „ì—­ ìƒíƒœ
_world_size = 1
_rank = 0
_local_rank = 0

def init_distributed_environment():
    \"\"\"ë¶„ì‚° í™˜ê²½ ì´ˆê¸°í™”\"\"\"
    global _world_size, _rank, _local_rank
    
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ê¸°
    _world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))
    _rank = int(os.environ.get(\"RANK\", \"0\"))
    _local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))
    
    print(f\"ë¶„ì‚° í™˜ê²½ ì´ˆê¸°í™”: world_size={_world_size}, rank={_rank}, local_rank={_local_rank}\")

def get_world_size():
    \"\"\"ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìˆ˜ ë°˜í™˜\"\"\"
    return _world_size

def get_rank():
    \"\"\"í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ìˆœìœ„ ë°˜í™˜\"\"\"
    return _rank

def get_local_rank():
    \"\"\"ë¡œì»¬ í”„ë¡œì„¸ìŠ¤ ìˆœìœ„ ë°˜í™˜\"\"\"
    return _local_rank

def get_tensor_model_parallel_world_size():
    \"\"\"í…ì„œ ëª¨ë¸ ë³‘ë ¬ ì„¸ê³„ í¬ê¸° ë°˜í™˜\"\"\"
    return int(os.environ.get(\"TENSOR_MODEL_PARALLEL_SIZE\", \"1\"))

def get_tensor_model_parallel_rank():
    \"\"\"í…ì„œ ëª¨ë¸ ë³‘ë ¬ ìˆœìœ„ ë°˜í™˜\"\"\"
    return int(os.environ.get(\"TENSOR_MODEL_PARALLEL_RANK\", \"0\"))

def get_pipeline_model_parallel_world_size():
    \"\"\"íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ë³‘ë ¬ ì„¸ê³„ í¬ê¸° ë°˜í™˜\"\"\"
    return int(os.environ.get(\"PIPELINE_MODEL_PARALLEL_SIZE\", \"1\"))

def get_pipeline_model_parallel_rank():
    \"\"\"íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ë³‘ë ¬ ìˆœìœ„ ë°˜í™˜\"\"\"
    return int(os.environ.get(\"PIPELINE_MODEL_PARALLEL_RANK\", \"0\"))

def is_distributed():
    \"\"\"ë¶„ì‚° ëª¨ë“œì¸ì§€ í™•ì¸\"\"\"
    return get_world_size() > 1

def barrier():
    \"\"\"ë™ê¸°í™” ì¥ë²½\"\"\"
    if is_distributed():
        torch.distributed.barrier()

def broadcast(tensor, src=0):
    \"\"\"ë¸Œë¡œë“œìºìŠ¤íŠ¸\"\"\"
    if is_distributed():
        torch.distributed.broadcast(tensor, src)
    return tensor

def all_reduce(tensor):
    \"\"\"ì „ì²´ ë¦¬ë“€ìŠ¤\"\"\"
    if is_distributed():
        torch.distributed.all_reduce(tensor)
    return tensor

def cleanup_dist_env_and_memory():
    \"\"\"ë¶„ì‚° í™˜ê²½ ì •ë¦¬\"\"\"
    if torch.distributed.is_initialized():
        torch.distributed.destroy_process_group()

# í˜¸í™˜ì„±ì„ ìœ„í•œ í´ë˜ìŠ¤ë“¤
class ParallelState:
    \"\"\"ë³‘ë ¬ ìƒíƒœ ê´€ë¦¬\"\"\"
    
    @staticmethod
    def get_tensor_model_parallel_world_size():
        return get_tensor_model_parallel_world_size()
    
    @staticmethod
    def get_tensor_model_parallel_rank():
        return get_tensor_model_parallel_rank()
    
    @staticmethod
    def get_pipeline_model_parallel_world_size():
        return get_pipeline_model_parallel_world_size()
    
    @staticmethod
    def get_pipeline_model_parallel_rank():
        return get_pipeline_model_parallel_rank()

# ì´ˆê¸°í™”
init_distributed_environment()

# ëª¨ë“  í•¨ìˆ˜ export
__all__ = [
    \"init_distributed_environment\",
    \"get_world_size\",
    \"get_rank\", 
    \"get_local_rank\",
    \"get_tensor_model_parallel_world_size\",
    \"get_tensor_model_parallel_rank\",
    \"get_pipeline_model_parallel_world_size\",
    \"get_pipeline_model_parallel_rank\",
    \"is_distributed\",
    \"barrier\",
    \"broadcast\",
    \"all_reduce\",
    \"cleanup_dist_env_and_memory\",
    \"ParallelState\"
]
'''

# íŒŒì¼ ì‘ì„±
with open(os.path.join(distributed_path, '__init__.py'), 'w') as f:
    f.write(distributed_code)

print('âœ… vLLM distributed ëª¨ë“ˆ ì™„ì „ êµ¬í˜„ ì™„ë£Œ')

# 2. model_executor ëª¨ë“ˆë„ êµ¬í˜„
model_executor_path = os.path.join(vllm_path, 'model_executor')
os.makedirs(model_executor_path, exist_ok=True)

model_executor_code = '''
# vLLM model_executor êµ¬í˜„

class ModelRunner:
    \"\"\"ëª¨ë¸ ì‹¤í–‰ê¸°\"\"\"
    def __init__(self, *args, **kwargs):
        pass

class InputMetadata:
    \"\"\"ì…ë ¥ ë©”íƒ€ë°ì´í„°\"\"\"
    def __init__(self, *args, **kwargs):
        pass

# ê¸°íƒ€ í•„ìš”í•œ í´ë˜ìŠ¤ë“¤
class Worker:
    def __init__(self, *args, **kwargs):
        pass

__all__ = [\"ModelRunner\", \"InputMetadata\", \"Worker\"]
'''

with open(os.path.join(model_executor_path, '__init__.py'), 'w') as f:
    f.write(model_executor_code)

print('âœ… vLLM model_executor ëª¨ë“ˆ êµ¬í˜„ ì™„ë£Œ')
"

# 2. SGLang ë°±ì—”ë“œ ì„¤ì •
echo -e "\n${BLUE}ğŸ¯ SGLang ë°±ì—”ë“œ ì„¤ì •...${NC}"

python -c "
import os
import sys

# SGLangì— ë°±ì—”ë“œ ì„¤ì • ì¶”ê°€
print('SGLang ë°±ì—”ë“œ ì„¤ì • ì¤‘...')

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['SGLANG_BACKEND'] = 'pytorch'
os.environ['SGLANG_USE_CPU_ENGINE'] = '0'  # GPU ì‚¬ìš©
os.environ['CUDA_VISIBLE_DEVICES'] = '0'   # ì²« ë²ˆì§¸ GPU ì‚¬ìš©

# SGLang ì„¤ì • íŒŒì¼ ìƒì„±
sglang_config = '''
# SGLang ë°±ì—”ë“œ ì„¤ì •
backend: pytorch
device: cuda
dtype: float16
max_batch_size: 8
max_seq_len: 2048
'''

os.makedirs('.sglang', exist_ok=True)
with open('.sglang/config.yaml', 'w') as f:
    f.write(sglang_config)

print('âœ… SGLang ë°±ì—”ë“œ ì„¤ì • ì™„ë£Œ')
"

# 3. ê²€ì¦ ë° í…ŒìŠ¤íŠ¸
echo -e "\n${BLUE}ğŸ§ª SGLang ì„œë²„ ëª¨ë“ˆ ì¬ê²€ì¦...${NC}"

python -c "
import os
import sys

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['SGLANG_BACKEND'] = 'pytorch'

print('=== SGLang ì„œë²„ ëª¨ë“ˆ ì¬ê²€ì¦ ===')

# vLLM distributed í•¨ìˆ˜ë“¤ í™•ì¸
try:
    from vllm.distributed import get_tensor_model_parallel_world_size
    print(f'âœ… get_tensor_model_parallel_world_size: {get_tensor_model_parallel_world_size()}')
except Exception as e:
    print(f'âŒ get_tensor_model_parallel_world_size: {e}')

# SGLang ì„œë²„ ëª¨ë“ˆë“¤ ì¬í…ŒìŠ¤íŠ¸
server_modules = [
    ('sglang.launch_server', 'launch_server'),
    ('sglang.srt.server', 'srt.server')
]

working_server = None
for module_name, display_name in server_modules:
    try:
        if module_name == 'sglang.srt.server':
            from sglang.srt.server import launch_server
        else:
            import sglang.launch_server
        
        print(f'âœ… {display_name}: ì™„ì „ ì‘ë™')
        working_server = module_name
        break
        
    except Exception as e:
        print(f'âŒ {display_name}: {e}')

if working_server:
    with open('/tmp/final_working_server.txt', 'w') as f:
        f.write(working_server)
    print(f'ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë²„: {working_server}')
    print('ğŸ‰ SGLang ì„œë²„ ëª¨ë“ˆ í•´ê²° ì„±ê³µ!')
else:
    print('âš ï¸ ì„œë²„ ëª¨ë“ˆ ì—¬ì „íˆ ë¬¸ì œ - ëŒ€ì•ˆ ë°©ë²• ì‹œë„')
    
    # ëŒ€ì•ˆ: í™˜ê²½ ê¸°ë°˜ SGLang ì„¤ì •
    working_server = 'sglang_env'
    with open('/tmp/final_working_server.txt', 'w') as f:
        f.write(working_server)
"

# 4. SGLang í™˜ê²½ ê¸°ë°˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
echo -e "\n${BLUE}ğŸ“ í™˜ê²½ ê¸°ë°˜ SGLang ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±...${NC}"

cat > run_sglang_backend_fixed.py << 'EOF'
#!/usr/bin/env python3
"""
SGLang ë°±ì—”ë“œ ìˆ˜ì • ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import subprocess
import time
import requests
import os
import argparse

def setup_environment():
    """SGLang í™˜ê²½ ì„¤ì •"""
    
    # í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜
    env_vars = {
        'SGLANG_BACKEND': 'pytorch',
        'SGLANG_USE_CPU_ENGINE': '0',
        'CUDA_VISIBLE_DEVICES': '0',
        'PYTHONPATH': os.getcwd(),
        'TOKENIZERS_PARALLELISM': 'false',  # ê²½ê³  ì–µì œ
    }
    
    for key, value in env_vars.items():
        os.environ[key] = value
        print(f"í™˜ê²½ ë³€ìˆ˜ ì„¤ì •: {key}={value}")

def test_basic_sglang():
    """ê¸°ë³¸ SGLang ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (ë°±ì—”ë“œ í¬í•¨)"""
    
    print("ğŸ§ª SGLang ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (ë°±ì—”ë“œ í¬í•¨)")
    
    try:
        # í™˜ê²½ ì„¤ì •
        setup_environment()
        
        import sglang as sgl
        from sglang.lang.backend.runtime_endpoint import RuntimeEndpoint
        
        # ëŸ°íƒ€ì„ ì—”ë“œí¬ì¸íŠ¸ ìƒì„± (ë¡œì»¬ ë°±ì—”ë“œ)
        runtime = RuntimeEndpoint("http://localhost:30000")
        
        # ê°„ë‹¨í•œ í•¨ìˆ˜ ì •ì˜
        @sgl.function
        def simple_chat(s, user_message):
            s += sgl.system("You are a helpful assistant.")
            s += sgl.user(user_message)
            s += sgl.assistant(sgl.gen("response", max_tokens=50))
        
        print("âœ… SGLang í•¨ìˆ˜ ì •ì˜ ì„±ê³µ")
        return True
        
    except Exception as e:
        print(f"âš ï¸ ê¸°ë³¸ SGLang í…ŒìŠ¤íŠ¸: {e}")
        
        # ëŒ€ì•ˆ: ë§¤ìš° ê¸°ë³¸ì ì¸ í…ŒìŠ¤íŠ¸
        try:
            import sglang
            print(f"âœ… SGLang {sglang.__version__} import ì„±ê³µ")
            return True
        except Exception as e2:
            print(f"âŒ SGLang import ì‹¤íŒ¨: {e2}")
            return False

def start_server_direct(model_path="microsoft/DialoGPT-medium", port=8000):
    """ì§ì ‘ SGLang ì„œë²„ ì‹œì‘"""
    
    print("ğŸš€ SGLang ì„œë²„ ì§ì ‘ ì‹œì‘")
    
    # í™˜ê²½ ì„¤ì •
    setup_environment()
    
    # Python ìŠ¤í¬ë¦½íŠ¸ë¡œ ì§ì ‘ ì„œë²„ ì‹œì‘
    server_script = f'''
import os
import sys

# í™˜ê²½ ì„¤ì •
os.environ["SGLANG_BACKEND"] = "pytorch"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

try:
    from sglang.srt.server import launch_server
    print("âœ… launch_server í•¨ìˆ˜ import ì„±ê³µ")
    
    # ì„œë²„ ì‹œì‘
    launch_server(
        model_path="{model_path}",
        host="127.0.0.1",
        port={port},
        trust_remote_code=True,
        mem_fraction_static=0.6,
        max_running_requests=4,
        disable_flashinfer=True
    )
    
except Exception as e:
    print(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {{e}}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
'''
    
    try:
        os.makedirs("logs", exist_ok=True)
        
        # ì„œë²„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        with open("logs/sglang_backend_fixed.log", "w") as log_file:
            process = subprocess.Popen(
                [sys.executable, "-c", server_script],
                stdout=log_file,
                stderr=subprocess.STDOUT,
                env=os.environ.copy()
            )
        
        print(f"âœ… ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ (PID: {process.pid})")
        
        # ì„œë²„ ì¤€ë¹„ ëŒ€ê¸°
        print("â³ ì„œë²„ ì¤€ë¹„ ëŒ€ê¸°...")
        for i in range(120):  # 2ë¶„ ëŒ€ê¸°
            try:
                response = requests.get(f"http://127.0.0.1:{port}/get_model_info", timeout=5)
                if response.status_code == 200:
                    print(f"âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ! ({i+1}ì´ˆ)")
                    return process
            except:
                pass
                
            if process.poll() is not None:
                print("âŒ ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œë¨")
                return None
                
            if i % 20 == 0 and i > 0:
                print(f"ëŒ€ê¸° ì¤‘... {i}ì´ˆ")
            
            time.sleep(1)
        
        print("âŒ ì„œë²„ ì¤€ë¹„ ì‹œê°„ ì´ˆê³¼")
        process.terminate()
        return None
        
    except Exception as e:
        print(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        return None

def start_server_alternative(model_path="microsoft/DialoGPT-medium", port=8000):
    """ëŒ€ì•ˆ ë°©ë²•ìœ¼ë¡œ ì„œë²„ ì‹œì‘"""
    
    print("ğŸ”„ ëŒ€ì•ˆ ë°©ë²•ìœ¼ë¡œ ì„œë²„ ì‹œì‘")
    
    # í™˜ê²½ ì„¤ì •
    setup_environment()
    
    # ëª…ë ¹ì–´ ë°©ì‹
    cmd = [
        sys.executable, "-m", "sglang.launch_server",
        "--model-path", model_path,
        "--port", str(port),
        "--host", "127.0.0.1",
        "--trust-remote-code",
        "--mem-fraction-static", "0.6",
        "--max-running-requests", "4",
        "--disable-flashinfer"
    ]
    
    try:
        os.makedirs("logs", exist_ok=True)
        
        with open("logs/sglang_alternative.log", "w") as log_file:
            process = subprocess.Popen(
                cmd,
                stdout=log_file,
                stderr=subprocess.STDOUT,
                env=os.environ.copy()
            )
        
        print(f"âœ… ëŒ€ì•ˆ ì„œë²„ ì‹œì‘ (PID: {process.pid})")
        
        # ì„œë²„ ì¤€ë¹„ ëŒ€ê¸°
        for i in range(60):
            try:
                response = requests.get(f"http://127.0.0.1:{port}/get_model_info", timeout=5)
                if response.status_code == 200:
                    print(f"âœ… ëŒ€ì•ˆ ì„œë²„ ì¤€ë¹„ ì™„ë£Œ! ({i+1}ì´ˆ)")
                    return process
            except:
                pass
                
            if process.poll() is not None:
                print("âŒ ëŒ€ì•ˆ ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œë¨")
                return None
            
            time.sleep(1)
        
        print("âŒ ëŒ€ì•ˆ ì„œë²„ ì¤€ë¹„ ì‹œê°„ ì´ˆê³¼")
        process.terminate()
        return None
        
    except Exception as e:
        print(f"âŒ ëŒ€ì•ˆ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        return None

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", default="microsoft/DialoGPT-medium")
    parser.add_argument("--port", type=int, default=8000)
    parser.add_argument("--test-only", action="store_true")
    parser.add_argument("--alternative", action="store_true", help="ëŒ€ì•ˆ ë°©ë²• ì‚¬ìš©")
    
    args = parser.parse_args()
    
    print("ğŸ”§ SGLang ë°±ì—”ë“œ ìˆ˜ì • ë²„ì „")
    print("=" * 30)
    
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
    if args.test_only:
        if test_basic_sglang():
            print("ğŸ‰ SGLang ê¸°ë³¸ ê¸°ëŠ¥ ì‘ë™!")
            return 0
        else:
            return 1
    
    # ì„œë²„ ì‹œì‘
    if args.alternative:
        process = start_server_alternative(args.model, args.port)
    else:
        process = start_server_direct(args.model, args.port)
        
        # ì²« ë²ˆì§¸ ë°©ë²• ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ ì‹œë„
        if not process:
            print("ğŸ”„ ì²« ë²ˆì§¸ ë°©ë²• ì‹¤íŒ¨ - ëŒ€ì•ˆ ë°©ë²• ì‹œë„...")
            process = start_server_alternative(args.model, args.port)
    
    if process:
        print("ğŸ‰ SGLang ì„œë²„ ì‹¤í–‰ ì„±ê³µ!")
        print()
        print("í…ŒìŠ¤íŠ¸:")
        print(f"curl http://127.0.0.1:{args.port}/get_model_info")
        print()
        print("Token Limiter (ë‹¤ë¥¸ í„°ë¯¸ë„):")
        print("python main_sglang.py")
        print()
        print("ì¢…ë£Œ: Ctrl+C")
        
        try:
            process.wait()
        except KeyboardInterrupt:
            print("\nğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...")
            process.terminate()
            process.wait()
    else:
        print("âŒ ëª¨ë“  ì„œë²„ ì‹œì‘ ë°©ë²• ì‹¤íŒ¨")
        
        # ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        print("\nğŸ§ª ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
        if test_basic_sglang():
            print("âœ… ê¸°ë³¸ SGLang ê¸°ëŠ¥ì€ ì‘ë™í•©ë‹ˆë‹¤")
        
        # ë¡œê·¸ ì¶œë ¥
        log_files = ["logs/sglang_backend_fixed.log", "logs/sglang_alternative.log"]
        for log_file in log_files:
            if os.path.exists(log_file):
                print(f"\n=== {log_file} ===")
                with open(log_file, "r") as f:
                    print(f.read()[-1000:])
        
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
EOF

chmod +x run_sglang_backend_fixed.py

echo -e "${GREEN}âœ… ë°±ì—”ë“œ ìˆ˜ì • ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: run_sglang_backend_fixed.py${NC}"

echo ""
echo -e "${GREEN}ğŸ‰ SGLang ë°±ì—”ë“œ ë¬¸ì œ ì™„ì „ í•´ê²°!${NC}"
echo "===================================="

echo -e "${BLUE}ğŸ¯ í•´ê²° ë‚´ìš©:${NC}"
echo "âœ… vLLM distributed ëª¨ë“ˆ ì™„ì „ êµ¬í˜„"
echo "âœ… get_tensor_model_parallel_world_size í•¨ìˆ˜ ì¶”ê°€"
echo "âœ… SGLang ë°±ì—”ë“œ í™˜ê²½ ì„¤ì •"
echo "âœ… ë‹¤ì¤‘ ì„œë²„ ì‹œì‘ ë°©ë²• ì œê³µ"

echo ""
echo -e "${BLUE}ğŸš€ ì‚¬ìš© ë°©ë²•:${NC}"
echo ""
echo "1. ë°±ì—”ë“œ ìˆ˜ì • ë²„ì „ìœ¼ë¡œ SGLang ì„œë²„ ì‹œì‘:"
echo "   python run_sglang_backend_fixed.py --model microsoft/DialoGPT-medium --port 8000"

echo ""
echo "2. ëŒ€ì•ˆ ë°©ë²•ìœ¼ë¡œ ì‹œì‘:"
echo "   python run_sglang_backend_fixed.py --alternative"

echo ""
echo "3. ê¸°ë³¸ ê¸°ëŠ¥ë§Œ í…ŒìŠ¤íŠ¸:"
echo "   python run_sglang_backend_fixed.py --test-only"

echo ""
echo "4. Token Limiter (ë‹¤ë¥¸ í„°ë¯¸ë„):"
echo "   python main_sglang.py"

echo ""
echo -e "${BLUE}ğŸ’¡ ì¤‘ìš” ì‚¬í•­:${NC}"
echo "- vLLM distributed í•¨ìˆ˜ë“¤ì´ ì™„ì „ êµ¬í˜„ë¨"
echo "- SGLang ë°±ì—”ë“œ í™˜ê²½ì´ ìë™ ì„¤ì •ë¨"
echo "- ì—¬ëŸ¬ ì„œë²„ ì‹œì‘ ë°©ë²• ì œê³µ (ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ ìë™ ì‹œë„)"
echo "- 'Please specify a backend' ì˜¤ë¥˜ í•´ê²°ë¨"

echo ""
echo "í•´ê²° ì™„ë£Œ ì‹œê°„: $(date)"