#!/usr/bin/env python3
"""
ëŒ€ì²´ ëª¨ë¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (Transformers ì§ì ‘ ì‚¬ìš©)
"""

import sys
import os
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
import uvicorn
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time
import json

# FastAPI ì•± ìƒì„±
app = FastAPI(title="ëŒ€ì²´ í•œêµ­ì–´ ëª¨ë¸ ì„œë²„")

# ê¸€ë¡œë²Œ ë³€ìˆ˜
model = None
tokenizer = None

@app.on_event("startup")
async def load_model():
    """ëª¨ë¸ ë¡œë“œ"""
    global model, tokenizer
    
    print("ğŸ”½ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model_name = "microsoft/DialoGPT-medium"
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,  # CPU í˜¸í™˜
            device_map="cpu"  # CPU ê°•ì œ
        )
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

@app.get("/get_model_info")
async def get_model_info():
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    return {
        "model_path": "microsoft/DialoGPT-medium",
        "max_total_tokens": 1024,
        "served_model_names": ["korean-qwen"],
        "is_generation": True
    }

@app.get("/v1/models")
async def list_models():
    """ëª¨ë¸ ëª©ë¡"""
    return {
        "data": [
            {
                "id": "korean-qwen",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "alternative-server"
            }
        ]
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    """ì±„íŒ… ì™„ì„±"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        return JSONResponse(
            status_code=503,
            content={"error": "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
        )
    
    try:
        body = await request.json()
        messages = body.get("messages", [])
        max_tokens = body.get("max_tokens", 50)
        
        # ë©”ì‹œì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if messages:
            user_message = messages[-1].get("content", "")
        else:
            user_message = "ì•ˆë…•í•˜ì„¸ìš”"
        
        # í† í°í™”
        inputs = tokenizer.encode(user_message, return_tensors="pt")
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=min(max_tokens, 100),
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ ì œê±°
        if response_text.startswith(user_message):
            response_text = response_text[len(user_message):].strip()
        
        # OpenAI í˜¸í™˜ ì‘ë‹µ
        return {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": "korean-qwen",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_text or "ì•ˆë…•í•˜ì„¸ìš”! ëŒ€ì²´ ëª¨ë¸ ì„œë²„ì…ë‹ˆë‹¤."
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": len(inputs[0]),
                "completion_tokens": len(response_text.split()),
                "total_tokens": len(inputs[0]) + len(response_text.split())
            }
        }
    
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"ìƒì„± ì˜¤ë¥˜: {str(e)}"}
        )

def main():
    print("ğŸš€ ëŒ€ì²´ í•œêµ­ì–´ ëª¨ë¸ ì„œë²„ ì‹œì‘")
    print("=" * 40)
    print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    print("ğŸ”— í¬íŠ¸: 8000")
    print()
    
    # í™˜ê²½ ì„¤ì •
    os.environ['CUDA_VISIBLE_DEVICES'] = ''
    
    try:
        uvicorn.run(
            app,
            host="127.0.0.1",
            port=8000,
            log_level="info"
        )
    except KeyboardInterrupt:
        print("\nğŸ›‘ ì„œë²„ ì¢…ë£Œ")

if __name__ == "__main__":
    main()
