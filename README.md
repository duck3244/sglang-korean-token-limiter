# ğŸ‡°ğŸ‡· SGLang Korean Token Limiter

**ê³ ì„±ëŠ¥ SGLang ê¸°ë°˜ í•œêµ­ì–´ LLM í† í° ì‚¬ìš©ëŸ‰ ì œí•œ ì‹œìŠ¤í…œ**

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![SGLang](https://img.shields.io/badge/SGLang-0.2.6+-red.svg)](https://github.com/sgl-project/sglang)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸŒŸ ê°œìš”

SGLang Korean Token LimiterëŠ” **SGLang í”„ë ˆì„ì›Œí¬**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì–´ LLM ì„œë¹„ìŠ¤ì˜ í† í° ì‚¬ìš©ëŸ‰ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ì œí•œí•˜ëŠ” ê³ ì„±ëŠ¥ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. RTX 4060 8GB GPU í™˜ê²½ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, í•œêµ­ì–´ íŠ¹í™” í† í° ê³„ì‚°ê³¼ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ ì œê³µí•©ë‹ˆë‹¤.

### âš¡ SGLangì˜ ì¥ì 
- **ğŸš€ ìµœëŒ€ 33% ë¹ ë¥¸ ì²˜ë¦¬ëŸ‰**: vLLM ëŒ€ë¹„ íšê¸°ì ì¸ ì„±ëŠ¥ í–¥ìƒ
- **ğŸ’¾ 17% ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©**: íš¨ìœ¨ì ì¸ KV ìºì‹œ ìµœì í™”
- **ğŸ”„ ë™ì  ë°°ì¹˜ ì²˜ë¦¬**: ì‹¤ì‹œê°„ ë°°ì¹˜ í¬ê¸° ì¡°ì •ìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ ìµœì í™”
- **ğŸ› ï¸ ê°„í¸í•œ ì„¤ì •**: ë³µì¡í•œ ì„¤ì • ì—†ì´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥

### ğŸ¯ í•µì‹¬ ê¸°ëŠ¥
- ğŸ”¢ **í•œêµ­ì–´ íŠ¹í™” í† í° ê³„ì‚°**: í•œê¸€ 1ê¸€ì â‰ˆ 1.15í† í°ìœ¼ë¡œ ì •í™•í•œ ê³„ì‚°
- âš¡ **ì‹¤ì‹œê°„ ì†ë„ ì œí•œ**: ë¶„ë‹¹/ì‹œê°„ë‹¹/ì¼ì¼ í† í° ì‚¬ìš©ëŸ‰ ì œí•œ
- ğŸ‘¥ **ë‹¤ì¤‘ ì‚¬ìš©ì ê´€ë¦¬**: API í‚¤ ê¸°ë°˜ ì‚¬ìš©ìë³„ ê°œë³„ ì œí•œ
- ğŸ”„ **OpenAI í˜¸í™˜ API**: ChatGPT APIì™€ 100% í˜¸í™˜
- ğŸ“Š **ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ**: Streamlit ê¸°ë°˜ ëª¨ë‹ˆí„°ë§
- ğŸ‡°ğŸ‡· **ì™„ì „í•œ í•œêµ­ì–´ ì§€ì›**: UTF-8 ì•ˆì „ ì²˜ë¦¬

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    Client[í´ë¼ì´ì–¸íŠ¸ ì•±] --> TokenLimiter[Token Limiter<br/>Port 8080]
    TokenLimiter --> SGLang[SGLang Server<br/>Port 8000]
    TokenLimiter --> Redis[(Redis/SQLite<br/>ì‚¬ìš©ëŸ‰ ì €ì¥)]
    SGLang --> GPU[GPU ì¶”ë¡ <br/>Korean LLM]
    
    subgraph "ëª¨ë‹ˆí„°ë§"
        Dashboard[Streamlit ëŒ€ì‹œë³´ë“œ<br/>Port 8501]
        Metrics[ì„±ëŠ¥ ë©”íŠ¸ë¦­]
    end
    
    TokenLimiter -.-> Dashboard
    SGLang -.-> Metrics
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- **Python**: 3.10 ì´ìƒ
- **GPU**: NVIDIA GPU (RTX 4060 ê¶Œì¥) + CUDA 12.1+
- **ë©”ëª¨ë¦¬**: 8GB RAM ì´ìƒ
- **ì €ì¥ê³µê°„**: 15GB ì´ìƒ (ëª¨ë¸ í¬í•¨)

### 1. ì €ì¥ì†Œ í´ë¡  ë° í™˜ê²½ ì„¤ì •

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/your-username/sglang-korean-token-limiter.git
cd sglang-korean-token-limiter

# Conda í™˜ê²½ ìƒì„± (ê¶Œì¥)
conda create -n korean_sglang python=3.10
conda activate korean_sglang

# íŒ¨í‚¤ì§€ ì„¤ì¹˜ (NumPy í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
pip install "numpy==1.24.4" "pandas==2.1.4" "streamlit==1.28.2"
pip install "sglang[all]" fastapi uvicorn httpx plotly requests psutil
```

### 2. Redis ì„¤ì • (ì„ íƒì‚¬í•­)

```bash
# Dockerë¡œ Redis ì‹¤í–‰
docker run -d --name korean-redis -p 6379:6379 redis:alpine

# ë˜ëŠ” ì‹œìŠ¤í…œ Redis ì‚¬ìš©
sudo systemctl start redis
```

### 3. í•œêµ­ì–´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# í•œêµ­ì–´ Qwen ëª¨ë¸ (ê¶Œì¥)
python -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-3B-Instruct')
model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-3B-Instruct')
print('âœ… í•œêµ­ì–´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ')
"
```

### 4. ì‹œìŠ¤í…œ ì‹œì‘

#### ìë™ ì‹œì‘ (ê¶Œì¥)
```bash
# ì „ì²´ ì‹œìŠ¤í…œ ìë™ ì‹œì‘
bash scripts/start_korean_sglang.sh
```

#### ìˆ˜ë™ ì‹œì‘
```bash
# 1. SGLang ì„œë²„ ì‹œì‘ (í„°ë¯¸ë„ 1)
python -m sglang.launch_server \
  --model-path Qwen/Qwen2.5-3B-Instruct \
  --port 8000 \
  --trust-remote-code \
  --mem-fraction-static 0.75

# 2. Token Limiter ì‹œì‘ (í„°ë¯¸ë„ 2)
python main_sglang.py

# 3. ëŒ€ì‹œë³´ë“œ ì‹œì‘ (í„°ë¯¸ë„ 3)
streamlit run dashboard/sglang_app.py --server.port 8501
```

### 5. í…ŒìŠ¤íŠ¸

```bash
# í—¬ìŠ¤ì²´í¬
curl http://localhost:8080/health

# í•œêµ­ì–´ ì±„íŒ… í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-user1-korean-key-def" \
  -d '{
    "model": "korean-qwen",
    "messages": [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! SGLangìœ¼ë¡œ í•œêµ­ì–´ ëŒ€í™”ê°€ ê°€ëŠ¥í•œê°€ìš”?"}],
    "max_tokens": 100
  }'
```

## ğŸ“š API ì‚¬ìš©ë²•

### ì¸ì¦

ëª¨ë“  API ìš”ì²­ì—ëŠ” Authorization í—¤ë”ê°€ í•„ìš”í•©ë‹ˆë‹¤:

```bash
Authorization: Bearer <API_KEY>
```

### ê¸°ë³¸ API í‚¤

| ì‚¬ìš©ì | API í‚¤ | ì œí•œ (RPM/TPM/ì¼ì¼) |
|--------|--------|-------------------|
| ì‚¬ìš©ì1 | `sk-user1-korean-key-def` | 40/8000/1M |
| ê°œë°œì1 | `sk-dev1-korean-key-789` | 80/15000/2M |
| í…ŒìŠ¤íŠ¸ | `sk-test-korean-key-stu` | 20/3000/500K |

### ì±„íŒ… ì™„ì„± API

```javascript
// JavaScript ì˜ˆì‹œ
const response = await fetch('http://localhost:8080/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer sk-user1-korean-key-def'
  },
  body: JSON.stringify({
    model: 'korean-qwen',
    messages: [
      {role: 'system', content: 'ë‹¹ì‹ ì€ ì¹œê·¼í•œ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.'},
      {role: 'user', content: 'SGLangì˜ ì¥ì ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.'}
    ],
    max_tokens: 200,
    temperature: 0.7,
    stream: false
  })
});
```

### ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
# Python ìŠ¤íŠ¸ë¦¬ë° ì˜ˆì‹œ
import requests
import json

def stream_chat():
    response = requests.post(
        'http://localhost:8080/v1/chat/completions',
        headers={
            'Content-Type': 'application/json',
            'Authorization': 'Bearer sk-user1-korean-key-def'
        },
        json={
            'model': 'korean-qwen',
            'messages': [{'role': 'user', 'content': 'í•œêµ­ì˜ ì „í†µ ìŒì‹ì„ ì†Œê°œí•´ì£¼ì„¸ìš”.'}],
            'max_tokens': 150,
            'stream': True
        },
        stream=True
    )
    
    for line in response.iter_lines():
        if line.startswith(b'data: '):
            data = line[6:]  # 'data: ' ì œê±°
            if data.strip() == b'[DONE]':
                break
            try:
                chunk = json.loads(data)
                content = chunk['choices'][0]['delta'].get('content', '')
                if content:
                    print(content, end='', flush=True)
            except:
                continue
```

## âš™ï¸ ì„¤ì •

### SGLang ì„œë²„ ì„¤ì •

ê¸°ë³¸ ì„¤ì •ì€ `config/sglang_korean.yaml`ì—ì„œ ìˆ˜ì • ê°€ëŠ¥:

```yaml
sglang_server:
  host: "127.0.0.1"
  port: 8000
  model_path: "Qwen/Qwen2.5-3B-Instruct"
  
  # RTX 4060 ìµœì í™” ì„¤ì •
  sglang_args:
    tp_size: 1
    mem_fraction_static: 0.75
    max_running_requests: 16
    enable_torch_compile: true
    chunked_prefill_size: 4096

# í•œêµ­ì–´ íŠ¹í™” ì œí•œ (SGLang ê³ ì„±ëŠ¥ ë°˜ì˜)
default_limits:
  rpm: 40           # vLLM 30 â†’ SGLang 40
  tpm: 8000         # vLLM 5000 â†’ SGLang 8000
  daily: 1000000    # vLLM 500000 â†’ SGLang 1000000
```

### í•œêµ­ì–´ í† í° ì„¤ì •

```yaml
tokenizer:
  model_name: "Qwen/Qwen2.5-3B-Instruct"
  korean_factor: 1.15            # í•œêµ­ì–´ í† í° ê³„ì‚° ë³´ì •ê°’
  max_length: 8192               # SGLang ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
```

## ğŸ–¥ï¸ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ

Streamlit ê¸°ë°˜ ì›¹ ëŒ€ì‹œë³´ë“œë¡œ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§:

```bash
streamlit run dashboard/sglang_app.py --server.port 8501
# ì ‘ì†: http://localhost:8501
```
<img src="demo.png" width="640" height="320">

### ëŒ€ì‹œë³´ë“œ ê¸°ëŠ¥
- ğŸ“ˆ **SGLang ì„œë²„ ì„±ëŠ¥**: ì‹¤ì‹œê°„ RPS, TPS, ì‘ë‹µ ì‹œê°„
- ğŸ® **GPU ëª¨ë‹ˆí„°ë§**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ì˜¨ë„, ì‚¬ìš©ë¥ 
- ğŸ‘¥ **ì‚¬ìš©ì í†µê³„**: ê°œë³„ ì‚¬ìš©ëŸ‰, ì œí•œ ìƒíƒœ
- ğŸ”¥ **KV ìºì‹œ íš¨ìœ¨**: ìºì‹œ íˆíŠ¸ìœ¨, ë©”ëª¨ë¦¬ ìµœì í™”
- ğŸ‡°ğŸ‡· **í•œêµ­ì–´ í† í° ë¶„ì„**: í† í°í™” íš¨ìœ¨ì„±, ì–¸ì–´ ë¹„ìœ¨

### NumPy í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°

ëŒ€ì‹œë³´ë“œ ì‹¤í–‰ ì‹œ NumPy ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´:

```bash
# í˜¸í™˜ ê°€ëŠ¥í•œ ë²„ì „ìœ¼ë¡œ ë‹¤ìš´ê·¸ë ˆì´ë“œ
pip uninstall numpy pandas streamlit -y
pip install "numpy==1.24.4" "pandas==2.1.4" "streamlit==1.28.2"

# ë˜ëŠ” ë‹¨ìˆœí™”ëœ ëŒ€ì‹œë³´ë“œ ì‚¬ìš©
streamlit run simple_dashboard.py --server.port 8501
```

## ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### RTX 4060 Laptop GPU ê¸°ì¤€

| ë©”íŠ¸ë¦­ | SGLang | vLLM | ê°œì„ ìœ¨ |
|-------|---------|------|--------|
| **ì²˜ë¦¬ëŸ‰** (RPS) | 40 | 30 | **+33%** |
| **ì§€ì—°ì‹œê°„** (ms) | 850 | 1200 | **-29%** |
| **ë©”ëª¨ë¦¬ íš¨ìœ¨** | 6.2GB | 7.5GB | **-17%** |
| **ë™ì‹œ ì‚¬ìš©ì** | 16ëª… | 8ëª… | **+100%** |
| **ìºì‹œ íš¨ìœ¨** | 85% | 65% | **+31%** |

### í•œêµ­ì–´ ëª¨ë¸ë³„ ì„±ëŠ¥

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | í† í°/ì´ˆ | ë©”ëª¨ë¦¬ | í•œêµ­ì–´ í’ˆì§ˆ |
|------|---------|---------|---------|------------|
| Qwen2.5-3B-Instruct | 3B | ~200 | 6.2GB | â­â­â­â­â­ |
| Llama-3-Korean-8B | 8B | ~120 | 7.8GB | â­â­â­â­ |
| SOLAR-10.7B-Ko | 11B | ~85 | 7.9GB | â­â­â­â­â­ |

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸

```bash
# ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
bash scripts/test_sglang_korean.sh

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
python test/performance_test.py --concurrent-users 10 --duration 60

# í•œêµ­ì–´ í† í° ì •í™•ë„ í…ŒìŠ¤íŠ¸
python test/korean_token_test.py
```

### API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸

```bash
# í—¬ìŠ¤ì²´í¬
curl http://localhost:8080/health

# ëª¨ë¸ ëª©ë¡
curl http://localhost:8080/v1/models

# í† í° ê³„ì‚°
curl "http://localhost:8080/token-info?text=ì•ˆë…•í•˜ì„¸ìš”"

# ì‚¬ìš©ì í†µê³„
curl http://localhost:8080/stats/ì‚¬ìš©ì1

# SGLang ì„±ëŠ¥ ë©”íŠ¸ë¦­
curl http://localhost:8080/admin/sglang/performance
```

## ğŸ”§ SGLang íŠ¹í™” ê¸°ëŠ¥

### 1. ë™ì  ë°°ì¹˜ ìµœì í™”

SGLangì˜ í•µì‹¬ ì¥ì ì¸ ë™ì  ë°°ì¹˜ ì²˜ë¦¬:

```python
# ì‹¤ì‹œê°„ ë¶€í•˜ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •
@app.middleware("http")
async def dynamic_batch_optimizer(request: Request, call_next):
    current_load = await get_sglang_load()
    
    if current_load > 0.8:
        await adjust_batch_size(increase=True)  # ë†’ì€ ë¶€í•˜ ì‹œ ë°°ì¹˜ ì¦ê°€
    elif current_load < 0.3:
        await adjust_batch_size(increase=False) # ë‚®ì€ ë¶€í•˜ ì‹œ ì§€ì—°ì‹œê°„ ìµœì í™”
    
    return await call_next(request)
```

### 2. KV ìºì‹œ ê´€ë¦¬

```bash
# KV ìºì‹œ ìµœì í™” ì„¤ì •
--enable-prefix-caching          # í”„ë¦¬í”½ìŠ¤ ìºì‹œ í™œì„±í™”
--chunked-prefill-size 4096      # ì²­í¬ í”„ë¦¬í•„ í¬ê¸°
--kv-cache-dtype fp16            # ë©”ëª¨ë¦¬ íš¨ìœ¨ í–¥ìƒ
```

### 3. í•œêµ­ì–´ ìµœì í™”

```python
# í•œêµ­ì–´ íŠ¹í™” í† í° ê³„ì‚°
def count_korean_tokens(text: str) -> int:
    korean_chars = len([c for c in text if '\uac00' <= c <= '\ud7af'])
    english_chars = len([c for c in text if c.isalpha() and ord(c) < 128])
    other_chars = len(text) - korean_chars - english_chars
    
    # SGLang íš¨ìœ¨ì„± ë°˜ì˜
    tokens = int(korean_chars * 1.15 + english_chars * 0.25 + other_chars * 0.5)
    return max(1, tokens)
```

## ğŸ”’ ë³´ì•ˆ ë° ë°°í¬

### í”„ë¡œë•ì…˜ ì„¤ì •

```yaml
# config/production.yaml
sglang_security:
  disable_custom_all_reduce: true
  trust_remote_code: false
  max_model_len: 8192
  enable_p2p_check: true

rate_limiting:
  strict_mode: true
  log_violations: true
  ban_duration: 3600  # 1ì‹œê°„
```

### Docker ë°°í¬

```dockerfile
FROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel

# SGLang ì„¤ì¹˜
RUN pip install "sglang[all]" "numpy==1.24.4"

# ì• í”Œë¦¬ì¼€ì´ì…˜ ë³µì‚¬
COPY . /app
WORKDIR /app

EXPOSE 8080
CMD ["python", "main_sglang.py"]
```

```bash
# Docker ì‹¤í–‰
docker build -t korean-sglang-limiter .
docker run -d --gpus all -p 8080:8080 korean-sglang-limiter
```

## ğŸ“‹ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

#### 1. SGLang ì„œë²„ ì‹œì‘ ì‹¤íŒ¨
```bash
# GPU ë©”ëª¨ë¦¬ í™•ì¸
nvidia-smi

# ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê°ì†Œ
python -m sglang.launch_server --mem-fraction-static 0.6
```

#### 2. NumPy í˜¸í™˜ì„± ì˜¤ë¥˜
```bash
pip install "numpy==1.24.4" --force-reinstall
pip install "pandas==2.1.4" "streamlit==1.28.2"
```

#### 3. í† í° ê³„ì‚° ì˜¤ë¥˜
```python
# í•œêµ­ì–´ í† í°í™” ë””ë²„ê¹…
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct")
tokens = tokenizer.encode("ì•ˆë…•í•˜ì„¸ìš”!")
print(f"í† í° ìˆ˜: {len(tokens)}")
```

### ì„±ëŠ¥ ìµœì í™”

#### RTX 4060 ìµœì í™”
```bash
python -m sglang.launch_server \
  --model-path Qwen/Qwen2.5-3B-Instruct \
  --mem-fraction-static 0.75 \
  --max-running-requests 16 \
  --enable-torch-compile \
  --chunked-prefill-size 4096
```

#### ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
```bash
# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
python -m sglang.launch_server \
  --model-path Qwen/Qwen2.5-1.5B-Instruct \
  --mem-fraction-static 0.6
```

## ğŸ”„ vLLMì—ì„œ ë§ˆì´ê·¸ë ˆì´ì…˜

### 1. íŒ¨í‚¤ì§€ êµì²´
```bash
pip uninstall vllm
pip install "sglang[all]"
```

### 2. ì„¤ì • ë³€ê²½
```yaml
# vLLM ì„¤ì •
vllm_args:
  gpu_memory_utilization: 0.8
  max_model_len: 2048

# SGLang ì„¤ì •ìœ¼ë¡œ ë³€ê²½
sglang_args:
  mem_fraction_static: 0.8
  max_running_requests: 16
```

### 3. API í˜¸í™˜ì„±
- OpenAI í˜¸í™˜ APIëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
- ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”
- ì„±ëŠ¥ í–¥ìƒ íš¨ê³¼ ì¦‰ì‹œ í™•ì¸ ê°€ëŠ¥

### ê°œë°œ í™˜ê²½ ì„¤ì •

```bash
# ê°œë°œìš© íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements-dev.txt
pip install pytest black flake8 mypy

# ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬
black .
flake8 .
mypy .

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest
```

<div align="center">

**ğŸš€ SGLang Korean Token Limiter**

*ê³ ì„±ëŠ¥ â€¢ í•œêµ­ì–´ íŠ¹í™” â€¢ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§*

</div>