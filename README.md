# ğŸ‡°ğŸ‡· Korean SGLang Token Limiter

í•œêµ­ì–´ íŠ¹í™” SGLang ê¸°ë°˜ LLM í† í° ì‚¬ìš©ëŸ‰ ì œí•œ ì‹œìŠ¤í…œ

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![SGLang](https://img.shields.io/badge/SGLang-0.2.6+-red.svg)](https://github.com/sgl-project/sglang)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ ê°œìš”

Korean SGLang Token LimiterëŠ” SGLangì„ ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì–´ LLM ì„œë¹„ìŠ¤ì˜ í† í° ì‚¬ìš©ëŸ‰ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ì œí•œí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. RTX 4060 8GB GPU í™˜ê²½ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, SGLangì˜ ê³ ì„±ëŠ¥ íŠ¹ì„±ì„ í™œìš©í•©ë‹ˆë‹¤.

### âœ¨ SGLangì˜ ì¥ì 
- **ğŸš€ ë” ë¹ ë¥¸ ì¶”ë¡  ì†ë„**: vLLM ëŒ€ë¹„ 20-30% ì„±ëŠ¥ í–¥ìƒ
- **ğŸ’¾ íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©**: KV ìºì‹œ ìµœì í™”ë¡œ ë” ë§ì€ ë™ì‹œ ì‚¬ìš©ì ì§€ì›
- **ğŸ”„ ë™ì  ë°°ì¹˜**: ì‹¤ì‹œê°„ ë°°ì¹˜ í¬ê¸° ì¡°ì •ìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ ìµœì í™”
- **ğŸ› ï¸ ê°„í¸í•œ ì„¤ì •**: ë” ë‹¨ìˆœí•œ ì„¤ì •ê³¼ ë””ë²„ê¹…

### ğŸ¯ ì£¼ìš” ê¸°ëŠ¥
- ğŸ”¢ **í•œêµ­ì–´ íŠ¹í™” í† í° ê³„ì‚°**: í•œê¸€ 1ê¸€ì â‰ˆ 1.2í† í°ìœ¼ë¡œ ì •í™•í•œ ê³„ì‚°
- âš¡ **ì‹¤ì‹œê°„ ì†ë„ ì œí•œ**: ë¶„ë‹¹/ì‹œê°„ë‹¹/ì¼ì¼ í† í° ì‚¬ìš©ëŸ‰ ì œí•œ
- ğŸ‘¥ **ë‹¤ì¤‘ ì‚¬ìš©ì ê´€ë¦¬**: API í‚¤ ê¸°ë°˜ ì‚¬ìš©ìë³„ ê°œë³„ ì œí•œ
- ğŸ”„ **OpenAI í˜¸í™˜ API**: í‘œì¤€ ChatGPT APIì™€ ì™„ì „ í˜¸í™˜
- ğŸ“Š **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ì‚¬ìš©ëŸ‰ í†µê³„ ë° ëŒ€ì‹œë³´ë“œ
- ğŸš€ **SGLang ê³ ì„±ëŠ¥**: ë™ì  ë°°ì¹˜ ë° KV ìºì‹œ ìµœì í™”

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client App    â”‚â”€â”€â”€â–¶â”‚  Token Limiter   â”‚â”€â”€â”€â–¶â”‚ SGLang Server   â”‚
â”‚                 â”‚    â”‚   (Port 8080)    â”‚    â”‚   (Port 8000)   â”‚
â”‚ - Web App       â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ - Mobile App    â”‚    â”‚ - Rate Limiting  â”‚    â”‚ - GPU Inference â”‚
â”‚ - API Client    â”‚    â”‚ - User Managementâ”‚    â”‚ - Dynamic Batch â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ - Token Counting â”‚    â”‚ - KV Cache Opt  â”‚
                       â”‚ - Statistics     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                â”‚                        â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                       â”‚   Redis/SQLite   â”‚              â”‚
                       â”‚                  â”‚              â”‚
                       â”‚ - Usage Data     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ - User Stats     â”‚    â”‚ Korean LLM Modelâ”‚
                       â”‚ - Rate Limits    â”‚    â”‚                 â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ - Qwen2.5       â”‚
                                              â”‚ - Llama3.1-Ko   â”‚
                                              â”‚ - SOLAR-Ko      â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- **Python**: 3.10 ì´ìƒ (SGLang ìš”êµ¬ì‚¬í•­)
- **GPU**: NVIDIA GPU (RTX 4060 ê¶Œì¥) + CUDA 12.1+
- **ë©”ëª¨ë¦¬**: 8GB RAM ì´ìƒ
- **ì €ì¥ê³µê°„**: 15GB ì´ìƒ (ëª¨ë¸ í¬í•¨)

### 1. ì €ì¥ì†Œ í´ë¡ 

```bash
git clone https://github.com/your-username/sglang-korean-token-limiter.git
cd sglang-korean-token-limiter
```

### 2. í™˜ê²½ ì„¤ì •

#### Conda í™˜ê²½ (ê¶Œì¥)

```bash
# Conda í™˜ê²½ ìƒì„±
conda create -n korean_sglang python=3.10
conda activate korean_sglang

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
bash scripts/install_sglang_packages.sh
```

#### Python venv í™˜ê²½

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ë˜ëŠ”
venv\Scripts\activate  # Windows

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
bash scripts/install_packages.sh
```

### 3. Redis ì„¤ì •

#### Docker ì‚¬ìš© (ê¶Œì¥)

```bash
docker run -d --name korean-redis -p 6379:6379 redis:alpine
```

### 4. í•œêµ­ì–´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# í•œêµ­ì–´ Qwen2.5 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ê¶Œì¥)
python scripts/download_korean_model.py --model Qwen/Qwen2.5-3B-Instruct

# ë˜ëŠ” ë‹¤ë¥¸ í•œêµ­ì–´ ëª¨ë¸
python scripts/download_korean_model.py --model beomi/Llama-3-Open-Ko-8B
```

### 5. ì‹œìŠ¤í…œ ì‹œì‘

```bash
# ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘ (SGLang + Token Limiter)
bash scripts/start_korean_sglang.sh
```

### 6. í…ŒìŠ¤íŠ¸

```bash
# í—¬ìŠ¤ì²´í¬
curl http://localhost:8080/health

# ì±„íŒ… ì™„ì„± í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-user1-korean-key-def" \
  -d '{
    "model": "korean-qwen",
    "messages": [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! SGLang ê¸°ë°˜ í•œêµ­ì–´ AIì…ë‹ˆë‹¤."}],
    "max_tokens": 100
  }'
```

## ğŸ“š API ì‚¬ìš©ë²•

### ì¸ì¦

ëª¨ë“  API ìš”ì²­ì—ëŠ” Authorization í—¤ë”ê°€ í•„ìš”í•©ë‹ˆë‹¤:

```bash
Authorization: Bearer <API_KEY>
```

### ê¸°ë³¸ ì‚¬ìš©ì API í‚¤

| ì‚¬ìš©ì | API í‚¤ | ì œí•œ (RPM/TPM/ì¼ì¼) |
|--------|--------|-------------------|
| ì‚¬ìš©ì1 | `sk-user1-korean-key-def` | 30/5000/1M |
| ê°œë°œì1 | `sk-dev1-korean-key-789` | 60/10000/2M |
| í…ŒìŠ¤íŠ¸ | `sk-test-korean-key-stu` | 15/2000/200K |

### ì±„íŒ… ì™„ì„± API

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-user1-korean-key-def" \
  -d '{
    "model": "korean-qwen",
    "messages": [
      {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œê·¼í•œ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
      {"role": "user", "content": "SGLangì˜ ì¥ì ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."}
    ],
    "max_tokens": 200,
    "temperature": 0.7,
    "stream": false
  }'
```

### ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-user1-korean-key-def" \
  -d '{
    "model": "korean-qwen",
    "messages": [{"role": "user", "content": "í•œêµ­ì˜ ì „í†µ ìŒì‹ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."}],
    "max_tokens": 150,
    "stream": true
  }'
```

## âš™ï¸ ì„¤ì •

### SGLang ì„œë²„ ì„¤ì • (`config/sglang_korean.yaml`)

```yaml
server:
  host: "0.0.0.0"
  port: 8080

sglang_server:
  host: "127.0.0.1"
  port: 8000
  model_path: "Qwen/Qwen2.5-3B-Instruct"
  
  # SGLang ìµœì í™” ì„¤ì • (RTX 4060 8GB)
  sglang_args:
    tp_size: 1                    # Tensor parallel size
    mem_fraction_static: 0.7      # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
    max_running_requests: 16      # ë™ì‹œ ì²˜ë¦¬ ìš”ì²­ ìˆ˜
    schedule_policy: "lpm"        # Longest Prefix Match
    disable_flashinfer: false     # FlashInfer ì‚¬ìš©
    enable_torch_compile: true    # Torch compile ìµœì í™”
    chunked_prefill_size: 8192    # Chunked prefill í¬ê¸°

storage:
  type: "redis"  # redis ë˜ëŠ” sqlite
  redis_url: "redis://localhost:6379"

# í•œêµ­ì–´ íŠ¹í™” ê¸°ë³¸ ì œí•œ (SGLang ê³ ì„±ëŠ¥ ë°˜ì˜)
default_limits:
  rpm: 40           # ë¶„ë‹¹ ìš”ì²­ ìˆ˜ (SGLang ì„±ëŠ¥ í–¥ìƒìœ¼ë¡œ ì¦ê°€)
  tpm: 8000         # ë¶„ë‹¹ í† í° ìˆ˜ (í•œêµ­ì–´ í† í° íŠ¹ì„± ê³ ë ¤)
  tph: 500000       # ì‹œê°„ë‹¹ í† í° ìˆ˜
  daily: 1000000    # ì¼ì¼ í† í° ìˆ˜ (ì¦ê°€)
  cooldown_minutes: 2  # ì œí•œ í›„ ëŒ€ê¸° ì‹œê°„ (ë‹¨ì¶•)

# í•œêµ­ì–´ í† í° ì„¤ì •
tokenizer:
  model_name: "Qwen/Qwen2.5-3B-Instruct"
  max_length: 4096               # SGLang ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
  korean_factor: 1.15            # í•œêµ­ì–´ í† í° ê³„ì‚° ë³´ì •ê°’
  cache_dir: "./tokenizer_cache"

# SGLang íŠ¹í™” ì„±ëŠ¥ ì„¤ì •
performance:
  max_concurrent_requests: 20    # ë™ì‹œ ì²˜ë¦¬ ìš”ì²­ ìˆ˜ ì¦ê°€
  request_timeout: 120           # ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
  batch_size: 8                  # ë°°ì¹˜ í¬ê¸°
  enable_streaming: true         # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›
```

### ì‚¬ìš©ì ì„¤ì • (`config/korean_users.yaml`)

```yaml
# í•œêµ­ì–´ í™˜ê²½ ì‚¬ìš©ì ì„¤ì • (SGLang ê³ ì„±ëŠ¥ ë°˜ì˜)
users:
  # ê´€ë¦¬ì ê³„ì •
  admin:
    rpm: 120                    # SGLang ì„±ëŠ¥ í–¥ìƒìœ¼ë¡œ ì¦ê°€
    tpm: 25000                  # ë¶„ë‹¹ 25,000 í† í°
    tph: 1500000                # ì‹œê°„ë‹¹ 1,500,000 í† í°
    daily: 5000000              # ì¼ì¼ 5,000,000 í† í°
    cooldown_minutes: 1         # ì œí•œ ì‹œ 1ë¶„ ëŒ€ê¸°
    description: "ì‹œìŠ¤í…œ ê´€ë¦¬ì"

  # í•œêµ­ì–´ ê°œë°œì ê³„ì •
  í•œêµ­ì–´ê°œë°œì:
    rpm: 80                     # ë¶„ë‹¹ 80íšŒ ìš”ì²­
    tpm: 15000                  # ë¶„ë‹¹ 15,000 í† í°
    tph: 900000                 # ì‹œê°„ë‹¹ 900,000 í† í°
    daily: 3000000              # ì¼ì¼ 3,000,000 í† í°
    cooldown_minutes: 1         # ì œí•œ ì‹œ 1ë¶„ ëŒ€ê¸°
    description: "í•œêµ­ì–´ ëª¨ë¸ ê°œë°œì"

  # ì¼ë°˜ ì‚¬ìš©ì ê³„ì •
  ì‚¬ìš©ì1:
    rpm: 30                     # ë¶„ë‹¹ 30íšŒ ìš”ì²­
    tpm: 5000                   # ë¶„ë‹¹ 5,000 í† í°
    tph: 300000                 # ì‹œê°„ë‹¹ 300,000 í† í°
    daily: 1000000              # ì¼ì¼ 1,000,000 í† í°
    cooldown_minutes: 3         # ì œí•œ ì‹œ 3ë¶„ ëŒ€ê¸°
    description: "ì¼ë°˜ ì‚¬ìš©ì 1"

# API í‚¤ ë§¤í•‘
api_keys:
  "sk-admin-korean-key-123": "admin"
  "sk-dev-korean-key-456": "í•œêµ­ì–´ê°œë°œì"
  "sk-user1-korean-key-def": "ì‚¬ìš©ì1"
```

## ğŸ–¥ï¸ ëŒ€ì‹œë³´ë“œ

Streamlit ê¸°ë°˜ ì›¹ ëŒ€ì‹œë³´ë“œë¡œ SGLang ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§:

```bash
# ëŒ€ì‹œë³´ë“œ ì‹œì‘
streamlit run dashboard/sglang_app.py --server.port 8501

# ì ‘ì†: http://localhost:8501
```

ëŒ€ì‹œë³´ë“œ ê¸°ëŠ¥:
- ğŸ“ˆ SGLang ì„œë²„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ğŸš€ ë™ì  ë°°ì¹˜ í¬ê¸° ë° ì²˜ë¦¬ëŸ‰ í†µê³„
- ğŸ‘¥ ì‚¬ìš©ìë³„ í†µê³„ ë° ì‘ë‹µ ì‹œê°„
- ğŸ”¥ KV ìºì‹œ íˆíŠ¸ìœ¨ ëª¨ë‹ˆí„°ë§
- ğŸ“Š ì‹œìŠ¤í…œ ìì› ì‚¬ìš©ë¥ 

## ğŸ”§ SGLang íŠ¹í™” ê¸°ëŠ¥

### 1. ë™ì  ë°°ì¹˜ ìµœì í™”

```python
# SGLangì˜ ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
@app.middleware("http")
async def dynamic_batch_optimizer(request: Request, call_next):
    current_load = await get_current_load()
    
    if current_load > 0.8:
        # ë†’ì€ ë¶€í•˜ ì‹œ ë°°ì¹˜ í¬ê¸° ì¦ê°€
        await adjust_sglang_batch_size(16)
    else:
        # ë‚®ì€ ë¶€í•˜ ì‹œ ì§€ì—°ì‹œê°„ ìµœì í™”
        await adjust_sglang_batch_size(4)
    
    return await call_next(request)
```

### 2. KV ìºì‹œ ê´€ë¦¬

```python
# KV ìºì‹œ ìµœì í™” ì„¤ì •
sglang_config = {
    "enable_prefix_caching": True,    # í”„ë¦¬í”½ìŠ¤ ìºì‹œ í™œì„±í™”
    "max_prefill_tokens": 8192,       # í”„ë¦¬í•„ í† í° ì œí•œ
    "kv_cache_dtype": "fp8",          # KV ìºì‹œ ë°ì´í„° íƒ€ì…
    "enable_chunked_prefill": True    # ì²­í¬ í”„ë¦¬í•„ í™œì„±í™”
}
```

### 3. ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”

```python
# SGLang ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
async def stream_response(messages, model="korean-qwen"):
    async for chunk in sglang_client.chat_completions(
        messages=messages,
        model=model,
        stream=True,
        max_tokens=500
    ):
        yield f"data: {json.dumps(chunk)}\n\n"
```

## ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### RTX 4060 Laptop GPU ê¸°ì¤€ (SGLang vs vLLM)

| ë©”íŠ¸ë¦­ | SGLang | vLLM | ê°œì„ ìœ¨ |
|-------|---------|------|--------|
| ì²˜ë¦¬ëŸ‰ (í† í°/ì´ˆ) | ~200 | ~150 | +33% |
| ì§€ì—°ì‹œê°„ (ms) | 850 | 1200 | -29% |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GB) | 6.2 | 7.5 | -17% |
| ë™ì‹œ ì‚¬ìš©ì | 8-12ëª… | 4-6ëª… | +100% |
| KV ìºì‹œ íš¨ìœ¨ | 85% | 65% | +31% |

### í•œêµ­ì–´ ëª¨ë¸ë³„ ì„±ëŠ¥

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | í† í°/ì´ˆ | ë©”ëª¨ë¦¬ | í•œêµ­ì–´ í’ˆì§ˆ |
|------|---------|---------|---------|------------|
| Qwen2.5-3B-Instruct | 3B | ~200 | 6.2GB | â­â­â­â­â­ |
| Llama-3-Open-Ko-8B | 8B | ~120 | 7.8GB | â­â­â­â­ |
| SOLAR-10.7B-Instruct-v1.0 | 11B | ~85 | 7.9GB | â­â­â­â­â­ |

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸

```bash
bash scripts/test_sglang_korean.sh
```

### SGLang ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

```bash
# ì²˜ë¦¬ëŸ‰ í…ŒìŠ¤íŠ¸
python test/performance_test.py --concurrent-users 10 --duration 60

# ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
python test/streaming_test.py --model korean-qwen

# KV ìºì‹œ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸
python test/kv_cache_test.py --prefix-length 1000
```

## ğŸ”’ ë³´ì•ˆ ë° ìµœì í™”

### SGLang ë³´ì•ˆ ì„¤ì •

```yaml
# í”„ë¡œë•ì…˜ SGLang ì„¤ì •
sglang_security:
  disable_custom_all_reduce: true    # ë³´ì•ˆ ê°•í™”
  trust_remote_code: false           # ì›ê²© ì½”ë“œ ë¹„í™œì„±í™”
  max_model_len: 4096               # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
  enable_p2p_check: true            # P2P í†µì‹  ê²€ì¦
```

### ë©”ëª¨ë¦¬ ìµœì í™”

```bash
# RTX 4060 ìµœì í™” ì‹¤í–‰
python -m sglang.launch_server \
  --model-path Qwen/Qwen2.5-3B-Instruct \
  --port 8000 \
  --tp-size 1 \
  --mem-fraction-static 0.75 \
  --max-running-requests 12 \
  --disable-flashinfer \
  --enable-torch-compile
```

## ğŸš¢ ë°°í¬

### Docker ë°°í¬

```dockerfile
# Dockerfile.sglang
FROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel

# SGLang ì„¤ì¹˜
RUN pip install "sglang[all]==0.2.6"

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . /app
WORKDIR /app

# í•œêµ­ì–´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
RUN python scripts/download_korean_model.py

EXPOSE 8080
CMD ["python", "main_sglang.py"]
```

```bash
# Docker ë¹Œë“œ ë° ì‹¤í–‰
docker build -f Dockerfile.sglang -t korean-sglang-limiter .

docker run -d \
  --name korean-sglang \
  --gpus all \
  -p 8080:8080 \
  -v $(pwd)/config:/app/config \
  korean-sglang-limiter
```

### Kubernetes ë°°í¬

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: korean-sglang-limiter
spec:
  replicas: 2
  selector:
    matchLabels:
      app: korean-sglang-limiter
  template:
    metadata:
      labels:
        app: korean-sglang-limiter
    spec:
      containers:
      - name: sglang-server
        image: korean-sglang-limiter:latest
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "16Gi"
          requests:
            memory: "8Gi"
        ports:
        - containerPort: 8080
```

## ğŸ“‹ ë¬¸ì œ í•´ê²°

### SGLang ê´€ë ¨ ì´ìŠˆ

#### 1. SGLang ì„œë²„ ì‹œì‘ ì‹¤íŒ¨

```bash
# GPU ë©”ëª¨ë¦¬ í™•ì¸
nvidia-smi

# ë” ì‘ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ë¡œ ì‹œì‘
python -m sglang.launch_server \
  --model-path Qwen/Qwen2.5-3B-Instruct \
  --mem-fraction-static 0.5 \
  --max-running-requests 8
```

#### 2. KV ìºì‹œ ì˜¤ë¥˜

```bash
# FlashInfer ë¹„í™œì„±í™”
--disable-flashinfer

# ë˜ëŠ” KV ìºì‹œ í¬ê¸° ì¡°ì •
--kv-cache-dtype fp16
```

#### 3. í† ì¹˜ ì»´íŒŒì¼ ì˜¤ë¥˜

```bash
# í† ì¹˜ ì»´íŒŒì¼ ë¹„í™œì„±í™”
--disable-torch-compile

# ë˜ëŠ” CUDA ì»´íŒŒì¼ ëª¨ë“œ ë³€ê²½
export TORCH_COMPILE_MODE=default
```

### í•œêµ­ì–´ í† í°í™” ì´ìŠˆ

```python
# í•œêµ­ì–´ í† í°í™” ë””ë²„ê¹…
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct")
test_text = "ì•ˆë…•í•˜ì„¸ìš”! SGLangìœ¼ë¡œ í•œêµ­ì–´ ì²˜ë¦¬í•˜ê¸°"
tokens = tokenizer.encode(test_text)
print(f"í† í° ìˆ˜: {len(tokens)}")
print(f"í† í°: {tokenizer.convert_ids_to_tokens(tokens)}")
```

## ğŸ”„ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

### vLLMì—ì„œ SGLangìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜

1. **íŒ¨í‚¤ì§€ êµì²´**
```bash
pip uninstall vllm
pip install "sglang[all]==0.2.6"
```

2. **ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸**
```yaml
# vLLM ì„¤ì •
vllm_args:
  gpu_memory_utilization: 0.8
  max_model_len: 2048

# SGLang ì„¤ì •ìœ¼ë¡œ ë³€ê²½
sglang_args:
  mem_fraction_static: 0.8
  max_running_requests: 16
```

3. **API í˜¸í™˜ì„± í™•ì¸**
```python
# ê¸°ì¡´ vLLM ì½”ë“œëŠ” ëŒ€ë¶€ë¶„ ê·¸ëŒ€ë¡œ ì‘ë™
# OpenAI í˜¸í™˜ API ìœ ì§€ë¨
```