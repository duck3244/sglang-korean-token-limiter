#!/usr/bin/env python3
"""
SGLang CUDA ë©€í‹°í”„ë¡œì„¸ì‹± ì˜¤ë¥˜ í•´ê²° ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
import subprocess
import time
import requests
import multiprocessing
import argparse

def set_multiprocessing_method():
    """ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œì‘ ë°©ë²•ì„ spawnìœ¼ë¡œ ì„¤ì •"""
    
    print("ğŸ”§ ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œì‘ ë°©ë²• ì„¤ì •...")
    
    # CUDAì™€ í˜¸í™˜ë˜ëŠ” spawn ë°©ë²• ì‚¬ìš©
    try:
        multiprocessing.set_start_method('spawn', force=True)
        print(f"âœ… ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œì‘ ë°©ë²•: {multiprocessing.get_start_method()}")
    except RuntimeError as e:
        print(f"âš ï¸ ë©€í‹°í”„ë¡œì„¸ì‹± ë°©ë²• ì„¤ì • ì‹¤íŒ¨: {e}")
        print("í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •ì„ ì‹œë„í•©ë‹ˆë‹¤...")
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    env_vars = {
        'TORCH_MULTIPROCESSING_START_METHOD': 'spawn',
        'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:512',
        'CUDA_LAUNCH_BLOCKING': '0',
        'TOKENIZERS_PARALLELISM': 'false'
    }
    
    for key, value in env_vars.items():
        os.environ[key] = value
        print(f"ğŸ”§ {key}={value}")

def clear_cuda_cache():
    """CUDA ìºì‹œ ì •ë¦¬"""
    
    print("ğŸ§¹ CUDA ìºì‹œ ì •ë¦¬...")
    
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            print("âœ… CUDA ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        else:
            print("ğŸ’» CPU ëª¨ë“œ")
    except Exception as e:
        print(f"âš ï¸ CUDA ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

def check_gpu_status():
    """GPU ìƒíƒœ í™•ì¸"""
    
    print("ğŸ” GPU ìƒíƒœ í™•ì¸...")
    
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            gpu_memory_used = torch.cuda.memory_allocated() / 1024**3
            
            print(f"âœ… GPU: {gpu_name}")
            print(f"ğŸ“Š ë©”ëª¨ë¦¬: {gpu_memory_used:.1f}GB / {gpu_memory:.1f}GB")
            
            return True, gpu_name
        else:
            print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
            return False, None
    except Exception as e:
        print(f"âŒ GPU í™•ì¸ ì‹¤íŒ¨: {e}")
        return False, None

def start_sglang_server_spawn(model_path="microsoft/DialoGPT-medium", port=8000):
    """SGLang ì„œë²„ ì‹œì‘ (spawn ë°©ë²• ì‚¬ìš©)"""
    
    print("ğŸš€ SGLang ì„œë²„ ì‹œì‘ (CUDA ë©€í‹°í”„ë¡œì„¸ì‹± í•´ê²° ë²„ì „)")
    print(f"ëª¨ë¸: {model_path}")
    print(f"í¬íŠ¸: {port}")
    
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì •
    set_multiprocessing_method()
    
    # CUDA ìºì‹œ ì •ë¦¬
    clear_cuda_cache()
    
    # GPU ìƒíƒœ í™•ì¸
    gpu_available, gpu_name = check_gpu_status()
    
    # ì„œë²„ ëª…ë ¹ì–´ êµ¬ì„±
    cmd = [
        sys.executable, "-m", "sglang.launch_server",
        "--model-path", model_path,
        "--port", str(port),
        "--host", "127.0.0.1",
        "--trust-remote-code"
    ]
    
    # GPU ì‚¬ìš© ì‹œ ì„¤ì •
    if gpu_available:
        cmd.extend([
            "--mem-fraction-static", "0.7",  # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë” ë³´ìˆ˜ì ìœ¼ë¡œ
            "--max-running-requests", "4",   # ë™ì‹œ ìš”ì²­ ìˆ˜ ì¤„ì„
            "--kv-cache-dtype", "auto",
            "--tensor-parallel-size", "1",
            "--disable-cuda-graph",         # CUDA Graph ë¹„í™œì„±í™” (ë©€í‹°í”„ë¡œì„¸ì‹± ì•ˆì •ì„±)
            "--disable-flashinfer"          # FlashInfer ë¹„í™œì„±í™” (ì•ˆì •ì„±)
        ])
        
        # RTX 4060 íŠ¹í™” ì„¤ì •
        if gpu_name and "4060" in gpu_name:
            cmd.extend([
                "--chunked-prefill-size", "1024",  # ë” ì‘ì€ ì²­í¬ í¬ê¸°
                "--max-total-tokens", "2048"       # í† í° ìˆ˜ ì œí•œ
            ])
    else:
        # CPU ëª¨ë“œ
        cmd.extend([
            "--disable-cuda-graph",
            "--disable-flashinfer"
        ])
    
    print(f"ì‹¤í–‰ ëª…ë ¹ì–´: {' '.join(cmd)}")
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    env = os.environ.copy()
    env.update({
        'TORCH_MULTIPROCESSING_START_METHOD': 'spawn',
        'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:512',
        'CUDA_LAUNCH_BLOCKING': '0',
        'TOKENIZERS_PARALLELISM': 'false',
        'SGLANG_DISABLE_FLASHINFER_WARNING': '1'
    })
    
    try:
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("logs", exist_ok=True)
        
        # ì„œë²„ ì‹œì‘ (ìƒˆë¡œìš´ í”„ë¡œì„¸ìŠ¤ì—ì„œ)
        print("ğŸ”„ ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì¤‘...")
        
        with open("logs/sglang_cuda_fixed.log", "w") as log_file:
            process = subprocess.Popen(
                cmd,
                stdout=log_file,
                stderr=subprocess.STDOUT,
                env=env,
                start_new_session=True  # ìƒˆë¡œìš´ ì„¸ì…˜ì—ì„œ ì‹œì‘
            )
        
        print(f"âœ… ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ (PID: {process.pid})")
        
        # ì„œë²„ ì¤€ë¹„ ëŒ€ê¸°
        print("â³ ì„œë²„ ì¤€ë¹„ ëŒ€ê¸° (CUDA ë©€í‹°í”„ë¡œì„¸ì‹± í•´ê²° ë²„ì „)...")
        
        # ì²˜ìŒ 30ì´ˆëŠ” ë” ìì£¼ ì²´í¬ (ì´ˆê¸°í™” ì‹œê°„)
        for i in range(30):
            if process.poll() is not None:
                print("âŒ ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¡°ê¸° ì¢…ë£Œ")
                return None
            
            try:
                response = requests.get(f"http://127.0.0.1:{port}/get_model_info", timeout=2)
                if response.status_code == 200:
                    print(f"âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ! ({i+1}ì´ˆ)")
                    return process
            except:
                pass
            
            if i % 10 == 0 and i > 0:
                print(f"ì´ˆê¸°í™” ì¤‘... {i}ì´ˆ")
            
            time.sleep(1)
        
        # ì¶”ê°€ ëŒ€ê¸° (ì´ 120ì´ˆê¹Œì§€)
        for i in range(30, 120):
            if process.poll() is not None:
                print("âŒ ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œë¨")
                return None
            
            try:
                response = requests.get(f"http://127.0.0.1:{port}/get_model_info", timeout=3)
                if response.status_code == 200:
                    print(f"âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ! ({i+1}ì´ˆ)")
                    
                    # ëª¨ë¸ ì •ë³´ í‘œì‹œ
                    try:
                        model_info = response.json()
                        print(f"ëª¨ë¸: {model_info.get('model_path', 'Unknown')}")
                        print(f"ìµœëŒ€ í† í°: {model_info.get('max_total_tokens', 'Unknown')}")
                    except:
                        pass
                    
                    return process
            except:
                pass
            
            if i % 20 == 0:
                print(f"ëŒ€ê¸° ì¤‘... {i}ì´ˆ")
                
                # ë¡œê·¸ ì¼ë¶€ í™•ì¸
                if os.path.exists("logs/sglang_cuda_fixed.log"):
                    with open("logs/sglang_cuda_fixed.log", "r") as f:
                        lines = f.readlines()
                        if lines:
                            print("ìµœê·¼ ë¡œê·¸:")
                            for line in lines[-3:]:
                                print(f"  {line.strip()}")
            
            time.sleep(1)
        
        print("âŒ ì„œë²„ ì¤€ë¹„ ì‹œê°„ ì´ˆê³¼")
        process.terminate()
        return None
        
    except Exception as e:
        print(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        return None

def test_server(port=8000):
    """ì„œë²„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    print("\nğŸ§ª ì„œë²„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
    
    try:
        # ëª¨ë¸ ì •ë³´ ì¡°íšŒ
        response = requests.get(f"http://127.0.0.1:{port}/get_model_info", timeout=5)
        if response.status_code == 200:
            print("âœ… ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì„±ê³µ")
            model_info = response.json()
            print(f"  ëª¨ë¸: {model_info.get('model_path', 'Unknown')}")
        
        # ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
        response = requests.get(f"http://127.0.0.1:{port}/v1/models", timeout=5)
        if response.status_code == 200:
            print("âœ… ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì„±ê³µ")
        
        print("ğŸ‰ ì„œë²„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âš ï¸ ì„œë²„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def main():
    parser = argparse.ArgumentParser(description="SGLang CUDA ë©€í‹°í”„ë¡œì„¸ì‹± í•´ê²° ì„œë²„")
    parser.add_argument("--model", default="microsoft/DialoGPT-medium", help="ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--port", type=int, default=8000, help="í¬íŠ¸ ë²ˆí˜¸")
    parser.add_argument("--test-only", action="store_true", help="ì„œë²„ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰")
    
    args = parser.parse_args()
    
    print("ğŸ‰ SGLang CUDA ë©€í‹°í”„ë¡œì„¸ì‹± í•´ê²° ë²„ì „")
    print("=" * 70)
    print(f"ëª¨ë¸: {args.model}")
    print(f"í¬íŠ¸: {args.port}")
    print()
    
    if args.test_only:
        test_server(args.port)
        return 0
    
    # ì„œë²„ ì‹œì‘
    process = start_sglang_server_spawn(args.model, args.port)
    
    if process:
        print("\nğŸ‰ SGLang ì„œë²„ CUDA ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œ í•´ê²° ì„±ê³µ!")
        print("=" * 80)
        
        print()
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´:")
        print(f"curl http://127.0.0.1:{args.port}/get_model_info")
        print(f"curl http://127.0.0.1:{args.port}/v1/models")
        print()
        print("ğŸ‡°ğŸ‡· í•œêµ­ì–´ Token Limiter ì‹œì‘ (ë‹¤ë¥¸ í„°ë¯¸ë„):")
        print("python main_sglang.py")
        print()
        print("ğŸ”— ì±„íŒ… í…ŒìŠ¤íŠ¸:")
        print(f'''curl -X POST http://localhost:8080/v1/chat/completions \\
  -H "Content-Type: application/json" \\
  -H "Authorization: Bearer sk-user1-korean-key-def" \\
  -d '{{"model": "korean-qwen", "messages": [{{"role": "user", "content": "CUDA ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆë‚˜ìš”?"}}], "max_tokens": 100}}' ''')
        print()
        print("âœ¨ í•´ê²°ëœ ë¬¸ì œ:")
        print("   âœ… CUDA ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œì‘ ë°©ë²•ì„ spawnìœ¼ë¡œ ë³€ê²½")
        print("   âœ… CUDA ìºì‹œ ì •ë¦¬ ë° ë©”ëª¨ë¦¬ ìµœì í™”")
        print("   âœ… RTX 4060 íŠ¹í™” ì•ˆì •ì„± ì„¤ì •")
        print("   âœ… í™˜ê²½ ë³€ìˆ˜ ìë™ ì„¤ì •")
        print("   âœ… ìƒˆë¡œìš´ ì„¸ì…˜ì—ì„œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        print()
        print("ğŸ›‘ ì¢…ë£Œ: Ctrl+C")
        
        # ì„œë²„ í…ŒìŠ¤íŠ¸
        test_server(args.port)
        
        try:
            process.wait()
        except KeyboardInterrupt:
            print("\nğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...")
            process.terminate()
            process.wait()
            print("âœ… ì„œë²„ ì •ìƒ ì¢…ë£Œ")
    else:
        print("âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨")
        
        if os.path.exists("logs/sglang_cuda_fixed.log"):
            print("\n=== ë¡œê·¸ (ë§ˆì§€ë§‰ 50ì¤„) ===")
            with open("logs/sglang_cuda_fixed.log", "r") as f:
                lines = f.readlines()
                for line in lines[-50:]:
                    print(line.rstrip())
        
        return 1
    
    return 0

if __name__ == "__main__":
    # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë©€í‹°í”„ë¡œì„¸ì‹± ë°©ë²• ì„¤ì •
    if __name__ == "__main__":
        try:
            multiprocessing.set_start_method('spawn', force=True)
        except RuntimeError:
            pass
    
    sys.exit(main())
