"""
SGLang í´ë¼ì´ì–¸íŠ¸ - SGLang ì„œë²„ì™€ì˜ í†µì‹  ë° í•œêµ­ì–´ ìµœì í™”
"""

import asyncio
import httpx
import json
import time
import logging
from typing import Dict, Any, Optional, AsyncGenerator, List, Tuple
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class SGLangModelStatus(Enum):
    """SGLang ëª¨ë¸ ìƒíƒœ"""
    UNKNOWN = "unknown"
    LOADING = "loading"
    READY = "ready"
    ERROR = "error"
    OVERLOADED = "overloaded"


@dataclass
class SGLangModelInfo:
    """SGLang ëª¨ë¸ ì •ë³´"""
    model_path: str
    served_model_names: List[str]
    max_total_tokens: int
    is_generation: bool
    architecture: str = ""
    vocab_size: int = 0
    context_length: int = 0


@dataclass
class SGLangServerInfo:
    """SGLang ì„œë²„ ì •ë³´"""
    queue_length: int
    running_requests: int
    memory_usage_gb: float
    requests_per_second: float
    tokens_per_second: float
    cache_hit_rate: float
    uptime_seconds: float


@dataclass
class SGLangPerformanceMetrics:
    """SGLang ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    avg_first_token_latency: float
    avg_inter_token_latency: float
    throughput_tokens_per_second: float
    batch_size_avg: float
    kv_cache_usage: float
    gpu_utilization: float
    memory_usage: float


class SGLangClient:
    """SGLang ì„œë²„ì™€ì˜ í†µì‹ ì„ ë‹´ë‹¹í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, base_url: str = "http://127.0.0.1:8000", timeout: float = 60.0):
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.model_info: Optional[SGLangModelInfo] = None
        self.server_info: Optional[SGLangServerInfo] = None
        self.status = SGLangModelStatus.UNKNOWN

        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_history = []
        self.request_count = 0
        self.error_count = 0

        # í•œêµ­ì–´ íŠ¹í™” ì„¤ì •
        self.korean_optimizations = {
            "enable_prefix_caching": True,
            "chunked_prefill": True,
            "korean_tokenizer_mode": True
        }

    async def health_check(self) -> bool:
        """SGLang ì„œë²„ í—¬ìŠ¤ ì²´í¬"""
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.base_url}/health")
                return response.status_code == 200
        except Exception as e:
            logger.debug(f"SGLang health check failed: {e}")
            return False

    async def get_model_info(self) -> Optional[SGLangModelInfo]:
        """SGLang ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(f"{self.base_url}/get_model_info")

                if response.status_code == 200:
                    data = response.json()

                    self.model_info = SGLangModelInfo(
                        model_path=data.get("model_path", ""),
                        served_model_names=data.get("served_model_names", []),
                        max_total_tokens=data.get("max_total_tokens", 0),
                        is_generation=data.get("is_generation", True),
                        architecture=data.get("architecture", ""),
                        vocab_size=data.get("vocab_size", 0),
                        context_length=data.get("context_length", 0)
                    )

                    self.status = SGLangModelStatus.READY
                    logger.info(f"âœ… SGLang ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì„±ê³µ: {self.model_info.model_path}")
                    return self.model_info

        except Exception as e:
            logger.error(f"âŒ SGLang ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            self.status = SGLangModelStatus.ERROR

        return None

    async def get_server_info(self) -> Optional[SGLangServerInfo]:
        """SGLang ì„œë²„ ìƒíƒœ ì •ë³´ ì¡°íšŒ"""
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.base_url}/get_server_info")

                if response.status_code == 200:
                    data = response.json()

                    self.server_info = SGLangServerInfo(
                        queue_length=data.get("queue_length", 0),
                        running_requests=data.get("running_requests", 0),
                        memory_usage_gb=data.get("memory_usage_gb", 0.0),
                        requests_per_second=data.get("requests_per_second", 0.0),
                        tokens_per_second=data.get("tokens_per_second", 0.0),
                        cache_hit_rate=data.get("cache_hit_rate", 0.0),
                        uptime_seconds=data.get("uptime_seconds", 0.0)
                    )

                    logger.debug(f"ğŸ“Š SGLang ì„œë²„ ìƒíƒœ: {self.server_info.running_requests}ê°œ ìš”ì²­ ì²˜ë¦¬ ì¤‘")
                    return self.server_info

        except Exception as e:
            logger.debug(f"SGLang ì„œë²„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        return None

    async def chat_completion(self,
                              messages: List[Dict[str, str]],
                              model: str = "korean-qwen",
                              max_tokens: int = 512,
                              temperature: float = 0.7,
                              top_p: float = 1.0,
                              frequency_penalty: float = 0.0,
                              presence_penalty: float = 0.0,
                              stop: Optional[List[str]] = None,
                              stream: bool = False,
                              korean_optimized: bool = True) -> Dict[str, Any]:
        """SGLang ì±„íŒ… ì™„ì„± ìš”ì²­ (í•œêµ­ì–´ ìµœì í™”)"""

        start_time = time.time()
        self.request_count += 1

        try:
            # í•œêµ­ì–´ ìµœì í™” ì„¤ì •
            request_data = {
                "model": model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "frequency_penalty": frequency_penalty,
                "presence_penalty": presence_penalty,
                "stream": stream
            }

            if stop:
                request_data["stop"] = stop

            # í•œêµ­ì–´ íŠ¹í™” ì˜µì…˜ ì¶”ê°€
            if korean_optimized:
                request_data.update({
                    "repetition_penalty": 1.1,  # í•œêµ­ì–´ ë°˜ë³µ ë°©ì§€
                    "do_sample": True,
                    "pad_token_id": None,
                    "eos_token_id": None
                })

                # í•œêµ­ì–´ ì •ì§€ ì‹œí€€ìŠ¤ ì¶”ê°€
                korean_stop_sequences = ["ì¸ê°„:", "ì‚¬ìš©ì:", "Human:", "User:", "ì§ˆë¬¸:", "ë‹µë³€:"]
                if stop:
                    stop.extend(korean_stop_sequences)
                else:
                    request_data["stop"] = korean_stop_sequences

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/v1/chat/completions",
                    json=request_data,
                    headers={"Content-Type": "application/json"}
                )

                response_time = time.time() - start_time

                if response.status_code == 200:
                    result = response.json()

                    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
                    await self._record_performance_metrics(response_time, True, len(str(messages)))

                    logger.debug(f"âœ… SGLang ì±„íŒ… ì™„ì„± ì„±ê³µ ({response_time:.2f}ì´ˆ)")
                    return result

                else:
                    self.error_count += 1
                    error_text = response.text
                    logger.error(f"âŒ SGLang ì±„íŒ… ì™„ì„± ì‹¤íŒ¨ (HTTP {response.status_code}): {error_text}")

                    return {
                        "error": {
                            "message": f"SGLang ì„œë²„ ì˜¤ë¥˜: {error_text}",
                            "type": "sglang_error",
                            "code": response.status_code
                        }
                    }

        except httpx.TimeoutException:
            self.error_count += 1
            logger.error(f"âŒ SGLang ìš”ì²­ íƒ€ì„ì•„ì›ƒ ({self.timeout}ì´ˆ)")
            return {
                "error": {
                    "message": f"SGLang ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ ({self.timeout}ì´ˆ)",
                    "type": "timeout_error",
                    "code": 408
                }
            }

        except Exception as e:
            self.error_count += 1
            logger.error(f"âŒ SGLang ì±„íŒ… ì™„ì„± ì˜¤ë¥˜: {e}")
            return {
                "error": {
                    "message": f"SGLang í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜: {str(e)}",
                    "type": "client_error",
                    "code": 500
                }
            }

    async def chat_completion_stream(self,
                                     messages: List[Dict[str, str]],
                                     model: str = "korean-qwen",
                                     max_tokens: int = 512,
                                     temperature: float = 0.7,
                                     korean_optimized: bool = True) -> AsyncGenerator[Dict[str, Any], None]:
        """SGLang ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì™„ì„± (í•œêµ­ì–´ ìµœì í™”)"""

        start_time = time.time()
        self.request_count += 1

        try:
            request_data = {
                "model": model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": True
            }

            # í•œêµ­ì–´ ìµœì í™”
            if korean_optimized:
                request_data.update({
                    "repetition_penalty": 1.1,
                    "stop": ["ì¸ê°„:", "ì‚¬ìš©ì:", "Human:", "User:"]
                })

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                async with client.stream(
                        "POST",
                        f"{self.base_url}/v1/chat/completions",
                        json=request_data,
                        headers={"Content-Type": "application/json"}
                ) as response:

                    if response.status_code != 200:
                        self.error_count += 1
                        error_text = await response.aread()
                        logger.error(f"âŒ SGLang ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ (HTTP {response.status_code}): {error_text.decode()}")

                        yield {
                            "error": {
                                "message": f"SGLang ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {error_text.decode()}",
                                "type": "sglang_stream_error",
                                "code": response.status_code
                            }
                        }
                        return

                    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                    chunk_count = 0
                    async for chunk in response.aiter_lines():
                        chunk_count += 1

                        if chunk.startswith("data: "):
                            data = chunk[6:]  # "data: " ì œê±°

                            if data.strip() == "[DONE]":
                                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
                                response_time = time.time() - start_time
                                await self._record_performance_metrics(response_time, True, len(str(messages)),
                                                                       is_stream=True)
                                logger.debug(f"âœ… SGLang ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ({response_time:.2f}ì´ˆ, {chunk_count}ê°œ ì²­í¬)")
                                break

                            try:
                                chunk_data = json.loads(data)

                                # OpenAI í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                                if "choices" in chunk_data:
                                    openai_chunk = {
                                        "id": f"chatcmpl-{int(time.time())}",
                                        "object": "chat.completion.chunk",
                                        "created": int(time.time()),
                                        "model": model,
                                        "choices": chunk_data["choices"]
                                    }
                                    yield openai_chunk

                            except json.JSONDecodeError:
                                continue

        except Exception as e:
            self.error_count += 1
            logger.error(f"âŒ SGLang ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            yield {
                "error": {
                    "message": f"SGLang ìŠ¤íŠ¸ë¦¬ë° í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜: {str(e)}",
                    "type": "stream_client_error",
                    "code": 500
                }
            }

    async def text_completion(self,
                              prompt: str,
                              model: str = "korean-qwen",
                              max_tokens: int = 256,
                              temperature: float = 0.7,
                              korean_optimized: bool = True) -> Dict[str, Any]:
        """SGLang í…ìŠ¤íŠ¸ ì™„ì„± ìš”ì²­"""

        start_time = time.time()
        self.request_count += 1

        try:
            request_data = {
                "model": model,
                "prompt": prompt,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": False
            }

            # í•œêµ­ì–´ ìµœì í™”
            if korean_optimized:
                request_data.update({
                    "repetition_penalty": 1.1,
                    "stop": ["ì¸ê°„:", "ì‚¬ìš©ì:", "Human:", "User:"]
                })

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/v1/completions",
                    json=request_data,
                    headers={"Content-Type": "application/json"}
                )

                response_time = time.time() - start_time

                if response.status_code == 200:
                    result = response.json()

                    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
                    await self._record_performance_metrics(response_time, True, len(prompt))

                    logger.debug(f"âœ… SGLang í…ìŠ¤íŠ¸ ì™„ì„± ì„±ê³µ ({response_time:.2f}ì´ˆ)")
                    return result

                else:
                    self.error_count += 1
                    error_text = response.text
                    logger.error(f"âŒ SGLang í…ìŠ¤íŠ¸ ì™„ì„± ì‹¤íŒ¨ (HTTP {response.status_code}): {error_text}")

                    return {
                        "error": {
                            "message": f"SGLang ì„œë²„ ì˜¤ë¥˜: {error_text}",
                            "type": "sglang_error",
                            "code": response.status_code
                        }
                    }

        except Exception as e:
            self.error_count += 1
            logger.error(f"âŒ SGLang í…ìŠ¤íŠ¸ ì™„ì„± ì˜¤ë¥˜: {e}")
            return {
                "error": {
                    "message": f"SGLang í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜: {str(e)}",
                    "type": "client_error",
                    "code": 500
                }
            }

    async def get_performance_metrics(self) -> SGLangPerformanceMetrics:
        """SGLang ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            # ì„œë²„ì—ì„œ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹œë„
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.base_url}/metrics")

                if response.status_code == 200:
                    data = response.json()

                    return SGLangPerformanceMetrics(
                        avg_first_token_latency=data.get("avg_first_token_latency", 0.0),
                        avg_inter_token_latency=data.get("avg_inter_token_latency", 0.0),
                        throughput_tokens_per_second=data.get("throughput_tokens_per_second", 0.0),
                        batch_size_avg=data.get("batch_size_avg", 0.0),
                        kv_cache_usage=data.get("kv_cache_usage", 0.0),
                        gpu_utilization=data.get("gpu_utilization", 0.0),
                        memory_usage=data.get("memory_usage", 0.0)
                    )

        except Exception as e:
            logger.debug(f"SGLang ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # ë¡œì»¬ ë©”íŠ¸ë¦­ ê³„ì‚°
        if self.performance_history:
            recent_metrics = self.performance_history[-10:]  # ìµœê·¼ 10ê°œ
            avg_response_time = sum(m["response_time"] for m in recent_metrics) / len(recent_metrics)
            success_rate = sum(1 for m in recent_metrics if m["success"]) / len(recent_metrics)

            return SGLangPerformanceMetrics(
                avg_first_token_latency=avg_response_time,
                avg_inter_token_latency=avg_response_time / 10,  # ê·¼ì‚¬ì¹˜
                throughput_tokens_per_second=1000 / avg_response_time if avg_response_time > 0 else 0,
                batch_size_avg=1.0,  # ê¸°ë³¸ê°’
                kv_cache_usage=success_rate * 100,  # ê·¼ì‚¬ì¹˜
                gpu_utilization=80.0 if success_rate > 0.9 else 50.0,  # ê·¼ì‚¬ì¹˜
                memory_usage=70.0  # ê·¼ì‚¬ì¹˜
            )

        # ê¸°ë³¸ê°’ ë°˜í™˜
        return SGLangPerformanceMetrics(
            avg_first_token_latency=0.0,
            avg_inter_token_latency=0.0,
            throughput_tokens_per_second=0.0,
            batch_size_avg=0.0,
            kv_cache_usage=0.0,
            gpu_utilization=0.0,
            memory_usage=0.0
        )

    async def _record_performance_metrics(self, response_time: float, success: bool,
                                          input_length: int, is_stream: bool = False):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        metric = {
            "timestamp": time.time(),
            "response_time": response_time,
            "success": success,
            "input_length": input_length,
            "is_stream": is_stream
        }

        self.performance_history.append(metric)

        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ìµœê·¼ 1000ê°œë§Œ ìœ ì§€)
        if len(self.performance_history) > 1000:
            self.performance_history = self.performance_history[-1000:]

    async def get_client_statistics(self) -> Dict[str, Any]:
        """í´ë¼ì´ì–¸íŠ¸ í†µê³„ ì •ë³´"""
        success_rate = 0.0
        avg_response_time = 0.0

        if self.request_count > 0:
            success_rate = ((self.request_count - self.error_count) / self.request_count) * 100

        if self.performance_history:
            avg_response_time = sum(m["response_time"] for m in self.performance_history) / len(
                self.performance_history)

        return {
            "total_requests": self.request_count,
            "successful_requests": self.request_count - self.error_count,
            "error_count": self.error_count,
            "success_rate": success_rate,
            "avg_response_time": avg_response_time,
            "server_status": self.status.value,
            "model_info": self.model_info.__dict__ if self.model_info else None,
            "server_info": self.server_info.__dict__ if self.server_info else None,
            "korean_optimizations": self.korean_optimizations,
            "framework": "SGLang"
        }

    async def warmup(self, korean_test_messages: Optional[List[Dict[str, str]]] = None):
        """SGLang ì„œë²„ ì›Œë°ì—… (í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ í¬í•¨)"""
        logger.info("ğŸ”¥ SGLang ì„œë²„ ì›Œë°ì—… ì‹œì‘...")

        try:
            # 1. í—¬ìŠ¤ ì²´í¬
            if not await self.health_check():
                logger.warning("âš ï¸ SGLang í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨")
                return False

            # 2. ëª¨ë¸ ì •ë³´ ì¡°íšŒ
            model_info = await self.get_model_info()
            if not model_info:
                logger.warning("âš ï¸ SGLang ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨")
                return False

            # 3. ì„œë²„ ì •ë³´ ì¡°íšŒ
            await self.get_server_info()

            # 4. í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ ìš”ì²­
            if not korean_test_messages:
                korean_test_messages = [
                    {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! SGLang í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."}
                ]

            result = await self.chat_completion(
                messages=korean_test_messages,
                max_tokens=50,
                korean_optimized=True
            )

            if "error" not in result:
                logger.info("âœ… SGLang í•œêµ­ì–´ ì›Œë°ì—… ì„±ê³µ")
                return True
            else:
                logger.warning(f"âš ï¸ SGLang í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result.get('error', {}).get('message')}")
                return False

        except Exception as e:
            logger.error(f"âŒ SGLang ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return False

    async def adaptive_batch_optimization(self, current_load: float) -> Dict[str, Any]:
        """SGLang ë™ì  ë°°ì¹˜ ìµœì í™” ì œì•ˆ"""
        try:
            server_info = await self.get_server_info()

            if not server_info:
                return {"status": "no_server_info"}

            recommendations = []

            # ë¶€í•˜ì— ë”°ë¥¸ ìµœì í™” ì œì•ˆ
            if current_load > 0.8:
                recommendations.append({
                    "type": "increase_batch_size",
                    "description": "ë†’ì€ ë¶€í•˜ ê°ì§€ - ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê¶Œì¥",
                    "suggested_value": "max_running_requests += 4"
                })

            elif current_load < 0.3:
                recommendations.append({
                    "type": "reduce_latency",
                    "description": "ë‚®ì€ ë¶€í•˜ ê°ì§€ - ì§€ì—°ì‹œê°„ ìµœì í™” ê¶Œì¥",
                    "suggested_value": "enable_chunked_prefill=True"
                })

            # KV ìºì‹œ ìµœì í™”
            if server_info.cache_hit_rate < 0.5:
                recommendations.append({
                    "type": "improve_caching",
                    "description": "ìºì‹œ íˆíŠ¸ìœ¨ ê°œì„  í•„ìš”",
                    "suggested_value": "enable_prefix_caching=True"
                })

            return {
                "status": "analyzed",
                "current_load": current_load,
                "server_metrics": server_info.__dict__,
                "recommendations": recommendations,
                "korean_optimizations": self.korean_optimizations
            }

        except Exception as e:
            logger.error(f"âŒ SGLang ë°°ì¹˜ ìµœì í™” ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}

    def set_korean_optimizations(self, **kwargs):
        """í•œêµ­ì–´ ìµœì í™” ì„¤ì • ì—…ë°ì´íŠ¸"""
        self.korean_optimizations.update(kwargs)
        logger.info(f"ğŸ‡°ğŸ‡· SGLang í•œêµ­ì–´ ìµœì í™” ì„¤ì • ì—…ë°ì´íŠ¸: {self.korean_optimizations}")

    async def close(self):
        """í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ"""
        logger.info("âœ… SGLang í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ")


# ì „ì—­ SGLang í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
sglang_client: Optional[SGLangClient] = None


def get_sglang_client(base_url: str = "http://127.0.0.1:8000") -> SGLangClient:
    """SGLang í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global sglang_client
    if sglang_client is None:
        sglang_client = SGLangClient(base_url)
    return sglang_client


async def initialize_sglang_client(base_url: str = "http://127.0.0.1:8000",
                                   warmup: bool = True) -> SGLangClient:
    """SGLang í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    global sglang_client

    sglang_client = SGLangClient(base_url)

    if warmup:
        await sglang_client.warmup()

    logger.info(f"âœ… SGLang í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {base_url}")
    return sglang_client


async def cleanup_sglang_client():
    """SGLang í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬"""
    global sglang_client
    if sglang_client:
        await sglang_client.close()
        sglang_client = None