# ìµœì¢… ì™„ì „í•œ SGLang constrained ëª¨ë“ˆ íŒ¨ì¹˜
import os
import sglang
import shutil


def apply_complete_patch():
    """SGLang constrained ëª¨ë“ˆ ì™„ì „ íŒ¨ì¹˜ ì ìš©"""

    # SGLang ê²½ë¡œ
    sglang_path = os.path.dirname(sglang.__file__)
    constrained_path = os.path.join(sglang_path, 'srt', 'constrained')
    init_file = os.path.join(constrained_path, '__init__.py')

    print(f"SGLang ê²½ë¡œ: {sglang_path}")
    print(f"Constrained ê²½ë¡œ: {constrained_path}")

    # ë°±ì—…
    backup_file = init_file + '.complete_backup'
    if not os.path.exists(backup_file):
        shutil.copy2(init_file, backup_file)
        print(f"âœ… ì›ë³¸ ë°±ì—…: {backup_file}")

    # ì™„ì „í•œ íŒ¨ì¹˜ ë‚´ìš© (ëª¨ë“  í•„ìš”í•œ í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ í¬í•¨)
    complete_patch_content = '''
# SGLang constrained module - complete patch with all required functions
import logging
import json
import re
import time
from typing import List, Dict, Any, Optional, Union, Tuple, Set
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

# Dummy cache function
def dummy_cache(func):
    """Dummy cache decorator"""
    return func

# Cache implementation
try:
    from outlines.caching import cache as disk_cache
except ImportError:
    disk_cache = dummy_cache
    logger.warning("outlines.caching not available, using dummy cache")

def disable_cache():
    """Disable cache function"""
    logger.info("Cache disabled (outlines not available)")
    pass

# FSMInfo class (required by jump_forward.py)
@dataclass
class FSMInfo:
    """Finite State Machine Information for SGLang compatibility"""
    vocab_size: int = 50257
    init_state: int = 0
    final_states: List[int] = None

    def __post_init__(self):
        if self.final_states is None:
            self.final_states = []

# Required function by jump_forward.py
def make_byte_level_fsm(regex_string: str, tokenizer=None) -> FSMInfo:
    """Create a byte-level FSM for regex - dummy implementation"""
    logger.info(f"Creating dummy byte-level FSM for regex: {regex_string}")
    return FSMInfo(
        vocab_size=getattr(tokenizer, 'vocab_size', 50257) if tokenizer else 50257,
        init_state=0,
        final_states=[1]  # Dummy final state
    )

# Required function by jump_forward.py  
def make_deterministic_fsm(fsm_info: FSMInfo) -> FSMInfo:
    """Make FSM deterministic - dummy implementation"""
    logger.info("Making FSM deterministic (dummy)")
    return fsm_info

# Required function by jump_forward.py
def create_fsm_index_tokenizer(fsm_info: FSMInfo, tokenizer=None) -> Dict:
    """Create FSM index tokenizer - dummy implementation"""
    logger.info("Creating FSM index tokenizer (dummy)")
    vocab_size = fsm_info.vocab_size if fsm_info else 50257
    return {
        'states_to_token_maps': {},
        'empty_token_ids': set(),
        'final_states': set(fsm_info.final_states) if fsm_info else set()
    }

# Required function by jump_forward.py
def convert_token_to_string(token, tokenizer=None) -> str:
    """Convert token to string - dummy implementation"""
    if tokenizer and hasattr(tokenizer, 'decode'):
        try:
            return tokenizer.decode([token]) if isinstance(token, int) else str(token)
        except:
            pass
    return str(token)

# Additional required functions
def build_regex_from_object(schema_object: Any) -> str:
    """Build regex from schema object - dummy implementation"""
    logger.info("Building regex from object (dummy)")
    return ".*"  # Accept everything

def make_json_fsm(schema: Union[str, Dict], tokenizer=None) -> FSMInfo:
    """Create JSON FSM - dummy implementation"""
    logger.info(f"Creating JSON FSM for schema: {type(schema)}")
    return FSMInfo(
        vocab_size=getattr(tokenizer, 'vocab_size', 50257) if tokenizer else 50257,
        init_state=0,
        final_states=[1]
    )

def make_choice_fsm(choices: List[str], tokenizer=None) -> FSMInfo:
    """Create choice FSM - dummy implementation"""
    logger.info(f"Creating choice FSM for {len(choices)} choices")
    return FSMInfo(
        vocab_size=getattr(tokenizer, 'vocab_size', 50257) if tokenizer else 50257,
        init_state=0,
        final_states=[1]
    )

# Dummy RegexGuide class
class RegexGuide:
    """Dummy RegexGuide for SGLang compatibility"""

    def __init__(self, regex_string: str, tokenizer=None):
        self.regex_string = regex_string
        self.tokenizer = tokenizer
        self.fsm_info = make_byte_level_fsm(regex_string, tokenizer)
        self.state = 0
        logger.info(f"Created dummy RegexGuide for pattern: {regex_string}")

    def get_next_instruction(self, state):
        # Return a simple instruction that allows any token
        return {"type": "generate", "allowed_tokens": None}

    def is_final_state(self, state):
        return False

    def copy(self):
        return RegexGuide(self.regex_string, self.tokenizer)

    def get_next_state(self, state, token_id):
        return state + 1

    def get_next_tokens(self, state):
        # Return all possible tokens (no restriction)
        if hasattr(self.tokenizer, 'vocab_size'):
            return list(range(self.tokenizer.vocab_size))
        return list(range(50257))  # Default vocab size

# Dummy TransformerTokenizer class
class TransformerTokenizer:
    """Dummy TransformerTokenizer for SGLang compatibility"""

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.vocabulary = getattr(tokenizer, 'get_vocab', lambda: {})()
        self.vocab_size = getattr(tokenizer, 'vocab_size', 50257)
        logger.info("Created dummy TransformerTokenizer")

    def encode(self, text: str) -> List[int]:
        if hasattr(self.tokenizer, 'encode'):
            return self.tokenizer.encode(text)
        return [0]  # Fallback

    def decode(self, token_ids: List[int]) -> str:
        if hasattr(self.tokenizer, 'decode'):
            return self.tokenizer.decode(token_ids)
        return ""  # Fallback

    def convert_token_to_string(self, token):
        return convert_token_to_string(token, self.tokenizer)

# Dummy JSONGuide class
class JSONGuide:
    """Dummy JSONGuide for SGLang compatibility"""

    def __init__(self, schema: Union[str, Dict], tokenizer=None):
        self.schema = schema
        self.tokenizer = tokenizer
        self.fsm_info = make_json_fsm(schema, tokenizer)
        self.state = 0
        logger.info(f"Created dummy JSONGuide for schema: {type(schema)}")

    def get_next_instruction(self, state):
        return {"type": "generate", "allowed_tokens": None}

    def is_final_state(self, state):
        return False

    def get_next_state(self, state, token_id):
        return state + 1

# Dummy ChoiceGuide class
class ChoiceGuide:
    """Dummy ChoiceGuide for SGLang compatibility"""

    def __init__(self, choices: List[str], tokenizer=None):
        self.choices = choices
        self.tokenizer = tokenizer
        self.fsm_info = make_choice_fsm(choices, tokenizer)
        self.state = 0
        logger.info(f"Created dummy ChoiceGuide with {len(choices)} choices")

    def get_next_instruction(self, state):
        return {"type": "generate", "allowed_tokens": None}

    def is_final_state(self, state):
        return False

    def get_next_state(self, state, token_id):
        return state + 1

# Additional classes that might be required
class GrammarGuide:
    """Dummy GrammarGuide for SGLang compatibility"""

    def __init__(self, grammar: str, tokenizer=None):
        self.grammar = grammar
        self.tokenizer = tokenizer
        self.fsm_info = make_byte_level_fsm(".*", tokenizer)  # Accept everything
        self.state = 0

    def get_next_instruction(self, state):
        return {"type": "generate", "allowed_tokens": None}

    def is_final_state(self, state):
        return False

class StopStrings:
    """Dummy StopStrings for SGLang compatibility"""

    def __init__(self, stop_strings: List[str]):
        self.stop_strings = stop_strings

    def __contains__(self, text: str):
        return any(stop in text for stop in self.stop_strings)

# Additional utility functions that might be needed
def get_token_vocabulary(tokenizer) -> Dict[int, str]:
    """Get token vocabulary - dummy implementation"""
    if hasattr(tokenizer, 'get_vocab'):
        vocab = tokenizer.get_vocab()
        return {v: k for k, v in vocab.items()}
    return {}

def get_vocabulary_transition_keys(fsm_info: FSMInfo, vocabulary: Dict) -> Dict:
    """Get vocabulary transition keys - dummy implementation"""
    return {}

def create_states_mapping(fsm_info: FSMInfo) -> Dict:
    """Create states mapping - dummy implementation"""
    return {0: set(range(fsm_info.vocab_size))}

def walk_fsm(fsm_info: FSMInfo, token_ids: List[int], start_state: int = 0) -> int:
    """Walk FSM with token IDs - dummy implementation"""
    return start_state + len(token_ids)

# Export all necessary symbols (comprehensive list)
__all__ = [
    # Cache functions
    'disable_cache',
    'disk_cache',

    # Core classes
    'FSMInfo',
    'RegexGuide',
    'TransformerTokenizer', 
    'JSONGuide',
    'ChoiceGuide',
    'GrammarGuide',
    'StopStrings',

    # Required functions by jump_forward.py
    'make_byte_level_fsm',
    'make_deterministic_fsm',
    'create_fsm_index_tokenizer',
    'convert_token_to_string',
    'build_regex_from_object',
    'make_json_fsm',
    'make_choice_fsm',

    # Utility functions
    'get_token_vocabulary',
    'get_vocabulary_transition_keys',
    'create_states_mapping',
    'walk_fsm'
]

logger.info("SGLang constrained module initialized with complete dummy implementations (all functions included)")
'''

    # íŒ¨ì¹˜ ì ìš©
    with open(init_file, 'w') as f:
        f.write(complete_patch_content)

    print("âœ… ì™„ì „í•œ __init__.py íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")

    return True


def verify_patch():
    """íŒ¨ì¹˜ ê²€ì¦"""
    try:
        print("\n=== ì™„ì „í•œ íŒ¨ì¹˜ ê²€ì¦ ===")

        # 1. ê¸°ë³¸ í´ë˜ìŠ¤ë“¤
        from sglang.srt.constrained import (
            FSMInfo, RegexGuide, TransformerTokenizer,
            JSONGuide, ChoiceGuide, disable_cache
        )
        print("âœ… ê¸°ë³¸ í´ë˜ìŠ¤ë“¤ import ì„±ê³µ")

        # 2. jump_forward.pyê°€ í•„ìš”ë¡œ í•˜ëŠ” í•¨ìˆ˜ë“¤
        from sglang.srt.constrained import (
            make_byte_level_fsm, make_deterministic_fsm,
            create_fsm_index_tokenizer, convert_token_to_string
        )
        print("âœ… jump_forward í•„ìˆ˜ í•¨ìˆ˜ë“¤ import ì„±ê³µ")

        # 3. ì¸ìŠ¤í„´ìŠ¤í™” í…ŒìŠ¤íŠ¸
        fsm_info = FSMInfo(vocab_size=50257)
        regex_guide = RegexGuide('[0-9]+')
        print("âœ… í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤í™” ì„±ê³µ")

        # 4. jump_forward ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸
        from sglang.srt.constrained.jump_forward import JumpForwardCache
        print("âœ… JumpForwardCache import ì„±ê³µ")

        # 5. ìµœì¢… ì„œë²„ ëŸ°ì²˜ í…ŒìŠ¤íŠ¸
        from sglang.srt.server import launch_server
        print("âœ… SGLang ì„œë²„ ëŸ°ì²˜ import ì„±ê³µ")

        print("\nğŸ‰ ëª¨ë“  ê²€ì¦ ì™„ë£Œ! SGLang ì„œë²„ ì‹œì‘ ê°€ëŠ¥!")
        return True

    except Exception as e:
        print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("ğŸ”§ SGLang constrained ëª¨ë“ˆ ìµœì¢… ì™„ì „ íŒ¨ì¹˜")
    print("=" * 50)

    try:
        # íŒ¨ì¹˜ ì ìš©
        if apply_complete_patch():
            # ê²€ì¦
            if verify_patch():
                print("\n" + "=" * 50)
                print("ğŸ‰ íŒ¨ì¹˜ ì„±ê³µ! ì´ì œ SGLang ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                print("ì‹¤í–‰ ëª…ë ¹ì–´: bash scripts/start_korean_sglang.sh")
            else:
                print("\nâŒ íŒ¨ì¹˜ ê²€ì¦ ì‹¤íŒ¨")
        else:
            print("\nâŒ íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨")

    except Exception as e:
        print(f"\nâŒ íŒ¨ì¹˜ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()