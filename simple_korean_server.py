#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ í•œêµ­ì–´ ëª¨ë¸ ì„œë²„ (ìµœì†Œ ì˜ì¡´ì„±)
"""

import sys
import os
import json
import time
import random
from contextlib import asynccontextmanager

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
import uvicorn

# ê¸€ë¡œë²Œ ë³€ìˆ˜
model_loaded = False
model_info = {
    "model_path": "korean-simple-server",
    "max_total_tokens": 2048,
    "served_model_names": ["korean-qwen"],
    "is_generation": True,
    "version": "1.0.0"
}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    # ì‹œì‘
    print("ğŸ”½ ì„œë²„ ì´ˆê¸°í™” ì¤‘...")
    global model_loaded
    
    try:
        # ì‹¤ì œ ëª¨ë¸ ëŒ€ì‹  ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±ê¸° ì‚¬ìš©
        print("âœ… ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±ê¸° ì¤€ë¹„ ì™„ë£Œ")
        model_loaded = True
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨í•˜ì§€ë§Œ ê³„ì† ì§„í–‰: {e}")
        model_loaded = False
    
    yield
    
    # ì¢…ë£Œ
    print("ğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="ê°„ë‹¨í•œ í•œêµ­ì–´ ëª¨ë¸ ì„œë²„",
    description="SGLang ëŒ€ì²´ ì„œë²„ (ìµœì†Œ ì˜ì¡´ì„±)",
    version="1.0.0",
    lifespan=lifespan
)

def generate_korean_response(user_message: str, max_tokens: int = 100) -> str:
    """ê°„ë‹¨í•œ í•œêµ­ì–´ ì‘ë‹µ ìƒì„±"""
    
    # ë¯¸ë¦¬ ì •ì˜ëœ í•œêµ­ì–´ ì‘ë‹µë“¤
    responses = [
        "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ê°„ë‹¨í•œ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
        "ë„¤, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
        "ì¢‹ì€ ì§ˆë¬¸ì´ë„¤ìš”. ë” ìì„¸íˆ ì„¤ëª…í•´ ì£¼ì‹œê² ì–´ìš”?",
        "SGLang ëŒ€ì²´ ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        "í•œêµ­ì–´ í† í° ì œí•œ ì‹œìŠ¤í…œê³¼ ì˜ ì—°ë™ë˜ê³  ìˆì–´ìš”.",
        "Token Limiterê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”.",
        "ì´ ì„œë²„ëŠ” CPU ëª¨ë“œë¡œ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.",
        "CUDA ë¬¸ì œ ì—†ì´ ì˜ ì‘ë™í•˜ê³  ìˆì–´ìš”!",
        "ê°„ë‹¨í•˜ì§€ë§Œ íš¨ê³¼ì ì¸ ëŒ€ì²´ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.",
        "ë” ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”."
    ]
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ì— ë”°ë¥¸ ë§ì¶¤ ì‘ë‹µ
    user_lower = user_message.lower()
    
    if any(word in user_lower for word in ['ì•ˆë…•', 'hello', 'í•˜ì´']):
        return "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” í•œêµ­ì–´ ëŒ€ì²´ ì„œë²„ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    elif any(word in user_lower for word in ['í† í°', 'token']):
        return "í† í° ì œí•œ ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤. SGLang ëŒ€ì²´ ì„œë²„ê°€ Token Limiterì™€ ì˜ ì—°ë™ë˜ê³  ìˆì–´ìš”."
    elif any(word in user_lower for word in ['ì„œë²„', 'server', 'sglang']):
        return "SGLang ëŒ€ì²´ ì„œë²„ê°€ CPU ëª¨ë“œë¡œ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. CUDA ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œ ì—†ì´ ì˜ ì‘ë™í•´ìš”!"
    elif any(word in user_lower for word in ['í…ŒìŠ¤íŠ¸', 'test']):
        return "ë„¤, í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤! ê°„ë‹¨í•œ ëŒ€ì²´ ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì´ì—ìš”."
    elif any(word in user_lower for word in ['ë¬¸ì œ', 'problem', 'ì˜¤ë¥˜', 'error']):
        return "ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤! ì´ ëŒ€ì²´ ì„œë²„ëŠ” SGLangì˜ ë³µì¡í•œ ë¬¸ì œë“¤ì„ ìš°íšŒí•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤."
    else:
        # ëœë¤ ì‘ë‹µ ì„ íƒ
        return random.choice(responses)

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "ê°„ë‹¨í•œ í•œêµ­ì–´ ëª¨ë¸ ì„œë²„",
        "status": "running",
        "model_loaded": model_loaded
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬"""
    return {
        "status": "healthy",
        "model_loaded": model_loaded,
        "server": "simple-korean-server",
        "timestamp": time.time()
    }

@app.get("/get_model_info")
async def get_model_info():
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ (SGLang í˜¸í™˜)"""
    return model_info

@app.get("/get_server_info")
async def get_server_info():
    """ì„œë²„ ì •ë³´ ì¡°íšŒ"""
    return {
        "requests_per_second": 0,
        "tokens_per_second": 0,
        "queue_length": 0,
        "running_requests": 0,
        "memory_usage_gb": 1.0,
        "cache_hit_rate": 0.8,
        "timestamp": time.time()
    }

@app.get("/v1/models")
async def list_models():
    """ëª¨ë¸ ëª©ë¡ (OpenAI í˜¸í™˜)"""
    return {
        "object": "list",
        "data": [
            {
                "id": "korean-qwen",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "simple-korean-server"
            }
        ]
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    """ì±„íŒ… ì™„ì„± (OpenAI í˜¸í™˜)"""
    
    try:
        body = await request.json()
        
        # ìš”ì²­ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        messages = body.get("messages", [])
        max_tokens = body.get("max_tokens", 100)
        temperature = body.get("temperature", 0.7)
        stream = body.get("stream", False)
        model = body.get("model", "korean-qwen")
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_message = ""
        if messages:
            for msg in reversed(messages):
                if msg.get("role") == "user":
                    user_message = msg.get("content", "")
                    break
        
        if not user_message:
            user_message = "ì•ˆë…•í•˜ì„¸ìš”"
        
        # ì‘ë‹µ ìƒì„±
        response_content = generate_korean_response(user_message, max_tokens)
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (í˜„ì¬ëŠ” ì§€ì›í•˜ì§€ ì•ŠìŒ)
        if stream:
            return JSONResponse(
                content={"error": "ìŠ¤íŠ¸ë¦¬ë°ì€ í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"}
            )
        
        # OpenAI í˜¸í™˜ ì‘ë‹µ
        response = {
            "id": f"chatcmpl-{int(time.time())}-{random.randint(1000, 9999)}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_content
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": len(user_message.split()),
                "completion_tokens": len(response_content.split()),
                "total_tokens": len(user_message.split()) + len(response_content.split())
            }
        }
        
        return response
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ì±„íŒ… ì™„ì„± ì˜¤ë¥˜: {str(e)}"
        )

@app.post("/v1/completions")
async def completions(request: Request):
    """í…ìŠ¤íŠ¸ ì™„ì„± (OpenAI í˜¸í™˜)"""
    
    try:
        body = await request.json()
        
        prompt = body.get("prompt", "")
        max_tokens = body.get("max_tokens", 100)
        
        # ì‘ë‹µ ìƒì„±
        response_text = generate_korean_response(prompt, max_tokens)
        
        # OpenAI í˜¸í™˜ ì‘ë‹µ
        response = {
            "id": f"cmpl-{int(time.time())}-{random.randint(1000, 9999)}",
            "object": "text_completion",
            "created": int(time.time()),
            "model": "korean-qwen",
            "choices": [
                {
                    "text": response_text,
                    "index": 0,
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": len(prompt.split()),
                "completion_tokens": len(response_text.split()),
                "total_tokens": len(prompt.split()) + len(response_text.split())
            }
        }
        
        return response
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"í…ìŠ¤íŠ¸ ì™„ì„± ì˜¤ë¥˜: {str(e)}"
        )

def main():
    print("ğŸš€ ê°„ë‹¨í•œ í•œêµ­ì–´ ëª¨ë¸ ì„œë²„ ì‹œì‘")
    print("=" * 50)
    print("ğŸ’» ìµœì†Œ ì˜ì¡´ì„±ìœ¼ë¡œ ì‹¤í–‰")
    print("ğŸ”— í¬íŠ¸: 8000")
    print("ğŸ‡°ğŸ‡· í•œêµ­ì–´ ì‘ë‹µ ì§€ì›")
    print("ğŸ“¡ OpenAI í˜¸í™˜ API")
    print()
    
    try:
        uvicorn.run(
            app,
            host="127.0.0.1",
            port=8000,
            log_level="info"
        )
    except KeyboardInterrupt:
        print("\nğŸ›‘ ì„œë²„ ì¢…ë£Œ")

if __name__ == "__main__":
    main()
